{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtest Analysis\n",
    "\n",
    "Comprehensive backtest with in-sample/out-of-sample validation and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "\n",
    "from data_acquisition import DataAcquisition\n",
    "from signal_generator import SignalGenerator\n",
    "from backtester import Backtester\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "data_acq = DataAcquisition(config)\n",
    "signal_gen = SignalGenerator(config)\n",
    "backtester = Backtester(config)\n",
    "\n",
    "start_date = datetime.strptime(config['backtest']['start_date'], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(config['backtest']['end_date'], '%Y-%m-%d')\n",
    "train_end = datetime.strptime(config['backtest']['train_end'], '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data\n",
    "df = data_acq.prepare_dataset('BTCUSDT', start_date, end_date)\n",
    "signals = signal_gen.generate_signals(df)\n",
    "\n",
    "# Split\n",
    "train_df = df[df.index < train_end]\n",
    "test_df = df[df.index >= train_end]\n",
    "train_signals = signals[signals.index < train_end]\n",
    "test_signals = signals[signals.index >= train_end]\n",
    "\n",
    "print(f\"Train period: {train_df.index[0]} to {train_df.index[-1]}\")\n",
    "print(f\"Test period: {test_df.index[0]} to {test_df.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. In-Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results, train_trades = backtester.run_backtest(train_df, train_signals)\n",
    "train_metrics = backtester.calculate_metrics(train_results, train_trades)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IN-SAMPLE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric:.<40} {value:>15.2f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Out-of-Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results, test_trades = backtester.run_backtest(test_df, test_signals)\n",
    "test_metrics = backtester.calculate_metrics(test_results, test_trades)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUT-OF-SAMPLE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric:.<40} {value:>15.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Selection criteria\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY SELECTION CRITERIA\")\n",
    "print(\"=\"*60)\n",
    "criteria = [\n",
    "    ('Sharpe Ratio > 1.5', test_metrics['Sharpe Ratio'] > 1.5),\n",
    "    ('Max Drawdown > -10%', test_metrics['Max Drawdown (%)'] > -10),\n",
    "    ('Win Rate > 60%', test_metrics['Win Rate (%)'] > 60)\n",
    "]\n",
    "for criterion, passed in criteria:\n",
    "    status = '✓ PASS' if passed else '✗ FAIL'\n",
    "    print(f\"{criterion:.<40} {status:>15}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_passed = all([c[1] for c in criteria])\n",
    "print(f\"\\nOverall: {'✓ STRATEGY SELECTED' if all_passed else '✗ STRATEGY REJECTED'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Equity Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Equity curve\n",
    "axes[0].plot(train_results.index, train_results['equity'], label='In-Sample', linewidth=2)\n",
    "axes[0].plot(test_results.index, test_results['equity'], label='Out-of-Sample', linewidth=2)\n",
    "axes[0].axhline(y=config['backtest']['initial_capital'], color='black', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('Equity Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Portfolio Value ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdown\n",
    "train_cummax = train_results['equity'].cummax()\n",
    "train_dd = (train_results['equity'] - train_cummax) / train_cummax * 100\n",
    "\n",
    "test_cummax = test_results['equity'].cummax()\n",
    "test_dd = (test_results['equity'] - test_cummax) / test_cummax * 100\n",
    "\n",
    "axes[1].fill_between(train_results.index, train_dd, 0, alpha=0.5, label='In-Sample')\n",
    "axes[1].fill_between(test_results.index, test_dd, 0, alpha=0.5, label='Out-of-Sample')\n",
    "axes[1].axhline(y=-10, color='red', linestyle='--', label='Max DD Threshold')\n",
    "axes[1].set_title('Drawdown', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Drawdown (%)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trade Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(test_trades) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # P&L distribution\n",
    "    axes[0, 0].hist(test_trades['pnl'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_title('P&L Distribution')\n",
    "    axes[0, 0].set_xlabel('P&L ($)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative P&L\n",
    "    test_trades['cumulative_pnl'] = test_trades['pnl'].cumsum()\n",
    "    axes[0, 1].plot(range(len(test_trades)), test_trades['cumulative_pnl'], linewidth=2)\n",
    "    axes[0, 1].set_title('Cumulative P&L')\n",
    "    axes[0, 1].set_xlabel('Trade Number')\n",
    "    axes[0, 1].set_ylabel('Cumulative P&L ($)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Returns distribution\n",
    "    axes[1, 0].hist(test_trades['return'] * 100, bins=30, alpha=0.7, edgecolor='black', color='green')\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_title('Return Distribution')\n",
    "    axes[1, 0].set_xlabel('Return (%)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trade duration\n",
    "    test_trades['duration'] = (test_trades['exit_time'] - test_trades['entry_time']).dt.total_seconds() / 3600\n",
    "    axes[1, 1].hist(test_trades['duration'], bins=30, alpha=0.7, edgecolor='black', color='orange')\n",
    "    axes[1, 1].set_title('Trade Duration')\n",
    "    axes[1, 1].set_xlabel('Duration (hours)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTrade Statistics:\")\n",
    "    print(f\"Avg P&L: ${test_trades['pnl'].mean():.2f}\")\n",
    "    print(f\"Avg Return: {test_trades['return'].mean() * 100:.4f}%\")\n",
    "    print(f\"Avg Duration: {test_trades['duration'].mean():.2f} hours\")\n",
    "    print(f\"Win Rate: {(test_trades['pnl'] > 0).sum() / len(test_trades) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(test_trades) > 0:\n",
    "    # T-test: mean return > 0\n",
    "    t_stat, p_value = stats.ttest_1samp(test_trades['return'], 0)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STATISTICAL SIGNIFICANCE TEST\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Null Hypothesis: Mean return = 0\")\n",
    "    print(f\"Alternative: Mean return > 0\")\n",
    "    print(f\"\\nT-statistic: {t_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.6f}\")\n",
    "    print(f\"\\nResult: {'✓ Statistically significant (p < 0.05)' if p_value < 0.05 else '✗ Not significant'}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Bootstrap Sharpe Ratio confidence interval\n",
    "    n_bootstrap = 1000\n",
    "    sharpe_samples = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = test_trades['return'].sample(n=len(test_trades), replace=True)\n",
    "        sharpe = sample.mean() / sample.std() * np.sqrt(252 * 24) if sample.std() > 0 else 0\n",
    "        sharpe_samples.append(sharpe)\n",
    "    \n",
    "    ci_lower = np.percentile(sharpe_samples, 2.5)\n",
    "    ci_upper = np.percentile(sharpe_samples, 97.5)\n",
    "    \n",
    "    print(f\"\\nSharpe Ratio 95% Confidence Interval: [{ci_lower:.2f}, {ci_upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Metric': list(train_metrics.keys()),\n",
    "    'In-Sample': list(train_metrics.values()),\n",
    "    'Out-of-Sample': list(test_metrics.values())\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
