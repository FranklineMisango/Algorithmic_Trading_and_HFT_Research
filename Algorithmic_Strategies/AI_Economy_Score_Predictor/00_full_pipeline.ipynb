{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš¡ Scoring Logic & Performance\n",
    "\n",
    "**Yes, the scorer uses chunking with score aggregation!** Here's how it works:\n",
    "\n",
    "#### 1. **Text Chunking** (Smart Splitting)\n",
    "- Each transcript is split into chunks (default: **2000 characters** each)\n",
    "- Splits on paragraph/sentence boundaries to maintain context\n",
    "- Example: 100,000 char transcript = **50 chunks**\n",
    "\n",
    "#### 2. **LLM Scoring** (Per Chunk)\n",
    "- Each chunk is scored independently via API call (score: 1-5)\n",
    "- Uses gpt-4o-mini with deterministic temperature (0.0)\n",
    "- Each chunk costs ~$0.001 and takes ~0.5-1 second\n",
    "\n",
    "#### 3. **Score Aggregation** (Sophisticated Methods)\n",
    "Combines chunk scores using:\n",
    "- **Trimmed mean**: Removes outliers (top/bottom 10%)\n",
    "- **Position weighting**: Early chunks weighted higher (forward guidance)\n",
    "- **Confidence scoring**: Based on score variance across chunks\n",
    "- **Trend analysis**: Detects sentiment shifts across transcript\n",
    "\n",
    "#### ðŸ“Š **Why It's Slow**\n",
    "- **Default chunk size (2000 chars) = too many chunks!**\n",
    "- Example: 100k transcript = 50 chunks = 50 API calls = ~50 seconds\n",
    "- 919 transcripts Ã— 50 chunks = **45,950 API calls** = ~10 hours!\n",
    "\n",
    "#### âš¡ **Speed Fix**\n",
    "**Increase chunk size** to reduce API calls:\n",
    "- 2000 chars â†’ 10,000 chars = **5x faster**\n",
    "- 50 chunks â†’ 10 chunks per transcript\n",
    "- 10 hours â†’ **2 hours**\n",
    "\n",
    "Run the optimization cell below before scoring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Economy Score Predictor - Full Pipeline\n",
    "\n",
    "Complete end-to-end implementation of the earnings call sentiment â†’ economic prediction â†’ trading strategy pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Pipeline modules loaded\n",
      "âœ“ Config loaded: 9 sections\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from data_acquisition import DataAcquisition\n",
    "from llm_scorer import LLMScorer\n",
    "from feature_engineering import FeatureEngineer\n",
    "from prediction_model import PredictionModel\n",
    "from signal_generator import SignalGenerator\n",
    "from backtester import Backtester\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ“ Pipeline modules loaded\")\n",
    "print(f\"âœ“ Config loaded: {len(config)} sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FRED API initialized\n",
      "âœ“ Loaded 503 S&P 500 constituents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>Adobe Inc.</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Application Software</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>1997-05-05</td>\n",
       "      <td>796343</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMD</td>\n",
       "      <td>Advanced Micro Devices</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>2488</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AES</td>\n",
       "      <td>AES Corporation</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Independent Power Producers &amp; Energy Traders</td>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>1998-10-02</td>\n",
       "      <td>874761</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AFL</td>\n",
       "      <td>Aflac</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Life &amp; Health Insurance</td>\n",
       "      <td>Columbus, Georgia</td>\n",
       "      <td>1999-05-28</td>\n",
       "      <td>4977</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Life Sciences Tools &amp; Services</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>2000-06-05</td>\n",
       "      <td>1090872</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                Security             GICS Sector  \\\n",
       "0    MMM                      3M             Industrials   \n",
       "1    AOS             A. O. Smith             Industrials   \n",
       "2    ABT     Abbott Laboratories             Health Care   \n",
       "3   ABBV                  AbbVie             Health Care   \n",
       "4    ACN               Accenture  Information Technology   \n",
       "5   ADBE              Adobe Inc.  Information Technology   \n",
       "6    AMD  Advanced Micro Devices  Information Technology   \n",
       "7    AES         AES Corporation               Utilities   \n",
       "8    AFL                   Aflac              Financials   \n",
       "9      A    Agilent Technologies             Health Care   \n",
       "\n",
       "                              GICS Sub-Industry    Headquarters Location  \\\n",
       "0                      Industrial Conglomerates    Saint Paul, Minnesota   \n",
       "1                             Building Products     Milwaukee, Wisconsin   \n",
       "2                         Health Care Equipment  North Chicago, Illinois   \n",
       "3                                 Biotechnology  North Chicago, Illinois   \n",
       "4                IT Consulting & Other Services          Dublin, Ireland   \n",
       "5                          Application Software     San Jose, California   \n",
       "6                                Semiconductors  Santa Clara, California   \n",
       "7  Independent Power Producers & Energy Traders      Arlington, Virginia   \n",
       "8                       Life & Health Insurance        Columbus, Georgia   \n",
       "9                Life Sciences Tools & Services  Santa Clara, California   \n",
       "\n",
       "   Date added      CIK      Founded  \n",
       "0  1957-03-04    66740         1902  \n",
       "1  2017-07-26    91142         1916  \n",
       "2  1957-03-04     1800         1888  \n",
       "3  2012-12-31  1551152  2013 (1888)  \n",
       "4  2011-07-06  1467373         1989  \n",
       "5  1997-05-05   796343         1982  \n",
       "6  2017-03-20     2488         1969  \n",
       "7  1998-10-02   874761         1981  \n",
       "8  1999-05-28     4977         1955  \n",
       "9  2000-06-05  1090872         1999  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize data acquisition\n",
    "data_acq = DataAcquisition('config.yaml')\n",
    "sp500 = data_acq.fetch_sp500_constituents()\n",
    "sp500.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FRED API initialized\n",
      "Fetching transcripts from Hugging Face (kurry/sp500_earnings_transcripts)...\n",
      "Downloading dataset...\n",
      "Converting to DataFrame...\n",
      "âœ“ Loaded 33,362 total transcripts\n",
      "âœ“ Loaded 503 S&P 500 constituents\n",
      "Filtering by date and S&P 500 membership...\n",
      "  After date filter: 941 transcripts\n",
      "âœ“ Final result: 919 S&P 500 transcripts (2025-01-01 to 2026-01-01)\n",
      "Loaded 919 transcripts for 2025\n",
      "Date range: 2025-01-10 to 2025-05-15\n",
      "âœ“ Fetched gdp: 3 observations\n",
      "âœ“ Fetched industrial_production: 12 observations\n",
      "âœ“ Fetched employment: 12 observations\n",
      "âœ“ Fetched wages: 12 observations\n",
      "Loaded 4 macro indicators\n",
      "âœ“ Loaded 503 S&P 500 constituents\n",
      "Loaded 503 S&P 500 stocks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_acquisition import DataAcquisition\n",
    "data = DataAcquisition(\"config.yaml\")\n",
    "transcripts = data.fetch_earnings_transcripts('2025-01-01', '2026-01-01')\n",
    "\n",
    "# Filter to ensure only 2025 data\n",
    "transcripts['date'] = pd.to_datetime(transcripts['date'])\n",
    "transcripts = transcripts[transcripts['date'] >= '2025-01-01'].copy()\n",
    "\n",
    "print(f\"Loaded {len(transcripts)} transcripts for 2025\")\n",
    "print(f\"Date range: {transcripts['date'].min().date()} to {transcripts['date'].max().date()}\")\n",
    "\n",
    "macro = data.fetch_macro_data('2025-01-01', '2025-12-31')\n",
    "print(f\"Loaded {len(macro)} macro indicators\")\n",
    "sp500 = data.fetch_sp500_constituents()\n",
    "print(f\"Loaded {len(sp500)} S&P 500 stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fetch Macro Data (FRED API)\n",
    "\n",
    "**Note**: If you get FRED API errors, restart the kernel to reload the config with the updated API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fetched gdp: 3 observations\n",
      "âœ“ Fetched industrial_production: 12 observations\n",
      "âœ“ Fetched employment: 12 observations\n",
      "âœ“ Fetched wages: 12 observations\n",
      "\n",
      "Macroeconomic Data (2025):\n",
      "  gdp: 3 observations (filtered to 2025)\n",
      "  industrial_production: 12 observations (filtered to 2025)\n",
      "  employment: 12 observations (filtered to 2025)\n",
      "  wages: 12 observations (filtered to 2025)\n"
     ]
    }
   ],
   "source": [
    "# Fetch macroeconomic data for 2025 only\n",
    "start_date = '2025-01-01'\n",
    "end_date = '2025-12-31'\n",
    "macro_data = data_acq.fetch_macro_data(start_date, end_date)\n",
    "print(f\"\\nMacroeconomic Data (2025):\")\n",
    "for name, df in macro_data.items():\n",
    "    if len(df) > 0:\n",
    "        df_temp = df.copy()\n",
    "        df_temp['date'] = pd.to_datetime(df_temp['date'])\n",
    "        # Filter to 2025\n",
    "        df_2025 = df_temp[df_temp['date'] >= '2025-01-01']\n",
    "        print(f\"  {name}: {len(df_2025)} observations (filtered to 2025)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in PMI file: ['date', 'pmi']\n",
      "Loaded PMI data: 133 rows\n",
      "          date   pmi\n",
      "128 2015-05-01  51.5\n",
      "129 2015-04-01  51.5\n",
      "130 2015-03-02  52.9\n",
      "131 2015-02-02  53.5\n",
      "132 2015-01-02  55.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pmi_path = 'pmi_data.csv'\n",
    "pmi_df = pd.read_csv(pmi_path)\n",
    "pmi_df.columns = [c.strip().lower().replace(' ', '_') for c in pmi_df.columns]\n",
    "print(\"Columns in PMI file:\", pmi_df.columns.tolist())\n",
    "\n",
    "# Find date and PMI columns\n",
    "date_col = [col for col in pmi_df.columns if 'date' in col][0]\n",
    "pmi_col = [col for col in pmi_df.columns if 'pmi' in col][0]\n",
    "\n",
    "def clean_date(val):\n",
    "    # Extract the part before the first parenthesis\n",
    "    val = str(val).split('(')[0].strip()\n",
    "    try:\n",
    "        return pd.to_datetime(val)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "pmi_df[date_col] = pmi_df[date_col].apply(clean_date)\n",
    "pmi_df = pmi_df.dropna(subset=[date_col, pmi_col])\n",
    "\n",
    "# Rename columns to standard names\n",
    "pmi_df = pmi_df.rename(columns={date_col: 'date', pmi_col: 'pmi'})\n",
    "\n",
    "print(f\"Loaded PMI data: {len(pmi_df)} rows\")\n",
    "print(pmi_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered PMI data: 14 original rows\n",
      "Created daily PMI data: 396 rows (forward-filled)\n",
      "\n",
      "First few rows:\n",
      "        date   pmi\n",
      "0 2024-12-01  48.4\n",
      "1 2024-12-02  48.4\n",
      "2 2024-12-03  48.4\n",
      "3 2024-12-04  48.4\n",
      "4 2024-12-05  48.4\n",
      "5 2024-12-06  48.4\n",
      "6 2024-12-07  48.4\n",
      "7 2024-12-08  48.4\n",
      "8 2024-12-09  48.4\n",
      "9 2024-12-10  48.4\n",
      "\n",
      "Check for remaining NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "# Filter PMI from 2025 onwards and create daily index for merging\n",
    "pmi_df = pmi_df[pmi_df['date'] >= '2024-12-01'].copy()\n",
    "pmi_df = pmi_df.sort_values('date')\n",
    "\n",
    "# Create a complete daily date range for 2025\n",
    "date_range = pd.date_range(start='2024-12-01', end='2025-12-31', freq='D')\n",
    "pmi_daily = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Merge and forward-fill PMI values\n",
    "pmi_daily = pmi_daily.merge(pmi_df, on='date', how='left')\n",
    "# Use both forward-fill and backward-fill to handle initial NaN values\n",
    "pmi_daily['pmi'] = pmi_daily['pmi'].ffill().bfill()\n",
    "\n",
    "print(f\"Filtered PMI data: {len(pmi_df)} original rows\")\n",
    "print(f\"Created daily PMI data: {len(pmi_daily)} rows (forward-filled)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(pmi_daily.head(10))\n",
    "print(f\"\\nCheck for remaining NaN values: {pmi_daily['pmi'].isna().sum()}\")\n",
    "\n",
    "# Use the daily PMI data for merging\n",
    "pmi_df = pmi_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI values at monthly transitions:\n",
      "  2025-01-03: 49.3\n",
      "  2025-02-03: 50.9\n",
      "  2025-03-03: 50.3\n",
      "  2025-04-01: 49.0\n",
      "  2025-05-01: 48.7\n",
      "  2025-06-02: 48.5\n",
      "  2025-07-01: 49.0\n",
      "  2025-08-01: 48.0\n",
      "\n",
      "Unique PMI values in 2025: [np.float64(48.0), np.float64(48.2), np.float64(48.4), np.float64(48.5), np.float64(48.7), np.float64(49.0), np.float64(49.1), np.float64(49.3), np.float64(50.3), np.float64(50.9)]\n",
      "This is correct - PMI is monthly, so each value repeats until the next release\n"
     ]
    }
   ],
   "source": [
    "# Verify PMI values change throughout the year (show monthly transitions)\n",
    "print(\"PMI values at monthly transitions:\")\n",
    "sample_dates = ['2025-01-03', '2025-02-03', '2025-03-03', '2025-04-01', \n",
    "                '2025-05-01', '2025-06-02', '2025-07-01', '2025-08-01']\n",
    "for date in sample_dates:\n",
    "    value = pmi_df[pmi_df['date'] == date]['pmi'].values\n",
    "    if len(value) > 0:\n",
    "        print(f\"  {date}: {value[0]}\")\n",
    "\n",
    "print(f\"\\nUnique PMI values in 2025: {sorted(pmi_df['pmi'].unique())}\")\n",
    "print(f\"This is correct - PMI is monthly, so each value repeats until the next release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fetched yield curve slope\n",
      "âœ“ Fetched consumer sentiment\n",
      "âœ“ Fetched unemployment rate\n",
      "âœ— No local PMI data provided; PMI not included in controls.\n",
      "âœ“ Control variables: 12 observations\n",
      "\n",
      "Control Variables: 12 observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yield_curve_slope</th>\n",
       "      <th>consumer_sentiment</th>\n",
       "      <th>unemployment_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>0.36</td>\n",
       "      <td>71.7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-01</th>\n",
       "      <td>0.24</td>\n",
       "      <td>64.7</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-01</th>\n",
       "      <td>0.31</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-01</th>\n",
       "      <td>0.50</td>\n",
       "      <td>52.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-01</th>\n",
       "      <td>0.50</td>\n",
       "      <td>52.2</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            yield_curve_slope  consumer_sentiment  unemployment_rate\n",
       "2025-01-01               0.36                71.7                4.0\n",
       "2025-02-01               0.24                64.7                4.2\n",
       "2025-03-01               0.31                57.0                4.2\n",
       "2025-04-01               0.50                52.2                4.2\n",
       "2025-05-01               0.50                52.2                4.3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch control variables\n",
    "controls = data_acq.fetch_control_variables(start_date, end_date)\n",
    "print(f\"\\nControl Variables: {len(controls)} observations\")\n",
    "controls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fetched yield curve slope\n",
      "âœ“ Fetched consumer sentiment\n",
      "âœ“ Fetched unemployment rate\n",
      "âœ“ Used local PMI data: 365 rows\n",
      "âœ“ Control variables: 365 observations\n"
     ]
    }
   ],
   "source": [
    "data_acq.pmi_df = pmi_df\n",
    "controls = data_acq.fetch_control_variables(start_date, end_date, pmi_df=pmi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename index column to date\n",
    "controls = controls.reset_index().rename(columns={'index': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled to monthly: 12 observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>yield_curve_slope</th>\n",
       "      <th>consumer_sentiment</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>0.36</td>\n",
       "      <td>71.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>49.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>0.24</td>\n",
       "      <td>64.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>50.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>0.31</td>\n",
       "      <td>57.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>50.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>0.50</td>\n",
       "      <td>52.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>0.50</td>\n",
       "      <td>52.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>48.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>60.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>48.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-07-31</td>\n",
       "      <td>0.51</td>\n",
       "      <td>61.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-08-31</td>\n",
       "      <td>0.56</td>\n",
       "      <td>58.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>0.55</td>\n",
       "      <td>55.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>48.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>0.54</td>\n",
       "      <td>53.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>49.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>0.54</td>\n",
       "      <td>51.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>48.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>0.64</td>\n",
       "      <td>52.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  yield_curve_slope  consumer_sentiment  unemployment_rate   pmi\n",
       "0  2025-01-31               0.36                71.7                4.0  49.3\n",
       "1  2025-02-28               0.24                64.7                4.2  50.9\n",
       "2  2025-03-31               0.31                57.0                4.2  50.3\n",
       "3  2025-04-30               0.50                52.2                4.2  49.0\n",
       "4  2025-05-31               0.50                52.2                4.3  48.7\n",
       "5  2025-06-30               0.49                60.7                4.1  48.5\n",
       "6  2025-07-31               0.51                61.7                4.3  49.0\n",
       "7  2025-08-31               0.56                58.2                4.3  48.0\n",
       "8  2025-09-30               0.55                55.1                4.4  48.7\n",
       "9  2025-10-31               0.54                53.6                4.4  49.1\n",
       "10 2025-11-30               0.54                51.0                4.5  48.7\n",
       "11 2025-12-31               0.64                52.9                4.4  48.2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the index column date\n",
    "controls = controls.groupby(controls['date'].dt.to_period('M')).last().reset_index(drop=True)\n",
    "controls['date'] = pd.to_datetime(controls['date'].astype(str))\n",
    "print(f\"Resampled to monthly: {len(controls)} observations\")\n",
    "controls.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LLM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM scorer\n",
    "scorer = LLMScorer('config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AGG score aggregation function defined\n"
     ]
    }
   ],
   "source": [
    "def aggregate_scores_by_quarter(scored_transcripts):\n",
    "    \"\"\"\n",
    "    Aggregate individual transcript scores into quarterly AGG scores.\n",
    "    \n",
    "    Args:\n",
    "        scored_transcripts: List of dicts with 'symbol', 'date', 'score', 'market_cap'\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quarterly AGG scores\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(scored_transcripts)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['quarter_date'] = df['date'].dt.to_period('Q').dt.to_timestamp()\n",
    "    \n",
    "    # Aggregate by quarter using value-weighted average\n",
    "    quarterly = df.groupby('quarter_date').apply(\n",
    "        lambda x: np.average(x['score'], weights=x.get('market_cap', [1]*len(x)))\n",
    "    ).reset_index()\n",
    "    \n",
    "    quarterly.columns = ['date', 'agg_score']\n",
    "    quarterly['year'] = quarterly['date'].dt.year\n",
    "    quarterly['quarter'] = quarterly['date'].dt.quarter\n",
    "    \n",
    "    return quarterly[['date', 'year', 'quarter', 'agg_score']]\n",
    "\n",
    "# Example usage (commented out - requires real transcript scores):\n",
    "# scored_transcripts = scorer.score_multiple_transcripts(transcripts)\n",
    "# agg_scores = aggregate_scores_by_quarter(scored_transcripts)\n",
    "# agg_scores.to_csv('agg_scores.csv', index=False)\n",
    "print(\"âœ“ AGG score aggregation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to score 919 transcripts\n",
      "Date range: 2025-01-10 to 2025-05-15\n",
      "Estimated cost: $0.92 - $1.84\n",
      "Estimated time: 30.6 - 46.0 minutes\n",
      "Results will be saved to: scored_transcripts_2025_2025.csv\n",
      "\n",
      "Transcripts by year:\n",
      "  2025: 919 transcripts\n",
      "\n",
      "Checkpoints will be saved every 50 transcripts\n"
     ]
    }
   ],
   "source": [
    "# Prepare transcripts for scoring (uses whatever was fetched based on config.yaml)\n",
    "if 'transcripts' not in dir() or len(transcripts) == 0:\n",
    "    print(\"ERROR: No transcripts loaded!\")\n",
    "    print(\"Please run the data acquisition cell first to load transcripts.\")\n",
    "    raise ValueError(\"Transcripts not loaded. Run data acquisition cell first.\")\n",
    "\n",
    "# Use the loaded transcripts\n",
    "scoring_transcripts = transcripts.copy()\n",
    "scoring_transcripts['date'] = pd.to_datetime(scoring_transcripts['date'])\n",
    "\n",
    "# Generate save path based on date range\n",
    "date_min = scoring_transcripts['date'].min()\n",
    "date_max = scoring_transcripts['date'].max()\n",
    "year_range = f\"{date_min.year}_{date_max.year}\"\n",
    "save_path = f'scored_transcripts_{year_range}.csv'\n",
    "\n",
    "print(f\"Preparing to score {len(scoring_transcripts)} transcripts\")\n",
    "print(f\"Date range: {date_min.date()} to {date_max.date()}\")\n",
    "print(f\"Estimated cost: ${len(scoring_transcripts) * 0.001:.2f} - ${len(scoring_transcripts) * 0.002:.2f}\")\n",
    "print(f\"Estimated time: {len(scoring_transcripts) * 2 / 60:.1f} - {len(scoring_transcripts) * 3 / 60:.1f} minutes\")\n",
    "print(f\"Results will be saved to: {save_path}\")\n",
    "\n",
    "# Show breakdown by year\n",
    "year_counts = scoring_transcripts['date'].dt.year.value_counts().sort_index()\n",
    "print(f\"\\nTranscripts by year:\")\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  {year}: {count} transcripts\")\n",
    "\n",
    "print(f\"\\nCheckpoints will be saved every 50 transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring function ready\n"
     ]
    }
   ],
   "source": [
    "# Define the scoring function with progress tracking\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def score_quarter_transcripts(transcripts_df, scorer, save_path='scored_transcripts.csv'):\n",
    "    \"\"\"\n",
    "    Score all transcripts with progress tracking, checkpointing, and error handling.\n",
    "    \"\"\"\n",
    "    # First, inspect the data structure\n",
    "    print(\"Inspecting data structure...\")\n",
    "    print(f\"Type: {type(transcripts_df)}\")\n",
    "    print(f\"Columns: {transcripts_df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst row type: {type(transcripts_df.iloc[0])}\")\n",
    "    print(f\"First row preview:\")\n",
    "    print(transcripts_df.iloc[0])\n",
    "    \n",
    "    print(f\"\\nScoring {len(transcripts_df)} transcripts...\")\n",
    "    print(f\"Estimated cost: ${len(transcripts_df) * 0.001:.2f} (GPT-4o-mini)\")\n",
    "    print(f\"Estimated time: {len(transcripts_df) * 2 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Check for existing progress\n",
    "    try:\n",
    "        existing = pd.read_csv(save_path)\n",
    "        already_scored = set(existing['symbol'] + '_' + existing['date'].astype(str))\n",
    "        print(f\"Found {len(already_scored)} previously scored transcripts\")\n",
    "    except FileNotFoundError:\n",
    "        already_scored = set()\n",
    "        existing = pd.DataFrame()\n",
    "    \n",
    "    scored_results = []\n",
    "    errors = []\n",
    "    \n",
    "    # Determine transcript column name - check what's actually in the DataFrame\n",
    "    available_cols = transcripts_df.columns.tolist()\n",
    "    transcript_col = None\n",
    "    \n",
    "    for possible_name in ['transcript', 'text', 'content', 'full_text', 'body']:\n",
    "        if possible_name in available_cols:\n",
    "            transcript_col = possible_name\n",
    "            break\n",
    "    \n",
    "    if transcript_col is None:\n",
    "        print(f\"ERROR: Could not find transcript column. Available columns: {available_cols}\")\n",
    "        return existing if len(existing) > 0 else pd.DataFrame()\n",
    "    \n",
    "    print(f\"Using transcript column: '{transcript_col}'\")\n",
    "    \n",
    "    # Convert to dict records for easier iteration\n",
    "    records = transcripts_df.to_dict('records')\n",
    "    \n",
    "    for idx, row in enumerate(tqdm(records, desc=\"Scoring\")):\n",
    "        # Handle different possible column names\n",
    "        symbol = row.get('symbol') or row.get('ticker') or 'UNKNOWN'\n",
    "        date = row.get('date') or row.get('filing_date') or 'UNKNOWN'\n",
    "        transcript_id = f\"{symbol}_{date}\"\n",
    "        \n",
    "        # Skip if already scored\n",
    "        if transcript_id in already_scored:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get the transcript text\n",
    "            transcript_text = row.get(transcript_col, '')\n",
    "            \n",
    "            if not transcript_text or transcript_text == '':\n",
    "                errors.append({'symbol': symbol, 'date': date, 'error': 'Empty transcript'})\n",
    "                continue\n",
    "            \n",
    "            # Score transcript - wrap in expected dictionary format\n",
    "            # The scorer expects a dict with 'full_text' key\n",
    "            transcript_dict = {'full_text': transcript_text}\n",
    "            result = scorer.score_transcript(transcript_dict, use_md_a_only=False)\n",
    "            score = result['firm_score']\n",
    "            \n",
    "            if score is None:\n",
    "                errors.append({'symbol': symbol, 'date': date, 'error': 'Scoring returned None'})\n",
    "                continue\n",
    "            \n",
    "            scored_results.append({\n",
    "                'symbol': symbol,\n",
    "                'date': date,\n",
    "                'score': score,\n",
    "                'transcript_length': len(str(transcript_text))\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint every 50 transcripts\n",
    "            if len(scored_results) % 50 == 0:\n",
    "                temp_df = pd.DataFrame(scored_results)\n",
    "                combined = pd.concat([existing, temp_df], ignore_index=True)\n",
    "                combined.to_csv(save_path, index=False)\n",
    "                print(f\"\\nCheckpoint: Saved {len(combined)} scores\")\n",
    "            \n",
    "            # Rate limiting (to avoid API limits)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append({'symbol': symbol, 'date': date, 'error': str(e)})\n",
    "            if idx < 5:  # Only print first few errors in detail\n",
    "                print(f\"\\nError scoring {symbol}: {e}\")\n",
    "    \n",
    "    # Final save - handle case where nothing was scored\n",
    "    if scored_results:\n",
    "        final_df = pd.DataFrame(scored_results)\n",
    "        combined = pd.concat([existing, final_df], ignore_index=True)\n",
    "        combined.to_csv(save_path, index=False)\n",
    "        print(f\"\\nSaved {len(combined)} total scored transcripts to {save_path}\")\n",
    "    elif len(existing) > 0:\n",
    "        combined = existing\n",
    "        print(f\"\\nNo new transcripts scored. Returning {len(existing)} existing scores.\")\n",
    "    else:\n",
    "        combined = pd.DataFrame(columns=['symbol', 'date', 'score', 'transcript_length'])\n",
    "        print(\"\\nWARNING: No transcripts were scored successfully!\")\n",
    "    \n",
    "    if errors:\n",
    "        error_df = pd.DataFrame(errors)\n",
    "        error_df.to_csv('scoring_errors.csv', index=False)\n",
    "        print(f\"\\nWARNING: {len(errors)} errors occurred (saved to scoring_errors.csv)\")\n",
    "        print(f\"First few unique errors:\")\n",
    "        unique_errors = error_df['error'].value_counts().head(3)\n",
    "        for error_msg, count in unique_errors.items():\n",
    "            print(f\"  {error_msg}: {count} occurrences\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"Scoring function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current chunk size: 2000 characters\n",
      "Estimated chunks per transcript: 50 (for 100k char transcript)\n",
      "\n",
      "âœ“ Updated chunk size to: 10000 characters\n",
      "âœ“ New estimated chunks: 10 per transcript\n",
      "âœ“ This will make scoring ~5x faster!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Check current chunk size\n",
    "print(f\"Current chunk size: {scorer.llm_config.get('chunk_size', 'not set')} characters\")\n",
    "print(f\"Estimated chunks per transcript: {100000 // scorer.llm_config.get('chunk_size', 2000)} (for 100k char transcript)\")\n",
    "\n",
    "# Increase chunk size for faster scoring (reduces API calls by 4-6x)\n",
    "scorer.llm_config['chunk_size'] = 10000  # Increase from 2000 to 10000\n",
    "\n",
    "print(f\"\\nâœ“ Updated chunk size to: {scorer.llm_config['chunk_size']} characters\")\n",
    "print(f\"âœ“ New estimated chunks: {100000 // scorer.llm_config['chunk_size']} per transcript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scoring at 2026-02-02 23:07:35\n",
      "======================================================================\n",
      "Inspecting data structure...\n",
      "Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Columns: ['symbol', 'quarter', 'year', 'date', 'content', 'structured_content', 'company_name', 'company_id']\n",
      "\n",
      "First row type: <class 'pandas.core.series.Series'>\n",
      "First row preview:\n",
      "symbol                                                                A\n",
      "quarter                                                               1\n",
      "year                                                               2025\n",
      "date                                                2025-02-26 16:30:00\n",
      "content               Operator: Good afternoon. My name is Regina, a...\n",
      "structured_content    [{'speaker': 'Operator', 'text': 'Good afterno...\n",
      "company_name                                 Agilent Technologies, Inc.\n",
      "company_id                                                     154924.0\n",
      "Name: 20, dtype: object\n",
      "\n",
      "Scoring 919 transcripts...\n",
      "Estimated cost: $0.92 (GPT-4o-mini)\n",
      "Estimated time: 30.6 minutes\n",
      "Using transcript column: 'content'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e14f156505439583b2cda6a59a4415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/919 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run scoring (make sure you've run the previous cells first)\n",
    "if 'scoring_transcripts' not in dir() or 'save_path' not in dir():\n",
    "    print(\"ERROR: Please run the previous cell to prepare transcripts first.\")\n",
    "    raise NameError(\"Run the transcript preparation cell first\")\n",
    "\n",
    "print(f\"Starting scoring at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scored_data = score_quarter_transcripts(\n",
    "    scoring_transcripts, \n",
    "    scorer, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scoring results\n",
    "if 'scored_data' in dir() and len(scored_data) > 0:\n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Total scored: {len(scored_data)}\")\n",
    "    print(f\"  Date range: {scored_data['date'].min()} to {scored_data['date'].max()}\")\n",
    "    print(f\"  Average score: {scored_data['score'].mean():.2f}\")\n",
    "    print(f\"  Score distribution:\")\n",
    "    print(scored_data['score'].value_counts().sort_index())\n",
    "    print(f\"\\nSaved to: {save_path}\")\n",
    "else:\n",
    "    print(\"No scored data available. Run the scoring cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate scored transcripts into quarterly AGG scores\n",
    "if 'scored_data' not in dir() or len(scored_data) == 0:\n",
    "    print(\"ERROR: No scored data available. Run the scoring cell first.\")\n",
    "else:\n",
    "    print(\"Aggregating individual scores into quarterly AGG scores...\")\n",
    "    \n",
    "    # Convert to DataFrame if needed\n",
    "    if isinstance(scored_data, pd.DataFrame):\n",
    "        scored_df = scored_data.copy()\n",
    "    else:\n",
    "        scored_df = pd.DataFrame(scored_data)\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    scored_df['date'] = pd.to_datetime(scored_df['date'])\n",
    "    scored_df['year'] = scored_df['date'].dt.year\n",
    "    scored_df['quarter'] = scored_df['date'].dt.quarter\n",
    "    \n",
    "    # Group by quarter and calculate aggregate score\n",
    "    agg_scores = scored_df.groupby(['year', 'quarter']).agg({\n",
    "        'score': ['mean', 'std', 'count']\n",
    "    }).reset_index()\n",
    "    \n",
    "    agg_scores.columns = ['year', 'quarter', 'agg_score', 'score_std', 'num_firms']\n",
    "    \n",
    "    # Create quarter date\n",
    "    agg_scores['date'] = pd.to_datetime(\n",
    "        agg_scores['year'].astype(str) + '-Q' + agg_scores['quarter'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # Reorder columns\n",
    "    final_agg_scores = agg_scores[['date', 'year', 'quarter', 'agg_score', 'score_std', 'num_firms']]\n",
    "    \n",
    "    # Save AGG scores (filename based on date range)\n",
    "    agg_filename = f'agg_scores_{year_range}.csv'\n",
    "    final_agg_scores.to_csv(agg_filename, index=False)\n",
    "    print(f\"\\nSUCCESS: Saved {len(final_agg_scores)} quarterly AGG scores to {agg_filename}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nAGG Scores Summary:\")\n",
    "    print(final_agg_scores)\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Quarters covered: {len(final_agg_scores)}\")\n",
    "    print(f\"  Date range: {final_agg_scores['date'].min().strftime('%Y-%m-%d')} to {final_agg_scores['date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  Mean AGG score: {final_agg_scores['agg_score'].mean():.3f}\")\n",
    "    print(f\"  Std AGG score: {final_agg_scores['agg_score'].std():.3f}\")\n",
    "    print(f\"  Average firms/quarter: {final_agg_scores['num_firms'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer('config.yaml')\n",
    "\n",
    "# Load real AGG scores from saved file or create from actual transcript scoring\n",
    "try:\n",
    "    agg_scores = pd.read_csv('agg_scores.csv')\n",
    "    agg_scores['date'] = pd.to_datetime(agg_scores['date'])\n",
    "    print(f\"âœ“ Loaded real AGG scores from file: {len(agg_scores)} quarters\")\n",
    "    print(agg_scores.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  No saved AGG scores found. You need to:\")\n",
    "    print(\"  1. Score earnings transcripts using LLMScorer.score_multiple_transcripts()\")\n",
    "    print(\"  2. Aggregate scores by quarter using aggregate_scores_by_quarter()\")\n",
    "    print(\"  3. Save to 'agg_scores.csv'\")\n",
    "    print(\"\\n For demonstration, showing expected data structure...\")\n",
    "    # Show expected structure instead of generating synthetic data\n",
    "    agg_scores = pd.DataFrame({\n",
    "        'date': pd.date_range(start='2015-01-01', end='2023-12-31', freq='Q'),\n",
    "        'year': [],\n",
    "        'quarter': [],\n",
    "        'agg_score': []  # Real scores would be 1-5 from LLM\n",
    "    })\n",
    "    print(\"\\nExpected columns: date, year, quarter, agg_score\")\n",
    "    print(\"Cannot proceed with feature engineering without real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize scores (only if we have real data)\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    normalized = engineer.normalize_scores(agg_scores, method='zscore', window=20)\n",
    "    print(\"\\nNormalized Scores:\")\n",
    "\n",
    "    print(normalized[['date', 'agg_score', 'agg_score_norm']].head(10))    normalized = pd.DataFrame()\n",
    "\n",
    "else:    print(\"âš  Cannot normalize without real AGG scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delta features (only if we have normalized data)\n",
    "if len(normalized) > 0:\n",
    "    with_deltas = engineer.create_delta_features(normalized)\n",
    "    print(\"\\nDelta Features:\")\n",
    "\n",
    "    print(with_deltas[['date', 'agg_score', 'yoy_change', 'qoq_change', 'momentum']].tail(10))    with_deltas = pd.DataFrame()\n",
    "\n",
    "else:    print(\"âš  Cannot create delta features without normalized scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AGG score and deltas (only if we have features)\n",
    "if len(with_deltas) > 0:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "    # AGG score\n",
    "    axes[0].plot(with_deltas['date'], with_deltas['agg_score'], linewidth=2)\n",
    "    axes[0].set_title('AGG Score (National Economic Sentiment)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # YoY change\n",
    "    valid_yoy = with_deltas.dropna(subset=['yoy_change'])\n",
    "    axes[1].bar(valid_yoy['date'], valid_yoy['yoy_change'], color='steelblue', alpha=0.7)\n",
    "    axes[1].set_title('YoY Change (AGG_t - AGG_t-4)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Change')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Momentum\n",
    "    valid_momentum = with_deltas.dropna(subset=['momentum'])\n",
    "    axes[2].bar(valid_momentum['date'], valid_momentum['momentum'], color='coral', alpha=0.7)\n",
    "    axes[2].set_title('Momentum (Acceleration)', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Momentum')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"âœ“ Feature visualization complete\")\n",
    "else:\n",
    "    print(\"âš  Cannot visualize features without delta features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = PredictionModel('config.yaml')\n",
    "print(dir(pred_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna().reset_index(drop=True)\n",
    "X_train['date'] = with_deltas.loc[X_train.index, 'date'].values\n",
    "\n",
    "gdp_df = macro_data['gdp'].copy()\n",
    "gdp_df['date'] = pd.to_datetime(gdp_df['date'])\n",
    "train_data = X_train.merge(gdp_df, on='date', how='inner')\n",
    "X_train = train_data[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].values\n",
    "y_train = train_data['value'].values\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Target data: {y_train.shape}\")\n",
    "gdp_models = pred_model.train_gdp_models(X_train, y_train)\n",
    "print(f\"Model RÂ²: {gdp_models['gdp'].score(X_train, y_train):.3f}\")\n",
    "gdp_model = pred_model.train_gdp_model(X_train.values, y_train.values)\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Target data: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GDP prediction model\n",
    "gdp_model = pred_model.train_gdp_model(X_train, y_train)\n",
    "print(f\"\\nGDP Model Trained\")\n",
    "print(f\"  Model type: {type(gdp_model).__name__}\")\n",
    "print(f\"  Training RÂ²: {gdp_model.score(X_train, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using real test data\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    # Use the most recent features for out-of-sample prediction\n",
    "    test_features = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna().tail(10)\n",
    "    test_dates = with_deltas.loc[test_features.index, 'date']\n",
    "    \n",
    "    predictions = gdp_model.predict(test_features.values)\n",
    "\n",
    "    print(f\"\\nGDP Predictions (1Q ahead) for recent quarters:\")\n",
    "    for date, pred in zip(test_dates, predictions):\n",
    "        print(f\"  {date.strftime('%Y-%m-%d')}: {pred:.3f}%\")\n",
    "    print(f\"\\n  Mean: {predictions.mean():.3f}%\")\n",
    "    print(f\"  Std: {predictions.std():.3f}%\")\n",
    "    print(f\"  Range: [{predictions.min():.3f}, {predictions.max():.3f}]%\")\n",
    "else:\n",
    "    print(\"âš  Cannot make predictions without real AGG scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Signal Generation & Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize signal generator\n",
    "signal_gen = SignalGenerator('config.yaml')\n",
    "\n",
    "# Use real predictions from trained models\n",
    "# This requires: \n",
    "# 1. Features from AGG scores\n",
    "# 2. Trained GDP/IP models\n",
    "# 3. SPF forecasts from data_acq.fetch_spf_forecasts()\n",
    "\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    # Use real model predictions\n",
    "    features_for_pred = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna()\n",
    "    dates_for_pred = with_deltas.loc[features_for_pred.index, 'date']\n",
    "    \n",
    "\n",
    "    # Get predictions from trained model    predictions_df = pd.DataFrame()\n",
    "\n",
    "    gdp_predictions = gdp_model.predict(features_for_pred.values)    print(\"âš  Cannot generate predictions without real AGG scores\")\n",
    "\n",
    "    else:\n",
    "\n",
    "    # Fetch real SPF forecasts    print(predictions_df.head())\n",
    "\n",
    "    try:    print(\"âœ“ Real Predictions vs SPF:\")\n",
    "\n",
    "        spf_data = data_acq.fetch_spf_forecasts(start_date, end_date)    \n",
    "\n",
    "        spf_data['date'] = pd.to_datetime(spf_data['date'])    predictions_df.rename(columns={'rgdp_1q': 'gdp_spf'}, inplace=True)\n",
    "\n",
    "    except Exception as e:    predictions_df = predictions_df.merge(spf_data[['date', 'rgdp_1q']], on='date', how='left')\n",
    "\n",
    "        print(f\"âš  Could not fetch SPF data: {e}\")    })\n",
    "\n",
    "        spf_data = pd.DataFrame({'date': dates_for_pred, 'rgdp_1q': [2.0]*len(dates_for_pred)})        'gdp_pred': gdp_predictions\n",
    "\n",
    "            'date': dates_for_pred.values,\n",
    "\n",
    "    # Combine predictions with SPF    predictions_df = pd.DataFrame({"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trading signals (only if we have real predictions)\n",
    "if len(predictions_df) > 0:\n",
    "    signals = signal_gen.generate_signals(predictions_df)\n",
    "    print(f\"\\nðŸ“Š Trading Signals Generated:\")\n",
    "    print(signals.head(10))\n",
    "    print(f\"\\nSignal distribution:\")\n",
    "    print(signals['signal'].value_counts())\n",
    "else:\n",
    "    print(\"âš  Cannot generate signals without predictions\")\n",
    "    signals = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize backtester\n",
    "backtester = Backtester('config.yaml')\n",
    "\n",
    "# Use real returns from strategy execution\n",
    "# This requires:\n",
    "# 1. Trading signals from signal_gen.generate_signals()\n",
    "# 2. Sector ETF price data\n",
    "# 3. Portfolio construction and rebalancing\n",
    "\n",
    "if len(predictions_df) > 0:\n",
    "    # Fetch real ETF price data for sectors\n",
    "    sector_etfs = config['strategy']['sector_etfs']\n",
    "    etf_start = config['backtest']['test_start']\n",
    "    etf_end = config['backtest']['test_end']\n",
    "    \n",
    "    etf_prices = data_acq.fetch_etf_prices(sector_etfs, etf_start, etf_end)\n",
    "    \n",
    "    if etf_prices:\n",
    "        print(f\"âœ“ Fetched price data for {len(etf_prices)} sector ETFs\")\n",
    "\n",
    "                    print(f\"  {metric}: {value}\")\n",
    "\n",
    "        # Run backtest with real data        else:\n",
    "\n",
    "        # Note: This requires implementing the full backtesting logic            print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "        # For now, we show the structure        if isinstance(value, float):\n",
    "\n",
    "        print(\"\\nâš  Full backtest execution requires:\")    for metric, value in metrics.items():\n",
    "\n",
    "        print(\"  1. Signals from signal_gen.generate_signals(predictions_df)\")    print(f\"\\nðŸ“ˆ Performance Metrics:\")\n",
    "\n",
    "        print(\"  2. Portfolio construction based on signals\")    metrics = backtester.calculate_metrics(portfolio_returns)\n",
    "\n",
    "        print(\"  3. Daily rebalancing and return calculation\")    # Calculate performance metrics\n",
    "\n",
    "        print(\"  4. Benchmark comparison (SPY or equal-weight)\")if len(portfolio_returns) > 0:\n",
    "\n",
    "        \n",
    "\n",
    "        portfolio_returns = pd.DataFrame()    portfolio_returns = pd.DataFrame()\n",
    "\n",
    "        print(\"\\nPlease implement backtester.run_backtest(signals, etf_prices) for real returns\")    print(\"âš  Cannot run backtest without predictions\")\n",
    "\n",
    "    else:else:\n",
    "\n",
    "        print(\"âš  No ETF price data available\")        portfolio_returns = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns and plot (only if we have real returns)\n",
    "if len(portfolio_returns) > 0 and 'strategy_return' in portfolio_returns.columns:\n",
    "    portfolio_returns['strategy_cumret'] = (1 + portfolio_returns['strategy_return']).cumprod() - 1\n",
    "    portfolio_returns['benchmark_cumret'] = (1 + portfolio_returns['benchmark_return']).cumprod() - 1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(portfolio_returns['date'], portfolio_returns['strategy_cumret'] * 100, \n",
    "            label='Strategy', linewidth=2)\n",
    "    ax.plot(portfolio_returns['date'], portfolio_returns['benchmark_cumret'] * 100, \n",
    "            label='Benchmark', linewidth=2, linestyle='--')\n",
    "\n",
    "    ax.set_title('Strategy vs Benchmark Cumulative Returns', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Return (%)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"âœ“ Backtest visualization complete\")    print(\"5. Execute backtest with real ETF prices\")\n",
    "\n",
    "else:    print(\"4. Generate trading signals\")\n",
    "\n",
    "    print(\"âš  No portfolio returns available for visualization\")    print(\"3. Train prediction models\")\n",
    "\n",
    "    print(\"\\nTo complete the full pipeline with real data:\")    print(\"2. Engineer features from AGG scores\")\n",
    "    print(\"1. Score earnings transcripts â†’ agg_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete Pipeline with Real Data\n",
    "\n",
    "This notebook demonstrates the **AI Economy Score Predictor** strategy pipeline using **real data sources**:\n",
    "\n",
    "### âœ… Real Data Used:\n",
    "1. **Macroeconomic Data**: From FRED API (GDP, Industrial Production, Employment, Wages)\n",
    "2. **Control Variables**: From FRED API (Yield Curve, Consumer Sentiment, Unemployment)\n",
    "3. **PMI Data**: Loaded from `pmi_data.csv` \n",
    "4. **S&P 500 Constituents**: From `constituents.csv`\n",
    "5. **ETF Prices**: Fetched via yfinance API\n",
    "\n",
    "### âš ï¸ Real Data Needed:\n",
    "- **Earnings Call Transcripts** with LLM sentiment scores aggregated quarterly â†’ `agg_scores.csv`\n",
    "\n",
    "### Pipeline Steps:\n",
    "1. **Data Acquisition** âœ“ Uses real FRED API and local files\n",
    "2. **LLM Scoring** â†’ Requires real earnings transcripts (Seeking Alpha, CapIQ, Bloomberg)\n",
    "3. **Feature Engineering** âœ“ Works with real AGG scores once available\n",
    "4. **Prediction Models** âœ“ Trains on real macro data + AGG features\n",
    "5. **Signal Generation** âœ“ Compares predictions to SPF forecasts\n",
    "6. **Backtesting** âœ“ Uses real sector ETF prices\n",
    "\n",
    "### Next Steps:\n",
    "1. Obtain earnings call transcripts from a data provider\n",
    "2. Score transcripts using `LLMScorer.score_multiple_transcripts()`\n",
    "3. Aggregate scores by quarter and save to `agg_scores.csv`\n",
    "4. Re-run this notebook to execute the full pipeline with real signals\n",
    "\n",
    "**No synthetic/random data is used for actual trading signals - all results require real transcript scoring.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data availability\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“ Data File Status:\\n\")\n",
    "\n",
    "required_files = {\n",
    "    'config.yaml': 'Configuration file',\n",
    "    'constituents.csv': 'S&P 500 constituents',\n",
    "    'pmi_data.csv': 'PMI data'\n",
    "}\n",
    "\n",
    "optional_files = {\n",
    "    'agg_scores.csv': 'Aggregated LLM sentiment scores (REQUIRED for full pipeline)'\n",
    "}\n",
    "\n",
    "for file, desc in required_files.items():\n",
    "    status = \"âœ“\" if os.path.exists(file) else \"âœ—\"\n",
    "    print(f\"{status} {file}: {desc}\")\n",
    "\n",
    "print(\"\\nOptional (but critical):\")\n",
    "for file, desc in optional_files.items():\n",
    "    status = \"âœ“\" if os.path.exists(file) else \"âœ— MISSING\"\n",
    "    print(f\"{status} {file}: {desc}\")\n",
    "\n",
    "if not os.path.exists('agg_scores.csv'):\n",
    "    print(\"\\nâš ï¸  To create agg_scores.csv, you need to:\")\n",
    "    print(\"   1. Get earnings transcripts from a data provider\")\n",
    "    print(\"   2. Run LLM scoring (see 'Note: To Use Real Data' section above)\")\n",
    "    print(\"   3. Use the aggregate_scores_by_quarter() function\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
