{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Economy Score Predictor - Full Pipeline\n",
    "\n",
    "Complete end-to-end implementation of the earnings call sentiment â†’ economic prediction â†’ trading strategy pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Pipeline modules loaded\n",
      "âœ“ Config loaded: 9 sections\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from data_acquisition import DataAcquisition\n",
    "from llm_scorer import LLMScorer\n",
    "from feature_engineering import FeatureEngineer\n",
    "from prediction_model import PredictionModel\n",
    "from signal_generator import SignalGenerator\n",
    "from backtester import Backtester\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ“ Pipeline modules loaded\")\n",
    "print(f\"âœ“ Config loaded: {len(config)} sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FRED API initialized\n",
      "âœ“ Loaded 503 S&P 500 constituents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ADBE</td>\n",
       "      <td>Adobe Inc.</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Application Software</td>\n",
       "      <td>San Jose, California</td>\n",
       "      <td>1997-05-05</td>\n",
       "      <td>796343</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMD</td>\n",
       "      <td>Advanced Micro Devices</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>2017-03-20</td>\n",
       "      <td>2488</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AES</td>\n",
       "      <td>AES Corporation</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Independent Power Producers &amp; Energy Traders</td>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>1998-10-02</td>\n",
       "      <td>874761</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AFL</td>\n",
       "      <td>Aflac</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Life &amp; Health Insurance</td>\n",
       "      <td>Columbus, Georgia</td>\n",
       "      <td>1999-05-28</td>\n",
       "      <td>4977</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Life Sciences Tools &amp; Services</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>2000-06-05</td>\n",
       "      <td>1090872</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol                Security             GICS Sector  \\\n",
       "0    MMM                      3M             Industrials   \n",
       "1    AOS             A. O. Smith             Industrials   \n",
       "2    ABT     Abbott Laboratories             Health Care   \n",
       "3   ABBV                  AbbVie             Health Care   \n",
       "4    ACN               Accenture  Information Technology   \n",
       "5   ADBE              Adobe Inc.  Information Technology   \n",
       "6    AMD  Advanced Micro Devices  Information Technology   \n",
       "7    AES         AES Corporation               Utilities   \n",
       "8    AFL                   Aflac              Financials   \n",
       "9      A    Agilent Technologies             Health Care   \n",
       "\n",
       "                              GICS Sub-Industry    Headquarters Location  \\\n",
       "0                      Industrial Conglomerates    Saint Paul, Minnesota   \n",
       "1                             Building Products     Milwaukee, Wisconsin   \n",
       "2                         Health Care Equipment  North Chicago, Illinois   \n",
       "3                                 Biotechnology  North Chicago, Illinois   \n",
       "4                IT Consulting & Other Services          Dublin, Ireland   \n",
       "5                          Application Software     San Jose, California   \n",
       "6                                Semiconductors  Santa Clara, California   \n",
       "7  Independent Power Producers & Energy Traders      Arlington, Virginia   \n",
       "8                       Life & Health Insurance        Columbus, Georgia   \n",
       "9                Life Sciences Tools & Services  Santa Clara, California   \n",
       "\n",
       "   Date added      CIK      Founded  \n",
       "0  1957-03-04    66740         1902  \n",
       "1  2017-07-26    91142         1916  \n",
       "2  1957-03-04     1800         1888  \n",
       "3  2012-12-31  1551152  2013 (1888)  \n",
       "4  2011-07-06  1467373         1989  \n",
       "5  1997-05-05   796343         1982  \n",
       "6  2017-03-20     2488         1969  \n",
       "7  1998-10-02   874761         1981  \n",
       "8  1999-05-28     4977         1955  \n",
       "9  2000-06-05  1090872         1999  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize data acquisition\n",
    "data_acq = DataAcquisition('config.yaml')\n",
    "sp500 = data_acq.fetch_sp500_constituents()\n",
    "sp500.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ FRED API initialized\n",
      "Fetching transcripts from Hugging Face (kurry/sp500_earnings_transcripts)...\n",
      "Downloading dataset...\n",
      "Converting to DataFrame...\n",
      "âœ“ Loaded 33,362 total transcripts\n",
      "âœ“ Loaded 503 S&P 500 constituents\n",
      "Filtering by date and S&P 500 membership...\n",
      "  After date filter: 21,135 transcripts\n",
      "âœ“ Final result: 18,103 S&P 500 transcripts (2015-01-01 to 2026-01-01)\n",
      "Loaded 18103 transcripts for Q1 2015\n",
      "âœ“ Fetched gdp: 43 observations\n",
      "âœ“ Fetched industrial_production: 132 observations\n",
      "âœ“ Fetched employment: 132 observations\n",
      "âœ“ Fetched wages: 132 observations\n",
      "Loaded 4 macro indicators\n",
      "âœ“ Loaded 503 S&P 500 constituents\n",
      "Loaded 503 S&P 500 stocks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_acquisition import DataAcquisition\n",
    "data = DataAcquisition(\"config.yaml\")\n",
    "transcripts = data.fetch_earnings_transcripts('2015-01-01', '2026-01-01')\n",
    "print(f\"Loaded {len(transcripts)} transcripts for Q1 2015\")\n",
    "macro = data.fetch_macro_data('2015-01-01', '2025-12-31')\n",
    "print(f\"Loaded {len(macro)} macro indicators\")\n",
    "sp500 = data.fetch_sp500_constituents()\n",
    "print(f\"Loaded {len(sp500)} S&P 500 stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fetch Macro Data (FRED API)\n",
    "\n",
    "**Note**: If you get FRED API errors, restart the kernel to reload the config with the updated API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fetched gdp: 39 observations\n",
      "âœ“ Fetched industrial_production: 120 observations\n",
      "âœ“ Fetched employment: 120 observations\n",
      "âœ“ Fetched wages: 120 observations\n",
      "\n",
      " Macroeconomic Data:\n",
      "  gdp: 39 observations\n",
      "  industrial_production: 120 observations\n",
      "  employment: 120 observations\n",
      "  wages: 120 observations\n"
     ]
    }
   ],
   "source": [
    "# Fetch macroeconomic data\n",
    "start_date = config['data']['transcripts']['start_date']\n",
    "end_date = config['data']['transcripts']['end_date']\n",
    "macro_data = data_acq.fetch_macro_data(start_date, end_date)\n",
    "print(f\"\\n Macroeconomic Data:\")\n",
    "for name, df in macro_data.items():\n",
    "    print(f\"  {name}: {len(df)} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in PMI file: ['date', 'pmi']\n",
      "Loaded PMI data: 133 rows\n",
      "          date   pmi\n",
      "128 2015-05-01  51.5\n",
      "129 2015-04-01  51.5\n",
      "130 2015-03-02  52.9\n",
      "131 2015-02-02  53.5\n",
      "132 2015-01-02  55.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pmi_path = 'pmi_data.csv'\n",
    "pmi_df = pd.read_csv(pmi_path)\n",
    "pmi_df.columns = [c.strip().lower().replace(' ', '_') for c in pmi_df.columns]\n",
    "print(\"Columns in PMI file:\", pmi_df.columns.tolist())\n",
    "date_col = [col for col in pmi_df.columns if 'date' in col][0]\n",
    "pmi_col = [col for col in pmi_df.columns if 'pmi' in col][0]\n",
    "def clean_date(val):\n",
    "    # Extract the part before the first parenthesis\n",
    "    val = str(val).split('(')[0].strip()\n",
    "    try:\n",
    "        return pd.to_datetime(val)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "pmi_df[date_col] = pmi_df[date_col].apply(clean_date)\n",
    "pmi_df = pmi_df.dropna(subset=[date_col, pmi_col])\n",
    "print(f\"Loaded PMI data: {len(pmi_df)} rows\")\n",
    "print(pmi_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fetched yield curve slope\n",
      "âœ“ Fetched consumer sentiment\n",
      "âœ“ Fetched unemployment rate\n",
      "âœ— No local PMI data provided; PMI not included in controls.\n",
      "âœ“ Control variables: 120 observations\n",
      "\n",
      "Control Variables: 120 observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yield_curve_slope</th>\n",
       "      <th>consumer_sentiment</th>\n",
       "      <th>unemployment_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>1.19</td>\n",
       "      <td>92.0</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01</th>\n",
       "      <td>1.05</td>\n",
       "      <td>91.7</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-01</th>\n",
       "      <td>1.01</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-01</th>\n",
       "      <td>1.04</td>\n",
       "      <td>89.0</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-01</th>\n",
       "      <td>0.99</td>\n",
       "      <td>94.7</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            yield_curve_slope  consumer_sentiment  unemployment_rate\n",
       "2016-01-01               1.19                92.0                4.8\n",
       "2016-02-01               1.05                91.7                4.9\n",
       "2016-03-01               1.01                91.0                5.0\n",
       "2016-04-01               1.04                89.0                5.1\n",
       "2016-05-01               0.99                94.7                4.8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch control variables\n",
    "controls = data_acq.fetch_control_variables(start_date, end_date)\n",
    "print(f\"\\nControl Variables: {len(controls)} observations\")\n",
    "controls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fetched yield curve slope\n",
      "âœ“ Fetched consumer sentiment\n",
      "âœ“ Fetched unemployment rate\n",
      "âœ“ Used local PMI data: 120 rows\n",
      "âœ“ Control variables: 161 observations\n"
     ]
    }
   ],
   "source": [
    "data_acq.pmi_df = pmi_df\n",
    "controls = data_acq.fetch_control_variables(start_date, end_date, pmi_df=pmi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yield_curve_slope</th>\n",
       "      <th>consumer_sentiment</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>1.19</td>\n",
       "      <td>92.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>1.19</td>\n",
       "      <td>92.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-01</th>\n",
       "      <td>1.05</td>\n",
       "      <td>91.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-03-01</th>\n",
       "      <td>1.01</td>\n",
       "      <td>91.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-01</th>\n",
       "      <td>1.04</td>\n",
       "      <td>89.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>51.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            yield_curve_slope  consumer_sentiment  unemployment_rate   pmi\n",
       "2016-01-01               1.19                92.0                4.8   NaN\n",
       "2016-01-04               1.19                92.0                4.8  48.2\n",
       "2016-02-01               1.05                91.7                4.9  48.2\n",
       "2016-03-01               1.01                91.0                5.0  49.5\n",
       "2016-04-01               1.04                89.0                5.1  51.8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>structured_content</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-11-23 16:30:00</td>\n",
       "      <td>Operator: Good afternoon, and welcome to the A...</td>\n",
       "      <td>[{'speaker': 'Operator', 'text': 'Good afterno...</td>\n",
       "      <td>Agilent Technologies, Inc.</td>\n",
       "      <td>154924.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol  quarter  year                date  \\\n",
       "0      A        4  2020 2020-11-23 16:30:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  Operator: Good afternoon, and welcome to the A...   \n",
       "\n",
       "                                  structured_content  \\\n",
       "0  [{'speaker': 'Operator', 'text': 'Good afterno...   \n",
       "\n",
       "                 company_name  company_id  \n",
       "0  Agilent Technologies, Inc.    154924.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date     0\n",
       "value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count NAN \n",
    "macro_data['wages'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LLM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM scorer\n",
    "scorer = LLMScorer('config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AGG score aggregation function defined\n"
     ]
    }
   ],
   "source": [
    "def aggregate_scores_by_quarter(scored_transcripts):\n",
    "    \"\"\"\n",
    "    Aggregate individual transcript scores into quarterly AGG scores.\n",
    "    \n",
    "    Args:\n",
    "        scored_transcripts: List of dicts with 'symbol', 'date', 'score', 'market_cap'\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quarterly AGG scores\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(scored_transcripts)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['quarter_date'] = df['date'].dt.to_period('Q').dt.to_timestamp()\n",
    "    \n",
    "    # Aggregate by quarter using value-weighted average\n",
    "    quarterly = df.groupby('quarter_date').apply(\n",
    "        lambda x: np.average(x['score'], weights=x.get('market_cap', [1]*len(x)))\n",
    "    ).reset_index()\n",
    "    \n",
    "    quarterly.columns = ['date', 'agg_score']\n",
    "    quarterly['year'] = quarterly['date'].dt.year\n",
    "    quarterly['quarter'] = quarterly['date'].dt.quarter\n",
    "    \n",
    "    return quarterly[['date', 'year', 'quarter', 'agg_score']]\n",
    "\n",
    "# Example usage (commented out - requires real transcript scores):\n",
    "# scored_transcripts = scorer.score_multiple_transcripts(transcripts)\n",
    "# agg_scores = aggregate_scores_by_quarter(scored_transcripts)\n",
    "# agg_scores.to_csv('agg_scores.csv', index=False)\n",
    "print(\"âœ“ AGG score aggregation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL MODE: Checking for existing transcript data...\n",
      "Loaded data has limited range (2015-01-06 to 2025-05-15)\n",
      "Fetching complete 2015-2025 dataset...\n",
      "Fetching transcripts from Hugging Face (kurry/sp500_earnings_transcripts)...\n",
      "Downloading dataset...\n",
      "Converting to DataFrame...\n",
      "âœ“ Loaded 33,362 total transcripts\n",
      "âœ“ Loaded 503 S&P 500 constituents\n",
      "Filtering by date and S&P 500 membership...\n",
      "  After date filter: 21,135 transcripts\n",
      "âœ“ Final result: 18,103 S&P 500 transcripts (2015-01-01 to 2025-12-31)\n",
      "\n",
      "Total transcripts to score: 18103\n",
      "  Estimated cost: $18.10 - $36.21\n",
      "  Estimated time: 10.1 - 15.1 hours\n",
      "\n",
      "Data will be saved to: all_scored_transcripts_2015_2025.csv\n",
      "\n",
      "Transcripts by year:\n",
      "  2015: 1380 transcripts\n",
      "  2016: 1455 transcripts\n",
      "  2017: 1530 transcripts\n",
      "  2018: 1577 transcripts\n",
      "  2019: 1655 transcripts\n",
      "  2020: 1857 transcripts\n",
      "  2021: 1911 transcripts\n",
      "  2022: 1925 transcripts\n",
      "  2023: 1934 transcripts\n",
      "  2024: 1960 transcripts\n",
      "  2025: 919 transcripts\n",
      "\n",
      "Ready to score 18103 transcripts\n",
      "Checkpoints will be saved every 50 transcripts\n"
     ]
    }
   ],
   "source": [
    "# Choose scoring mode\n",
    "TEST_MODE = False  # Set to False to run full dataset (2015-2025)\n",
    "\n",
    "if TEST_MODE:\n",
    "    # OPTION A: Test with 2024-2025 data\n",
    "    print(\"TEST MODE: Checking for existing transcript data...\")\n",
    "    \n",
    "    # Check if we already have filtered 2024-2025 data\n",
    "    if 'transcripts_2024_2025' in dir() and len(transcripts_2024_2025) > 0:\n",
    "        test_transcripts = transcripts_2024_2025.copy()\n",
    "        print(f\"Using pre-filtered transcripts_2024_2025 data: {len(test_transcripts)} transcripts\")\n",
    "    elif 'transcripts' in dir() and len(transcripts) > 0:\n",
    "        # Filter existing transcripts to 2024-2025\n",
    "        print(\"Filtering full transcript data to 2024-2025...\")\n",
    "        transcripts_copy = transcripts.copy()\n",
    "        transcripts_copy['date'] = pd.to_datetime(transcripts_copy['date'])\n",
    "        test_transcripts = transcripts_copy[\n",
    "            (transcripts_copy['date'] >= '2024-01-01') & \n",
    "            (transcripts_copy['date'] <= '2025-12-31')\n",
    "        ].copy()\n",
    "        print(f\"Filtered {len(transcripts)} â†’ {len(test_transcripts)} transcripts\")\n",
    "    else:\n",
    "        print(\"No transcripts loaded yet, fetching 2024-2025...\")\n",
    "        test_transcripts = data_acq.fetch_earnings_transcripts('2024-01-01', '2025-12-31')\n",
    "    \n",
    "    print(f\"\\nTotal transcripts to score: {len(test_transcripts)}\")\n",
    "    print(f\"  Estimated cost: ${len(test_transcripts) * 0.001:.2f} - ${len(test_transcripts) * 0.002:.2f}\")\n",
    "    print(f\"  Estimated time: {len(test_transcripts) * 2 / 60:.1f} - {len(test_transcripts) * 3 / 60:.1f} minutes\")\n",
    "    print(f\"\\nData will be saved to: test_scored_transcripts_2024_2025.csv\")\n",
    "    \n",
    "    # Show breakdown by year\n",
    "    test_transcripts['year'] = pd.to_datetime(test_transcripts['date']).dt.year\n",
    "    year_counts = test_transcripts['year'].value_counts().sort_index()\n",
    "    print(f\"\\nTranscripts by year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} transcripts\")\n",
    "    \n",
    "    scoring_transcripts = test_transcripts\n",
    "    save_path = 'test_scored_transcripts_2024_2025.csv'\n",
    "    \n",
    "else:\n",
    "    # OPTION B: Full dataset (2015-2025)\n",
    "    print(\"FULL MODE: Checking for existing transcript data...\")\n",
    "    \n",
    "    # Check if we already have full dataset loaded\n",
    "    if 'transcripts' in dir() and len(transcripts) > 0:\n",
    "        transcripts_copy = transcripts.copy()\n",
    "        transcripts_copy['date'] = pd.to_datetime(transcripts_copy['date'])\n",
    "        date_range = (transcripts_copy['date'].min(), transcripts_copy['date'].max())\n",
    "        \n",
    "        # Check if we have enough coverage\n",
    "        if date_range[0] <= pd.Timestamp('2015-01-01') and date_range[1] >= pd.Timestamp('2025-01-01'):\n",
    "            print(f\"Reusing {len(transcripts_copy)} transcripts from already-loaded data\")\n",
    "            print(f\"  Date range: {date_range[0].date()} to {date_range[1].date()}\")\n",
    "            all_transcripts = transcripts_copy[\n",
    "                (transcripts_copy['date'] >= '2015-01-01') & \n",
    "                (transcripts_copy['date'] <= '2025-12-31')\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Loaded data has limited range ({date_range[0].date()} to {date_range[1].date()})\")\n",
    "            print(\"Fetching complete 2015-2025 dataset...\")\n",
    "            all_transcripts = data_acq.fetch_earnings_transcripts('2015-01-01', '2025-12-31')\n",
    "    else:\n",
    "        print(\"No transcripts loaded yet, fetching 2015-2025...\")\n",
    "        all_transcripts = data_acq.fetch_earnings_transcripts('2015-01-01', '2025-12-31')\n",
    "    \n",
    "    print(f\"\\nTotal transcripts to score: {len(all_transcripts)}\")\n",
    "    print(f\"  Estimated cost: ${len(all_transcripts) * 0.001:.2f} - ${len(all_transcripts) * 0.002:.2f}\")\n",
    "    print(f\"  Estimated time: {len(all_transcripts) * 2 / 3600:.1f} - {len(all_transcripts) * 3 / 3600:.1f} hours\")\n",
    "    print(f\"\\nData will be saved to: all_scored_transcripts_2015_2025.csv\")\n",
    "    \n",
    "    # Show breakdown by year\n",
    "    all_transcripts['year'] = pd.to_datetime(all_transcripts['date']).dt.year\n",
    "    year_counts = all_transcripts['year'].value_counts().sort_index()\n",
    "    print(f\"\\nTranscripts by year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} transcripts\")\n",
    "    \n",
    "    scoring_transcripts = all_transcripts\n",
    "    save_path = 'all_scored_transcripts_2015_2025.csv'\n",
    "\n",
    "print(f\"\\nReady to score {len(scoring_transcripts)} transcripts\")\n",
    "print(f\"Checkpoints will be saved every 50 transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AWS SPOT INSTANCE SCORING - FIRE & FORGET MODE\n",
      "======================================================================\n",
      "ðŸ“… Started: 2026-02-02 15:11:42\n",
      "\n",
      "âœ“ S3 bucket validated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aws_job_submitter:Initialized job submitter for bucket: transcript-scoring-1770013499\n",
      "INFO:aws_job_submitter:Creating batches of 50 transcripts each\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Processing 18103 transcripts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aws_job_submitter:Uploaded batch 1 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0001.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 2 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0002.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 3 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0003.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 4 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0004.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 5 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0005.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 6 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0006.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 7 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0007.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 8 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0008.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 9 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0009.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 10 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0010.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 11 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0011.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 12 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0012.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 13 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0013.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 14 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0014.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 15 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0015.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 16 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0016.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 17 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0017.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 18 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0018.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 19 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0019.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 20 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0020.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 21 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0021.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 22 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0022.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 23 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0023.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 24 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0024.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 25 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0025.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 26 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0026.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 27 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0027.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 28 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0028.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 29 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0029.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 30 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0030.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 31 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0031.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 32 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0032.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 33 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0033.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 34 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0034.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 35 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0035.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 36 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0036.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 37 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0037.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 38 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0038.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 39 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0039.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 40 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0040.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 41 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0041.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 42 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0042.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 43 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0043.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 44 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0044.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 45 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0045.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 46 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0046.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 47 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0047.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 48 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0048.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 49 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0049.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 50 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0050.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 51 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0051.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 52 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0052.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 53 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0053.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 54 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0054.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 55 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0055.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 56 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0056.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 57 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0057.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 58 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0058.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 59 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0059.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 60 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0060.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 61 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0061.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 62 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0062.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 63 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0063.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 64 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0064.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 65 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0065.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 66 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0066.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 67 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0067.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 68 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0068.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 69 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0069.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 70 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0070.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 71 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0071.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 72 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0072.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 73 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0073.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 74 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0074.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 75 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0075.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 76 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0076.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 77 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0077.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 78 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0078.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 79 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0079.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 80 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0080.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 81 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0081.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 82 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0082.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 83 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0083.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 84 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0084.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 85 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0085.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 86 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0086.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 87 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0087.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 88 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0088.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 89 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0089.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 90 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0090.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 91 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0091.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 92 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0092.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 93 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0093.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 94 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0094.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 95 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0095.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 96 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0096.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 97 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0097.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 98 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0098.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 99 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0099.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 100 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0100.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 101 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0101.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 102 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0102.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 103 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0103.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 104 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0104.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 105 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0105.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 106 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0106.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 107 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0107.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 108 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0108.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 109 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0109.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 110 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0110.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 111 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0111.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 112 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0112.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 113 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0113.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 114 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0114.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 115 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0115.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 116 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0116.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 117 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0117.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 118 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0118.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 119 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0119.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 120 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0120.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 121 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0121.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 122 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0122.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 123 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0123.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 124 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0124.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 125 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0125.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 126 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0126.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 127 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0127.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 128 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0128.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 129 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0129.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 130 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0130.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 131 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0131.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 132 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0132.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 133 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0133.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 134 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0134.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 135 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0135.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 136 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0136.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 137 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0137.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 138 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0138.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 139 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0139.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 140 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0140.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 141 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0141.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 142 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0142.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 143 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0143.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 144 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0144.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 145 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0145.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 146 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0146.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 147 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0147.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 148 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0148.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 149 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0149.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 150 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0150.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 151 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0151.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 152 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0152.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 153 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0153.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 154 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0154.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 155 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0155.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 156 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0156.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 157 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0157.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 158 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0158.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 159 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0159.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 160 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0160.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 161 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0161.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 162 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0162.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 163 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0163.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 164 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0164.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 165 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0165.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 166 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0166.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 167 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0167.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 168 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0168.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 169 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0169.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 170 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0170.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 171 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0171.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 172 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0172.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 173 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0173.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 174 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0174.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 175 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0175.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 176 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0176.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 177 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0177.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 178 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0178.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 179 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0179.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 180 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0180.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 181 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0181.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 182 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0182.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 183 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0183.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 184 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0184.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 185 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0185.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 186 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0186.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 187 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0187.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 188 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0188.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 189 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0189.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 190 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0190.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 191 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0191.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 192 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0192.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 193 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0193.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 194 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0194.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 195 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0195.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 196 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0196.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 197 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0197.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 198 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0198.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 199 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0199.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 200 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0200.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 201 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0201.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 202 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0202.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 203 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0203.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 204 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0204.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 205 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0205.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 206 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0206.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 207 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0207.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 208 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0208.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 209 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0209.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 210 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0210.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 211 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0211.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 212 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0212.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 213 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0213.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 214 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0214.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 215 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0215.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 216 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0216.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 217 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0217.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 218 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0218.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 219 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0219.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 220 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0220.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 221 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0221.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 222 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0222.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 223 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0223.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 224 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0224.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 225 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0225.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 226 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0226.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 227 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0227.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 228 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0228.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 229 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0229.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 230 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0230.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 231 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0231.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 232 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0232.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 233 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0233.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 234 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0234.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 235 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0235.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 236 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0236.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 237 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0237.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 238 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0238.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 239 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0239.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 240 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0240.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 241 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0241.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 242 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0242.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 243 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0243.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 244 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0244.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 245 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0245.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 246 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0246.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 247 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0247.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 248 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0248.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 249 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0249.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 250 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0250.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 251 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0251.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 252 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0252.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 253 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0253.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 254 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0254.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 255 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0255.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 256 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0256.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 257 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0257.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 258 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0258.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 259 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0259.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 260 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0260.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 261 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0261.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 262 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0262.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 263 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0263.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 264 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0264.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 265 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0265.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 266 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0266.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 267 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0267.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 268 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0268.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 269 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0269.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 270 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0270.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 271 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0271.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 272 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0272.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 273 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0273.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 274 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0274.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 275 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0275.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 276 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0276.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 277 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0277.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 278 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0278.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 279 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0279.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 280 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0280.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 281 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0281.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 282 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0282.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 283 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0283.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 284 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0284.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 285 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0285.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 286 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0286.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 287 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0287.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 288 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0288.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 289 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0289.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 290 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0290.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 291 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0291.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 292 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0292.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 293 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0293.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 294 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0294.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 295 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0295.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 296 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0296.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 297 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0297.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 298 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0298.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 299 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0299.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 300 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0300.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 301 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0301.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 302 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0302.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 303 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0303.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 304 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0304.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 305 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0305.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 306 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0306.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 307 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0307.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 308 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0308.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 309 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0309.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 310 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0310.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 311 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0311.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 312 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0312.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 313 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0313.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 314 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0314.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 315 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0315.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 316 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0316.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 317 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0317.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 318 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0318.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 319 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0319.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 320 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0320.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 321 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0321.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 322 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0322.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 323 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0323.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 324 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0324.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 325 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0325.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 326 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0326.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 327 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0327.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 328 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0328.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 329 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0329.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 330 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0330.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 331 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0331.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 332 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0332.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 333 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0333.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 334 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0334.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 335 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0335.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 336 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0336.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 337 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0337.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 338 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0338.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 339 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0339.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 340 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0340.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 341 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0341.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 342 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0342.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 343 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0343.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 344 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0344.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 345 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0345.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 346 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0346.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 347 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0347.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 348 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0348.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 349 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0349.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 350 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0350.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 351 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0351.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 352 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0352.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 353 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0353.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 354 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0354.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 355 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0355.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 356 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0356.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 357 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0357.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 358 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0358.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 359 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0359.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 360 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0360.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 361 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0361.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 362 (50 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0362.csv\n",
      "INFO:aws_job_submitter:Uploaded batch 363 (3 transcripts) to s3://transcript-scoring-1770013499/input/batches/20260202_151144/batch_0363.csv\n",
      "INFO:aws_job_submitter:Created 363 batches\n",
      "INFO:aws_job_submitter:Creating 363 jobs in DynamoDB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 363 batches â†’ uploaded to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aws_job_submitter:Created job 105eefce-8961-4ef0-9d35-15e224b7c12e for batch input/batches/20260202_151144/batch_0001.csv\n",
      "INFO:aws_job_submitter:Created job b399ea30-e59e-440b-8fd9-544396dcd079 for batch input/batches/20260202_151144/batch_0002.csv\n",
      "INFO:aws_job_submitter:Created job a99dc391-9c05-4f13-8489-07fcd6414f6f for batch input/batches/20260202_151144/batch_0003.csv\n",
      "INFO:aws_job_submitter:Created job 87ef78dc-b43c-47ec-b22e-1290552c5efb for batch input/batches/20260202_151144/batch_0004.csv\n",
      "INFO:aws_job_submitter:Created job 3ac5d5fd-f143-4927-b1ef-c24a04848935 for batch input/batches/20260202_151144/batch_0005.csv\n",
      "INFO:aws_job_submitter:Created job 9ec37a02-62ca-47b7-a3e8-486cc238cb68 for batch input/batches/20260202_151144/batch_0006.csv\n",
      "INFO:aws_job_submitter:Created job bea062f3-0776-4d7c-be14-bb8179bc2cba for batch input/batches/20260202_151144/batch_0007.csv\n",
      "INFO:aws_job_submitter:Created job 7131f2b1-8ab3-4371-ab54-337eeaec43de for batch input/batches/20260202_151144/batch_0008.csv\n",
      "INFO:aws_job_submitter:Created job 6489bd35-f3d4-43ee-91b7-4741816e38ea for batch input/batches/20260202_151144/batch_0009.csv\n",
      "INFO:aws_job_submitter:Created job ec35c564-cb93-48ac-a0d1-bbc4dc1f3d04 for batch input/batches/20260202_151144/batch_0010.csv\n",
      "INFO:aws_job_submitter:Created job 34101bd7-75f1-458b-9f55-dde42952eb69 for batch input/batches/20260202_151144/batch_0011.csv\n",
      "INFO:aws_job_submitter:Created job 4d6b8313-bb83-445d-bef5-fda59ba163c5 for batch input/batches/20260202_151144/batch_0012.csv\n",
      "INFO:aws_job_submitter:Created job 5547f1bd-c082-4929-975c-66a76bf747db for batch input/batches/20260202_151144/batch_0013.csv\n",
      "INFO:aws_job_submitter:Created job 968b7f2c-6c55-48b4-98e1-3d03f4b6c8f0 for batch input/batches/20260202_151144/batch_0014.csv\n",
      "INFO:aws_job_submitter:Created job 5efe0277-7961-4d16-b00d-24d1d05e212b for batch input/batches/20260202_151144/batch_0015.csv\n",
      "INFO:aws_job_submitter:Created job 0f6d9aeb-f524-454c-9112-ec93384a4f9e for batch input/batches/20260202_151144/batch_0016.csv\n",
      "INFO:aws_job_submitter:Created job 05dc5b01-9890-4bc9-b495-64a8571defa7 for batch input/batches/20260202_151144/batch_0017.csv\n",
      "INFO:aws_job_submitter:Created job 4e78e494-e16d-4953-853a-a08634456a5f for batch input/batches/20260202_151144/batch_0018.csv\n",
      "INFO:aws_job_submitter:Created job 4b4c06a0-dc2f-4d3a-96f3-30cc7a0534aa for batch input/batches/20260202_151144/batch_0019.csv\n",
      "INFO:aws_job_submitter:Created job 366c64a3-7759-4471-a229-32142c9aa957 for batch input/batches/20260202_151144/batch_0020.csv\n",
      "INFO:aws_job_submitter:Created job db9216a8-ef90-4765-b8be-f30b4b8b9832 for batch input/batches/20260202_151144/batch_0021.csv\n",
      "INFO:aws_job_submitter:Created job 42cee082-4529-4866-a301-458151ccb2f2 for batch input/batches/20260202_151144/batch_0022.csv\n",
      "INFO:aws_job_submitter:Created job ed3aed51-55be-4d50-971a-6d43be50c362 for batch input/batches/20260202_151144/batch_0023.csv\n",
      "INFO:aws_job_submitter:Created job 023f46f0-8eb3-4d5e-a342-be7e398d53d7 for batch input/batches/20260202_151144/batch_0024.csv\n",
      "INFO:aws_job_submitter:Created job e58e59bc-9d8a-48a7-a5fd-3c243b65ba9c for batch input/batches/20260202_151144/batch_0025.csv\n",
      "INFO:aws_job_submitter:Created job 789a5d2a-8511-4f70-b1e1-fc208d186c1e for batch input/batches/20260202_151144/batch_0026.csv\n",
      "INFO:aws_job_submitter:Created job e47347ff-c1e8-4481-b576-fc112840a247 for batch input/batches/20260202_151144/batch_0027.csv\n",
      "INFO:aws_job_submitter:Created job 1c6c5de5-b3d9-455f-867a-caec04028ff4 for batch input/batches/20260202_151144/batch_0028.csv\n",
      "INFO:aws_job_submitter:Created job db32aa14-a233-4e39-b2e2-a1c648ce6c7f for batch input/batches/20260202_151144/batch_0029.csv\n",
      "INFO:aws_job_submitter:Created job 1b5381ee-deb1-4efa-81d3-e112d9aacca7 for batch input/batches/20260202_151144/batch_0030.csv\n",
      "INFO:aws_job_submitter:Created job 300d24ab-9eaf-4c03-9c82-be6ac92528fb for batch input/batches/20260202_151144/batch_0031.csv\n",
      "INFO:aws_job_submitter:Created job bfa7e5d2-4019-4c30-b12c-b504450a2b1e for batch input/batches/20260202_151144/batch_0032.csv\n",
      "INFO:aws_job_submitter:Created job 26fb8b57-7f14-430d-97ca-857ab927836d for batch input/batches/20260202_151144/batch_0033.csv\n",
      "INFO:aws_job_submitter:Created job 2dd5b482-46b1-4add-99aa-e95f0f187601 for batch input/batches/20260202_151144/batch_0034.csv\n",
      "INFO:aws_job_submitter:Created job cc1ba78d-e860-491b-b3b2-128961525278 for batch input/batches/20260202_151144/batch_0035.csv\n",
      "INFO:aws_job_submitter:Created job 1d7a2750-b0b8-49e1-b8fd-b676b2ff8e03 for batch input/batches/20260202_151144/batch_0036.csv\n",
      "INFO:aws_job_submitter:Created job 28ddfd2b-5603-47e6-af77-193c8128578d for batch input/batches/20260202_151144/batch_0037.csv\n",
      "INFO:aws_job_submitter:Created job 645b796e-5a5c-4375-963a-83ca98b8816a for batch input/batches/20260202_151144/batch_0038.csv\n",
      "INFO:aws_job_submitter:Created job 0859dfa7-bf67-448c-845c-25b50c59e45a for batch input/batches/20260202_151144/batch_0039.csv\n",
      "INFO:aws_job_submitter:Created job 20b68449-7a2d-4efb-9226-4fd3a50ce33f for batch input/batches/20260202_151144/batch_0040.csv\n",
      "INFO:aws_job_submitter:Created job 9a28df1a-1310-42bf-ab58-6cd2d369e90a for batch input/batches/20260202_151144/batch_0041.csv\n",
      "INFO:aws_job_submitter:Created job 2095f31a-b88e-47ae-b3cb-2b9e7bece2b7 for batch input/batches/20260202_151144/batch_0042.csv\n",
      "INFO:aws_job_submitter:Created job e0e958a9-31e6-4b25-9af9-07765d846e06 for batch input/batches/20260202_151144/batch_0043.csv\n",
      "INFO:aws_job_submitter:Created job 555f09e9-0374-4f62-82d5-87dcf16920e6 for batch input/batches/20260202_151144/batch_0044.csv\n",
      "INFO:aws_job_submitter:Created job 12ddee4f-5519-4d15-8ae3-72f8a6fe02c2 for batch input/batches/20260202_151144/batch_0045.csv\n",
      "INFO:aws_job_submitter:Created job 1bbf0be8-3931-44db-947d-6d13665931e1 for batch input/batches/20260202_151144/batch_0046.csv\n",
      "INFO:aws_job_submitter:Created job dcbb4556-3f2c-4188-aca8-09ed8f829bdb for batch input/batches/20260202_151144/batch_0047.csv\n",
      "INFO:aws_job_submitter:Created job 4d6a7078-fc06-405c-8441-4ca316c4fd81 for batch input/batches/20260202_151144/batch_0048.csv\n",
      "INFO:aws_job_submitter:Created job 6659c1e7-3830-4c47-ba79-9747d73cedaa for batch input/batches/20260202_151144/batch_0049.csv\n",
      "INFO:aws_job_submitter:Created job e3a7ae3f-3eec-473e-a888-ee825a13b173 for batch input/batches/20260202_151144/batch_0050.csv\n",
      "INFO:aws_job_submitter:Created job 87a20c15-0a38-4d91-9f9d-3f80a1c4e05b for batch input/batches/20260202_151144/batch_0051.csv\n",
      "INFO:aws_job_submitter:Created job 46ae6e85-d2f2-4608-8085-3575dbd2a795 for batch input/batches/20260202_151144/batch_0052.csv\n",
      "INFO:aws_job_submitter:Created job 191e963b-af24-4a32-a749-5d3a498dd0b6 for batch input/batches/20260202_151144/batch_0053.csv\n",
      "INFO:aws_job_submitter:Created job 9e955871-a392-4910-baf9-3657d0cacab0 for batch input/batches/20260202_151144/batch_0054.csv\n",
      "INFO:aws_job_submitter:Created job eecf21ee-8c76-4dde-b7d3-05ba0a8b7808 for batch input/batches/20260202_151144/batch_0055.csv\n",
      "INFO:aws_job_submitter:Created job 2f3fb553-e7ba-414b-a9c0-2c128df77499 for batch input/batches/20260202_151144/batch_0056.csv\n",
      "INFO:aws_job_submitter:Created job 27532162-d331-403c-b14f-58781d95517a for batch input/batches/20260202_151144/batch_0057.csv\n",
      "INFO:aws_job_submitter:Created job 2e3369ab-05db-478d-98db-769e9d8ac637 for batch input/batches/20260202_151144/batch_0058.csv\n",
      "INFO:aws_job_submitter:Created job 364190c5-2cf3-44d6-bf8e-bc46d79027a1 for batch input/batches/20260202_151144/batch_0059.csv\n",
      "INFO:aws_job_submitter:Created job 65ad7827-e96e-4162-9f0c-6f15fb7c1a68 for batch input/batches/20260202_151144/batch_0060.csv\n",
      "INFO:aws_job_submitter:Created job 7f08f4d2-1f67-4c18-b657-988c876d8fde for batch input/batches/20260202_151144/batch_0061.csv\n",
      "INFO:aws_job_submitter:Created job 5fd5b2fc-bff4-40f6-9f3e-567a1b1bd6f8 for batch input/batches/20260202_151144/batch_0062.csv\n",
      "INFO:aws_job_submitter:Created job 4b33bbb2-e141-40b9-907a-c3a820199786 for batch input/batches/20260202_151144/batch_0063.csv\n",
      "INFO:aws_job_submitter:Created job 4f0b9fbb-47f4-4ab5-aa77-3f590a7ba1d9 for batch input/batches/20260202_151144/batch_0064.csv\n",
      "INFO:aws_job_submitter:Created job 01db3965-dc83-40fd-b56d-20e977bdd213 for batch input/batches/20260202_151144/batch_0065.csv\n",
      "INFO:aws_job_submitter:Created job 7b0fb8bf-bc2a-47b4-a587-d30ebf89b6b3 for batch input/batches/20260202_151144/batch_0066.csv\n",
      "INFO:aws_job_submitter:Created job 8d5a4a48-4053-47fe-b262-1cc420646d01 for batch input/batches/20260202_151144/batch_0067.csv\n",
      "INFO:aws_job_submitter:Created job aceafce4-19ad-4e90-ae08-058b9a06d8b9 for batch input/batches/20260202_151144/batch_0068.csv\n",
      "INFO:aws_job_submitter:Created job 4fa1ba2b-0afb-4e36-919f-56a11cb3c90e for batch input/batches/20260202_151144/batch_0069.csv\n",
      "INFO:aws_job_submitter:Created job a58cd276-ce82-4f46-a408-d6a534a1a899 for batch input/batches/20260202_151144/batch_0070.csv\n",
      "INFO:aws_job_submitter:Created job eb49a9fe-09c7-49d1-a9bd-7f50b91eaeb0 for batch input/batches/20260202_151144/batch_0071.csv\n",
      "INFO:aws_job_submitter:Created job e4b0769e-5434-4b8b-8645-a091c8090d3e for batch input/batches/20260202_151144/batch_0072.csv\n",
      "INFO:aws_job_submitter:Created job b5aa681e-5455-496f-ade9-d742da325736 for batch input/batches/20260202_151144/batch_0073.csv\n",
      "INFO:aws_job_submitter:Created job b5b5ad12-b69b-479e-9249-551933d236bb for batch input/batches/20260202_151144/batch_0074.csv\n",
      "INFO:aws_job_submitter:Created job 20b3d68b-8095-453b-8d33-b2935de03997 for batch input/batches/20260202_151144/batch_0075.csv\n",
      "INFO:aws_job_submitter:Created job 8997594c-2a63-4515-98f4-26c9aaec042a for batch input/batches/20260202_151144/batch_0076.csv\n",
      "INFO:aws_job_submitter:Created job f7053669-9594-4c00-abd2-3eda84f08833 for batch input/batches/20260202_151144/batch_0077.csv\n",
      "INFO:aws_job_submitter:Created job 43e616b7-9709-4d81-831f-f7754c5b73ea for batch input/batches/20260202_151144/batch_0078.csv\n",
      "INFO:aws_job_submitter:Created job 2a6f21ba-7498-48d0-b09c-a6351b23ce88 for batch input/batches/20260202_151144/batch_0079.csv\n",
      "INFO:aws_job_submitter:Created job 55370168-50d9-4e79-a47f-0c1a17d070be for batch input/batches/20260202_151144/batch_0080.csv\n",
      "INFO:aws_job_submitter:Created job 031a8034-539f-4537-a13e-d5c74811c0bf for batch input/batches/20260202_151144/batch_0081.csv\n",
      "INFO:aws_job_submitter:Created job 7e571ab8-5f29-4717-8161-a3d87804fdce for batch input/batches/20260202_151144/batch_0082.csv\n",
      "INFO:aws_job_submitter:Created job 1ede7738-969f-4d14-b69a-078ceed0af3d for batch input/batches/20260202_151144/batch_0083.csv\n",
      "INFO:aws_job_submitter:Created job af96ef11-0df2-4941-aad6-30a736cf8c52 for batch input/batches/20260202_151144/batch_0084.csv\n",
      "INFO:aws_job_submitter:Created job 3759f670-9dc1-4185-9705-3da8aadd51c8 for batch input/batches/20260202_151144/batch_0085.csv\n",
      "INFO:aws_job_submitter:Created job 4ae645aa-5c73-4185-8598-e1bc06b629ef for batch input/batches/20260202_151144/batch_0086.csv\n",
      "INFO:aws_job_submitter:Created job ac077adf-d1d7-4d4a-812c-dccbaa4c61ce for batch input/batches/20260202_151144/batch_0087.csv\n",
      "INFO:aws_job_submitter:Created job f2cc731e-4a83-4457-95d6-894c8a934d4d for batch input/batches/20260202_151144/batch_0088.csv\n",
      "INFO:aws_job_submitter:Created job ed3f2751-4c36-463e-922d-ff9571334519 for batch input/batches/20260202_151144/batch_0089.csv\n",
      "INFO:aws_job_submitter:Created job 78c9a1ee-eea4-435b-9684-69c8fe15d0fa for batch input/batches/20260202_151144/batch_0090.csv\n",
      "INFO:aws_job_submitter:Created job 59064432-2211-41d6-be40-ce15ce4ad2d5 for batch input/batches/20260202_151144/batch_0091.csv\n",
      "INFO:aws_job_submitter:Created job 540947fa-090e-48ec-92b7-415e825483e3 for batch input/batches/20260202_151144/batch_0092.csv\n",
      "INFO:aws_job_submitter:Created job b7ff649e-a92f-4821-b6ba-13290376261b for batch input/batches/20260202_151144/batch_0093.csv\n",
      "INFO:aws_job_submitter:Created job 582ca34a-961b-49aa-b01b-405a4bcbdd98 for batch input/batches/20260202_151144/batch_0094.csv\n",
      "INFO:aws_job_submitter:Created job ea0a5456-4271-4037-bc89-24ae20f09187 for batch input/batches/20260202_151144/batch_0095.csv\n",
      "INFO:aws_job_submitter:Created job 36dbfa12-b5ee-4391-bb93-7993dbf1405e for batch input/batches/20260202_151144/batch_0096.csv\n",
      "INFO:aws_job_submitter:Created job 457e96b2-9180-4b86-9e30-337398577688 for batch input/batches/20260202_151144/batch_0097.csv\n",
      "INFO:aws_job_submitter:Created job 07c4859e-2f55-4b60-b9f7-b4aef79ceb99 for batch input/batches/20260202_151144/batch_0098.csv\n",
      "INFO:aws_job_submitter:Created job b54ecaa2-adbe-429a-a4f8-8232b530bbae for batch input/batches/20260202_151144/batch_0099.csv\n",
      "INFO:aws_job_submitter:Created job c1d6eb6d-1158-46d4-a482-cd0884d5d77d for batch input/batches/20260202_151144/batch_0100.csv\n",
      "INFO:aws_job_submitter:Created job 25267612-721b-4b6c-9a25-f4c4100a6e0d for batch input/batches/20260202_151144/batch_0101.csv\n",
      "INFO:aws_job_submitter:Created job 7c51d811-b562-4035-9b3b-d6e0ff62f0c1 for batch input/batches/20260202_151144/batch_0102.csv\n",
      "INFO:aws_job_submitter:Created job 1eb75c00-42dc-4959-a1bd-707e666634bf for batch input/batches/20260202_151144/batch_0103.csv\n",
      "INFO:aws_job_submitter:Created job 6e6fc0f7-07b4-4ea2-9c4e-d5651ce1bcbb for batch input/batches/20260202_151144/batch_0104.csv\n",
      "INFO:aws_job_submitter:Created job 5078cafb-8b2c-4a9f-b9c4-ed8b19241ff4 for batch input/batches/20260202_151144/batch_0105.csv\n",
      "INFO:aws_job_submitter:Created job dc1cde94-dcb4-4507-aa96-3311d05555df for batch input/batches/20260202_151144/batch_0106.csv\n",
      "INFO:aws_job_submitter:Created job cb086886-482d-43e0-8854-c94ac278faa1 for batch input/batches/20260202_151144/batch_0107.csv\n",
      "INFO:aws_job_submitter:Created job 69c50701-c3ad-407a-b235-a1ae7adb653a for batch input/batches/20260202_151144/batch_0108.csv\n",
      "INFO:aws_job_submitter:Created job 3122ffea-e0a0-4804-b0db-bcb04e4c8ace for batch input/batches/20260202_151144/batch_0109.csv\n",
      "INFO:aws_job_submitter:Created job c151be47-cfd1-4c34-9fc3-64fcf3cbd4dc for batch input/batches/20260202_151144/batch_0110.csv\n",
      "INFO:aws_job_submitter:Created job 41164959-a217-4d6d-ad90-38a4456cc912 for batch input/batches/20260202_151144/batch_0111.csv\n",
      "INFO:aws_job_submitter:Created job 9c5505d6-9884-46c7-901b-7ef99eff2efb for batch input/batches/20260202_151144/batch_0112.csv\n",
      "INFO:aws_job_submitter:Created job 97d641ba-05ff-4c1d-ac0f-d47c3d69fac1 for batch input/batches/20260202_151144/batch_0113.csv\n",
      "INFO:aws_job_submitter:Created job b8b801c1-7d86-473b-a5c5-a91b7ec30239 for batch input/batches/20260202_151144/batch_0114.csv\n",
      "INFO:aws_job_submitter:Created job 1b45023c-5770-488a-87cf-cad3bd9cd744 for batch input/batches/20260202_151144/batch_0115.csv\n",
      "INFO:aws_job_submitter:Created job 4367f5a9-7944-470f-b409-0d816ade8d00 for batch input/batches/20260202_151144/batch_0116.csv\n",
      "INFO:aws_job_submitter:Created job beb4bfee-b4fc-4148-97be-40295b50df2f for batch input/batches/20260202_151144/batch_0117.csv\n",
      "INFO:aws_job_submitter:Created job 404fcbf8-3e05-4e6f-897b-8641e431b38d for batch input/batches/20260202_151144/batch_0118.csv\n",
      "INFO:aws_job_submitter:Created job c0475213-955d-4cd6-b16d-93c6e4ec7c81 for batch input/batches/20260202_151144/batch_0119.csv\n",
      "INFO:aws_job_submitter:Created job 5592248a-4aa1-43bb-8f0d-19c12a053c6e for batch input/batches/20260202_151144/batch_0120.csv\n",
      "INFO:aws_job_submitter:Created job 88a14ba5-09d0-4e31-bf9d-d1f8e12e41cc for batch input/batches/20260202_151144/batch_0121.csv\n",
      "INFO:aws_job_submitter:Created job 82464ee0-01f9-4e0e-9cd8-9ffaa2acccb8 for batch input/batches/20260202_151144/batch_0122.csv\n",
      "INFO:aws_job_submitter:Created job 538c0dcc-fbd7-4d9e-8a00-7415ff3575b5 for batch input/batches/20260202_151144/batch_0123.csv\n",
      "INFO:aws_job_submitter:Created job 0b3a77bf-5ba5-42b8-8d06-ae2404ae656a for batch input/batches/20260202_151144/batch_0124.csv\n",
      "INFO:aws_job_submitter:Created job c2934416-0a55-42d8-b5fc-de27b7474340 for batch input/batches/20260202_151144/batch_0125.csv\n",
      "INFO:aws_job_submitter:Created job aca753c0-3870-4276-8f11-26144624dd08 for batch input/batches/20260202_151144/batch_0126.csv\n",
      "INFO:aws_job_submitter:Created job 6eb55d83-b4f0-4691-a1d4-1c5c6cac5b66 for batch input/batches/20260202_151144/batch_0127.csv\n",
      "INFO:aws_job_submitter:Created job 666c4688-b22e-41a8-83ba-80e999389658 for batch input/batches/20260202_151144/batch_0128.csv\n",
      "INFO:aws_job_submitter:Created job 5587589b-60ed-4d74-be34-e9de73414171 for batch input/batches/20260202_151144/batch_0129.csv\n",
      "INFO:aws_job_submitter:Created job f9e0c3a4-f200-4436-9b64-c8e63ebe065f for batch input/batches/20260202_151144/batch_0130.csv\n",
      "INFO:aws_job_submitter:Created job dd605f31-7551-4ea8-abe6-552d843bd80b for batch input/batches/20260202_151144/batch_0131.csv\n",
      "INFO:aws_job_submitter:Created job 726265c6-3116-4854-b644-32878393f78d for batch input/batches/20260202_151144/batch_0132.csv\n",
      "INFO:aws_job_submitter:Created job c85a410d-8fb5-428f-be8d-a4b28da7e789 for batch input/batches/20260202_151144/batch_0133.csv\n",
      "INFO:aws_job_submitter:Created job 53bd665c-265a-4212-bd4e-e567289d3159 for batch input/batches/20260202_151144/batch_0134.csv\n",
      "INFO:aws_job_submitter:Created job a575342d-e517-432e-b91b-61a848136953 for batch input/batches/20260202_151144/batch_0135.csv\n",
      "INFO:aws_job_submitter:Created job dc84be68-cc89-40a3-b129-7850f436ac95 for batch input/batches/20260202_151144/batch_0136.csv\n",
      "INFO:aws_job_submitter:Created job f06bcd24-e268-4965-be6f-51311e449967 for batch input/batches/20260202_151144/batch_0137.csv\n",
      "INFO:aws_job_submitter:Created job 2d385510-7c8d-4387-b3e2-bd80f2e13992 for batch input/batches/20260202_151144/batch_0138.csv\n",
      "INFO:aws_job_submitter:Created job 27c7d3fa-abe1-492d-8292-857aeb50c5bb for batch input/batches/20260202_151144/batch_0139.csv\n",
      "INFO:aws_job_submitter:Created job 8abc484b-6ed2-4dc2-948b-840dd2d93373 for batch input/batches/20260202_151144/batch_0140.csv\n",
      "INFO:aws_job_submitter:Created job 337bac1a-7e3f-4f7b-aa07-2cf142b267f1 for batch input/batches/20260202_151144/batch_0141.csv\n",
      "INFO:aws_job_submitter:Created job d9e60056-cd07-461a-b862-9ba06d69ec2d for batch input/batches/20260202_151144/batch_0142.csv\n",
      "INFO:aws_job_submitter:Created job d6480674-654c-4bc1-9505-bf434eeaa656 for batch input/batches/20260202_151144/batch_0143.csv\n",
      "INFO:aws_job_submitter:Created job 3ec1093e-0680-48b8-9346-96e0f05ab955 for batch input/batches/20260202_151144/batch_0144.csv\n",
      "INFO:aws_job_submitter:Created job a06b81fe-16c2-45fd-ab1a-841ddde5809a for batch input/batches/20260202_151144/batch_0145.csv\n",
      "INFO:aws_job_submitter:Created job d62df0ca-cb47-4d0e-9620-ac63d5a95d36 for batch input/batches/20260202_151144/batch_0146.csv\n",
      "INFO:aws_job_submitter:Created job 4f2b6fb5-8199-4acd-8bd5-53a0208d5dc8 for batch input/batches/20260202_151144/batch_0147.csv\n",
      "INFO:aws_job_submitter:Created job 73b36c36-6e59-4535-af9c-0ee9fc0fc134 for batch input/batches/20260202_151144/batch_0148.csv\n",
      "INFO:aws_job_submitter:Created job b8e28875-6fc9-42b2-a355-ffbd775f8d8b for batch input/batches/20260202_151144/batch_0149.csv\n",
      "INFO:aws_job_submitter:Created job 4b527c0d-b134-451c-86a6-9e06d27a352c for batch input/batches/20260202_151144/batch_0150.csv\n",
      "INFO:aws_job_submitter:Created job 11872095-2b16-4535-9ad5-772d77b6db68 for batch input/batches/20260202_151144/batch_0151.csv\n",
      "INFO:aws_job_submitter:Created job 942e3af8-a2b4-48b8-9e54-b8cb17bc386f for batch input/batches/20260202_151144/batch_0152.csv\n",
      "INFO:aws_job_submitter:Created job e722597a-101a-41d6-b7c0-0ea4e739cabf for batch input/batches/20260202_151144/batch_0153.csv\n",
      "INFO:aws_job_submitter:Created job fb6f6852-8b02-4127-857e-c9d002df9e3b for batch input/batches/20260202_151144/batch_0154.csv\n",
      "INFO:aws_job_submitter:Created job e735137c-da34-4c4d-b12f-a071020f4cc8 for batch input/batches/20260202_151144/batch_0155.csv\n",
      "INFO:aws_job_submitter:Created job bfc8ca63-969d-43b2-9ba3-3d2502ab49a3 for batch input/batches/20260202_151144/batch_0156.csv\n",
      "INFO:aws_job_submitter:Created job 1b3c7015-07d0-42eb-8db3-947d6208dbed for batch input/batches/20260202_151144/batch_0157.csv\n",
      "INFO:aws_job_submitter:Created job 92c25818-2465-4fbb-b2f6-0c743a98e58d for batch input/batches/20260202_151144/batch_0158.csv\n",
      "INFO:aws_job_submitter:Created job 3fb38b1c-5b39-4894-9fcf-dac4eeea7ad3 for batch input/batches/20260202_151144/batch_0159.csv\n",
      "INFO:aws_job_submitter:Created job ca1dd27d-0f37-4efc-b267-8679b638a59c for batch input/batches/20260202_151144/batch_0160.csv\n",
      "INFO:aws_job_submitter:Created job d05585e5-eeac-4fcc-b1ec-76fc4169bb85 for batch input/batches/20260202_151144/batch_0161.csv\n",
      "INFO:aws_job_submitter:Created job 53f24efc-041b-49fe-b23b-c357eb5e70c8 for batch input/batches/20260202_151144/batch_0162.csv\n",
      "INFO:aws_job_submitter:Created job 7d02c38f-9adb-4f80-9648-767f199f9389 for batch input/batches/20260202_151144/batch_0163.csv\n",
      "INFO:aws_job_submitter:Created job 6019a72b-ff50-4ff2-b52d-29d7817febb3 for batch input/batches/20260202_151144/batch_0164.csv\n",
      "INFO:aws_job_submitter:Created job 9cc4643b-c274-4082-81aa-e64fbac548a5 for batch input/batches/20260202_151144/batch_0165.csv\n",
      "INFO:aws_job_submitter:Created job 4bfa55ab-eb5c-44e1-8f65-10196a9214c2 for batch input/batches/20260202_151144/batch_0166.csv\n",
      "INFO:aws_job_submitter:Created job f582ed68-b5e0-4b7f-9b74-a842e41ef477 for batch input/batches/20260202_151144/batch_0167.csv\n",
      "INFO:aws_job_submitter:Created job e42667bd-9f28-4df0-8fd1-367c056765f8 for batch input/batches/20260202_151144/batch_0168.csv\n",
      "INFO:aws_job_submitter:Created job 0178999d-8643-4341-9cc4-96834f0b5f31 for batch input/batches/20260202_151144/batch_0169.csv\n",
      "INFO:aws_job_submitter:Created job 2d35b348-3832-40f0-b832-47b807b3f371 for batch input/batches/20260202_151144/batch_0170.csv\n",
      "INFO:aws_job_submitter:Created job 1da39666-bb78-44c7-a004-224761b2c86f for batch input/batches/20260202_151144/batch_0171.csv\n",
      "INFO:aws_job_submitter:Created job d2287493-8887-4cac-92ee-0711e21ca1f7 for batch input/batches/20260202_151144/batch_0172.csv\n",
      "INFO:aws_job_submitter:Created job fcf8d15c-5905-42b3-afbc-eac579eb205e for batch input/batches/20260202_151144/batch_0173.csv\n",
      "INFO:aws_job_submitter:Created job 5808252a-7171-4975-bacf-4b237f39e39d for batch input/batches/20260202_151144/batch_0174.csv\n",
      "INFO:aws_job_submitter:Created job 10106ba0-0397-4159-8a86-e8724a1743d7 for batch input/batches/20260202_151144/batch_0175.csv\n",
      "INFO:aws_job_submitter:Created job 7a291739-0f2f-4d93-bd9a-609bbf4259c9 for batch input/batches/20260202_151144/batch_0176.csv\n",
      "INFO:aws_job_submitter:Created job 762374c6-b6a3-40a5-b131-5d6551569d74 for batch input/batches/20260202_151144/batch_0177.csv\n",
      "INFO:aws_job_submitter:Created job 0c2274ea-4ca3-40b7-8c8d-9092a7aa1f1d for batch input/batches/20260202_151144/batch_0178.csv\n",
      "INFO:aws_job_submitter:Created job 52af3854-1dcc-4db6-b88a-7c7ccb89ea56 for batch input/batches/20260202_151144/batch_0179.csv\n",
      "INFO:aws_job_submitter:Created job d68c9966-f1b1-42af-a7bd-ec38da2a6d7d for batch input/batches/20260202_151144/batch_0180.csv\n",
      "INFO:aws_job_submitter:Created job d0b143fa-99f8-4d91-8416-5005a3d2ea97 for batch input/batches/20260202_151144/batch_0181.csv\n",
      "INFO:aws_job_submitter:Created job b0eb5b0a-2528-4ad4-8b8e-3724ca6a2e7b for batch input/batches/20260202_151144/batch_0182.csv\n",
      "INFO:aws_job_submitter:Created job 10eaab82-b52f-412c-b640-7130403661d1 for batch input/batches/20260202_151144/batch_0183.csv\n",
      "INFO:aws_job_submitter:Created job bd9f674a-e157-4548-8138-2ca69afded18 for batch input/batches/20260202_151144/batch_0184.csv\n",
      "INFO:aws_job_submitter:Created job 43a37d33-fc66-4ff1-897e-0599e87f0189 for batch input/batches/20260202_151144/batch_0185.csv\n",
      "INFO:aws_job_submitter:Created job d18ff267-5979-45ec-a07d-584568562513 for batch input/batches/20260202_151144/batch_0186.csv\n",
      "INFO:aws_job_submitter:Created job 9e08c7ca-30e1-4d58-94a6-d709c0ab933c for batch input/batches/20260202_151144/batch_0187.csv\n",
      "INFO:aws_job_submitter:Created job 019449b3-7a8f-4b53-a3e7-95c86ab84ac8 for batch input/batches/20260202_151144/batch_0188.csv\n",
      "INFO:aws_job_submitter:Created job f864ea8f-4d3f-4ebc-84d2-27cd56ac4cc3 for batch input/batches/20260202_151144/batch_0189.csv\n",
      "INFO:aws_job_submitter:Created job bd5ae155-7de5-4d0d-86c3-39935cb5b0b0 for batch input/batches/20260202_151144/batch_0190.csv\n",
      "INFO:aws_job_submitter:Created job 9dc97308-9aac-4b47-a5ec-dc0de54015ac for batch input/batches/20260202_151144/batch_0191.csv\n",
      "INFO:aws_job_submitter:Created job 20b745a7-6167-4143-9f1b-8f33e0e187a2 for batch input/batches/20260202_151144/batch_0192.csv\n",
      "INFO:aws_job_submitter:Created job 1401996c-c372-4edb-8582-6c469ce8530f for batch input/batches/20260202_151144/batch_0193.csv\n",
      "INFO:aws_job_submitter:Created job af2d5387-79c8-4efd-af6e-a6a17fcac9af for batch input/batches/20260202_151144/batch_0194.csv\n",
      "INFO:aws_job_submitter:Created job afa8f334-0e27-48fe-9591-b4b222799505 for batch input/batches/20260202_151144/batch_0195.csv\n",
      "INFO:aws_job_submitter:Created job 1e9f8254-666e-40dc-a551-4ab80e18616a for batch input/batches/20260202_151144/batch_0196.csv\n",
      "INFO:aws_job_submitter:Created job f75b122e-4b96-4ed8-bae5-bf5fd126f9ce for batch input/batches/20260202_151144/batch_0197.csv\n",
      "INFO:aws_job_submitter:Created job 6fbdc8b0-96a1-40fc-ba5c-7b6db5db8298 for batch input/batches/20260202_151144/batch_0198.csv\n",
      "INFO:aws_job_submitter:Created job a23b90a0-7686-42bd-bbf0-9e90668ed3c2 for batch input/batches/20260202_151144/batch_0199.csv\n",
      "INFO:aws_job_submitter:Created job b0c76dfd-1c9c-43c2-b74d-8d3fa5f29166 for batch input/batches/20260202_151144/batch_0200.csv\n",
      "INFO:aws_job_submitter:Created job 3bf33143-1f99-4e46-a4fd-19920a5a00d4 for batch input/batches/20260202_151144/batch_0201.csv\n",
      "INFO:aws_job_submitter:Created job 65977998-5ec0-4d97-b99f-b3c6e3f853b7 for batch input/batches/20260202_151144/batch_0202.csv\n",
      "INFO:aws_job_submitter:Created job 8f001605-c95d-437f-bd23-4fdaef5b1e32 for batch input/batches/20260202_151144/batch_0203.csv\n",
      "INFO:aws_job_submitter:Created job c07c7cd1-2c7b-40af-8989-16e5a4e47287 for batch input/batches/20260202_151144/batch_0204.csv\n",
      "INFO:aws_job_submitter:Created job 51aeb872-2df1-4c04-a2cc-55dfadca8b74 for batch input/batches/20260202_151144/batch_0205.csv\n",
      "INFO:aws_job_submitter:Created job 020ab63e-c1d6-484c-bd54-d5f29615e423 for batch input/batches/20260202_151144/batch_0206.csv\n",
      "INFO:aws_job_submitter:Created job cd4f9311-1f69-49fe-8aa3-425cfec76ee1 for batch input/batches/20260202_151144/batch_0207.csv\n",
      "INFO:aws_job_submitter:Created job 1fa52b15-cff6-4147-94d6-51463d9949ae for batch input/batches/20260202_151144/batch_0208.csv\n",
      "INFO:aws_job_submitter:Created job 42a11ec0-8eac-4a9b-aa29-b358fa102e29 for batch input/batches/20260202_151144/batch_0209.csv\n",
      "INFO:aws_job_submitter:Created job 7ffa8faa-f7b2-445d-b655-df64177409c8 for batch input/batches/20260202_151144/batch_0210.csv\n",
      "INFO:aws_job_submitter:Created job 18aa72f5-42c9-458d-9ba5-3073106bb6eb for batch input/batches/20260202_151144/batch_0211.csv\n",
      "INFO:aws_job_submitter:Created job 0d80e94a-2ecc-4e5f-b5ed-c6de6e85d98e for batch input/batches/20260202_151144/batch_0212.csv\n",
      "INFO:aws_job_submitter:Created job 518506fb-4b3f-41ed-a6a9-6a38fe639037 for batch input/batches/20260202_151144/batch_0213.csv\n",
      "INFO:aws_job_submitter:Created job a591850c-0d7c-4e84-86ba-e06b0fbf8503 for batch input/batches/20260202_151144/batch_0214.csv\n",
      "INFO:aws_job_submitter:Created job 69c0ae47-6878-4a78-a90a-2e2fab280fea for batch input/batches/20260202_151144/batch_0215.csv\n",
      "INFO:aws_job_submitter:Created job cfd25665-5b98-42b5-bca4-ca1ddb15e2a7 for batch input/batches/20260202_151144/batch_0216.csv\n",
      "INFO:aws_job_submitter:Created job 6f7196e9-9eb3-415b-8295-315e5600380f for batch input/batches/20260202_151144/batch_0217.csv\n",
      "INFO:aws_job_submitter:Created job 7ed2065a-753b-4fe3-b919-53cafe86e2d2 for batch input/batches/20260202_151144/batch_0218.csv\n",
      "INFO:aws_job_submitter:Created job b386bb60-9e52-4067-8f99-47340441a18d for batch input/batches/20260202_151144/batch_0219.csv\n",
      "INFO:aws_job_submitter:Created job 0cf2e939-4666-4ad6-9450-921569d589eb for batch input/batches/20260202_151144/batch_0220.csv\n",
      "INFO:aws_job_submitter:Created job 94d72089-6542-4bc7-b8cb-8274329c27fc for batch input/batches/20260202_151144/batch_0221.csv\n",
      "INFO:aws_job_submitter:Created job b8e9df46-63a5-42a1-9496-addc30abb2ab for batch input/batches/20260202_151144/batch_0222.csv\n",
      "INFO:aws_job_submitter:Created job 26fe6b68-b392-46cc-9d98-d1489569fa70 for batch input/batches/20260202_151144/batch_0223.csv\n",
      "INFO:aws_job_submitter:Created job 42f77117-368b-4e0c-a212-3975da125605 for batch input/batches/20260202_151144/batch_0224.csv\n",
      "INFO:aws_job_submitter:Created job dd3423d3-1e07-4c82-9d35-1d93a63b7500 for batch input/batches/20260202_151144/batch_0225.csv\n",
      "INFO:aws_job_submitter:Created job 2440e5b2-4b34-4b55-a0d7-929c234d9dc7 for batch input/batches/20260202_151144/batch_0226.csv\n",
      "INFO:aws_job_submitter:Created job 65bfbfbf-d62a-443e-bea1-1e0ed6083080 for batch input/batches/20260202_151144/batch_0227.csv\n",
      "INFO:aws_job_submitter:Created job 41d4acd4-1231-4d80-ab60-df03c2205879 for batch input/batches/20260202_151144/batch_0228.csv\n",
      "INFO:aws_job_submitter:Created job 3291e3dd-8f9a-4f95-8b14-6fe545bfcdd0 for batch input/batches/20260202_151144/batch_0229.csv\n",
      "INFO:aws_job_submitter:Created job c169b701-f6bf-48b5-868a-fe223fbd5f97 for batch input/batches/20260202_151144/batch_0230.csv\n",
      "INFO:aws_job_submitter:Created job 3dd6a9f8-cf80-485f-bf32-b9dbf1959b24 for batch input/batches/20260202_151144/batch_0231.csv\n",
      "INFO:aws_job_submitter:Created job 0e245b93-f288-4f93-b1db-84b31730438c for batch input/batches/20260202_151144/batch_0232.csv\n",
      "INFO:aws_job_submitter:Created job 4c47b300-0095-4ee9-b9c3-17f7756e846e for batch input/batches/20260202_151144/batch_0233.csv\n",
      "INFO:aws_job_submitter:Created job 067159fb-61df-41b2-824b-d70157754ef5 for batch input/batches/20260202_151144/batch_0234.csv\n",
      "INFO:aws_job_submitter:Created job 9b9d8fe0-e31a-4ece-ae0d-c6e44aefa5eb for batch input/batches/20260202_151144/batch_0235.csv\n",
      "INFO:aws_job_submitter:Created job 78ee003a-90dc-4528-9272-f08774572e7e for batch input/batches/20260202_151144/batch_0236.csv\n",
      "INFO:aws_job_submitter:Created job eb953d5d-77b6-4251-ac3f-e0db8ff1fd5b for batch input/batches/20260202_151144/batch_0237.csv\n",
      "INFO:aws_job_submitter:Created job d3f21fcb-e215-48d6-95e7-cfc5b5079d60 for batch input/batches/20260202_151144/batch_0238.csv\n",
      "INFO:aws_job_submitter:Created job 5f4f22d5-5f49-45a8-b64c-6c0d9d7fac1e for batch input/batches/20260202_151144/batch_0239.csv\n",
      "INFO:aws_job_submitter:Created job fc1cd28b-ed7a-45b6-8209-f3ece360b2bb for batch input/batches/20260202_151144/batch_0240.csv\n",
      "INFO:aws_job_submitter:Created job 16ce24dd-5026-4b32-875d-6e4a45f994a7 for batch input/batches/20260202_151144/batch_0241.csv\n",
      "INFO:aws_job_submitter:Created job 7b902073-5186-43ce-bdf0-2a888f504645 for batch input/batches/20260202_151144/batch_0242.csv\n",
      "INFO:aws_job_submitter:Created job 672cffec-ea41-445f-a94f-8d280ea2cfa5 for batch input/batches/20260202_151144/batch_0243.csv\n",
      "INFO:aws_job_submitter:Created job 717d6a7f-2212-4027-9083-8d992d5a17a9 for batch input/batches/20260202_151144/batch_0244.csv\n",
      "INFO:aws_job_submitter:Created job 6a0033c7-5633-47f0-aa4b-eaffac503a0d for batch input/batches/20260202_151144/batch_0245.csv\n",
      "INFO:aws_job_submitter:Created job 53fca7ff-de69-4e1b-b34d-0d5974d092db for batch input/batches/20260202_151144/batch_0246.csv\n",
      "INFO:aws_job_submitter:Created job 01b56771-d9d2-43ec-863e-cd9122e57be5 for batch input/batches/20260202_151144/batch_0247.csv\n",
      "INFO:aws_job_submitter:Created job 6b13afda-6326-4b45-a639-7804a456e75a for batch input/batches/20260202_151144/batch_0248.csv\n",
      "INFO:aws_job_submitter:Created job d4e0e287-65ad-4dd3-8be2-c84ead98f06b for batch input/batches/20260202_151144/batch_0249.csv\n",
      "INFO:aws_job_submitter:Created job 67769bb6-a319-40a9-abd4-629e7ba10941 for batch input/batches/20260202_151144/batch_0250.csv\n",
      "INFO:aws_job_submitter:Created job 989e06bc-47fa-4d0e-bffa-430bfdf67ec5 for batch input/batches/20260202_151144/batch_0251.csv\n",
      "INFO:aws_job_submitter:Created job 5df1cdea-4644-491a-9c90-08805916d057 for batch input/batches/20260202_151144/batch_0252.csv\n",
      "INFO:aws_job_submitter:Created job 16c1910a-c466-4057-a985-c5ca0ccf994a for batch input/batches/20260202_151144/batch_0253.csv\n",
      "INFO:aws_job_submitter:Created job 11d24855-3bf3-4769-811f-e72e16a8738e for batch input/batches/20260202_151144/batch_0254.csv\n",
      "INFO:aws_job_submitter:Created job 6ac9d2de-f03e-4b82-a4f2-c5f1db7d3e36 for batch input/batches/20260202_151144/batch_0255.csv\n",
      "INFO:aws_job_submitter:Created job 4a2d1e9e-a9fe-407e-ac51-0f422d35bff3 for batch input/batches/20260202_151144/batch_0256.csv\n",
      "INFO:aws_job_submitter:Created job 9979ac07-06c5-43ad-a559-744df3fd994f for batch input/batches/20260202_151144/batch_0257.csv\n",
      "INFO:aws_job_submitter:Created job 45779527-1e11-4b1c-855e-d0f1d8c45c44 for batch input/batches/20260202_151144/batch_0258.csv\n",
      "INFO:aws_job_submitter:Created job 11500470-4252-40e8-9ed0-e5c0003f8b13 for batch input/batches/20260202_151144/batch_0259.csv\n",
      "INFO:aws_job_submitter:Created job 663abea4-6109-41ad-9ab1-b090d454da5a for batch input/batches/20260202_151144/batch_0260.csv\n",
      "INFO:aws_job_submitter:Created job e2e8764b-e533-48c0-bb62-c60f490f30bf for batch input/batches/20260202_151144/batch_0261.csv\n",
      "INFO:aws_job_submitter:Created job 3373efc7-134a-4b3f-8efe-201a2bdae2aa for batch input/batches/20260202_151144/batch_0262.csv\n",
      "INFO:aws_job_submitter:Created job 781b8e25-344e-4ce4-8ffd-e16da452b055 for batch input/batches/20260202_151144/batch_0263.csv\n",
      "INFO:aws_job_submitter:Created job ffdb2422-21a1-4b98-80de-8cc0578328d6 for batch input/batches/20260202_151144/batch_0264.csv\n",
      "INFO:aws_job_submitter:Created job 1af6aa78-3f09-4974-a9e8-5a8d556180cc for batch input/batches/20260202_151144/batch_0265.csv\n",
      "INFO:aws_job_submitter:Created job 221600f6-6979-41ab-8c56-b2845997f426 for batch input/batches/20260202_151144/batch_0266.csv\n",
      "INFO:aws_job_submitter:Created job 82c465a6-1d1a-40ba-98e5-9d9e2897eb05 for batch input/batches/20260202_151144/batch_0267.csv\n",
      "INFO:aws_job_submitter:Created job d1364599-10e9-4657-aaca-bfea5d919153 for batch input/batches/20260202_151144/batch_0268.csv\n",
      "INFO:aws_job_submitter:Created job 952fe4bf-8cc6-429a-96d3-c3fcc77a98c7 for batch input/batches/20260202_151144/batch_0269.csv\n",
      "INFO:aws_job_submitter:Created job c7e71bae-3797-4e39-bfaa-b7542843c332 for batch input/batches/20260202_151144/batch_0270.csv\n",
      "INFO:aws_job_submitter:Created job 8237993c-de36-406b-87ed-5629fd434d64 for batch input/batches/20260202_151144/batch_0271.csv\n",
      "INFO:aws_job_submitter:Created job 09c470f9-9d74-41cc-a70c-df7fcb7203ee for batch input/batches/20260202_151144/batch_0272.csv\n",
      "INFO:aws_job_submitter:Created job e0f913b0-88cd-4692-878a-cca6c22541a3 for batch input/batches/20260202_151144/batch_0273.csv\n",
      "INFO:aws_job_submitter:Created job 75f496ee-aa10-43fb-bb45-14393fe5c1da for batch input/batches/20260202_151144/batch_0274.csv\n",
      "INFO:aws_job_submitter:Created job f0da35e0-566c-43d5-b49e-e2630ac8d66b for batch input/batches/20260202_151144/batch_0275.csv\n",
      "INFO:aws_job_submitter:Created job 3a728fbd-354a-4d34-9cf9-b7bc692b470c for batch input/batches/20260202_151144/batch_0276.csv\n",
      "INFO:aws_job_submitter:Created job a69a330b-f437-470f-bc4d-96b587cab765 for batch input/batches/20260202_151144/batch_0277.csv\n",
      "INFO:aws_job_submitter:Created job 7b91680f-e63d-467e-b3a1-d53e57127a97 for batch input/batches/20260202_151144/batch_0278.csv\n",
      "INFO:aws_job_submitter:Created job 17d769a4-2127-4bb5-9f2e-69718e711352 for batch input/batches/20260202_151144/batch_0279.csv\n",
      "INFO:aws_job_submitter:Created job 82a649d7-91c6-44ab-ba72-f64699172045 for batch input/batches/20260202_151144/batch_0280.csv\n",
      "INFO:aws_job_submitter:Created job bd4cc3a4-6919-4f0f-a6e3-86cbacb449e3 for batch input/batches/20260202_151144/batch_0281.csv\n",
      "INFO:aws_job_submitter:Created job 3e4721b7-d040-413a-832d-574b36802a96 for batch input/batches/20260202_151144/batch_0282.csv\n",
      "INFO:aws_job_submitter:Created job debc3405-ebf8-4f90-8348-31adde91566d for batch input/batches/20260202_151144/batch_0283.csv\n",
      "INFO:aws_job_submitter:Created job 66888b64-c73b-4bbb-a231-97f0f379dd2d for batch input/batches/20260202_151144/batch_0284.csv\n",
      "INFO:aws_job_submitter:Created job fa746bac-312a-42ff-9c61-267c4583164f for batch input/batches/20260202_151144/batch_0285.csv\n",
      "INFO:aws_job_submitter:Created job aa3adc74-9fee-4470-9718-be4809a9f141 for batch input/batches/20260202_151144/batch_0286.csv\n",
      "INFO:aws_job_submitter:Created job 9654fd3a-9bc3-47c8-85a3-b79f164c7a37 for batch input/batches/20260202_151144/batch_0287.csv\n",
      "INFO:aws_job_submitter:Created job abc208e6-5820-4d3b-b300-8f62125e5260 for batch input/batches/20260202_151144/batch_0288.csv\n",
      "INFO:aws_job_submitter:Created job bf2c9530-4eb1-4d53-9230-10d7e98932de for batch input/batches/20260202_151144/batch_0289.csv\n",
      "INFO:aws_job_submitter:Created job fb8076db-3dde-4c91-aa1d-7f2348918906 for batch input/batches/20260202_151144/batch_0290.csv\n",
      "INFO:aws_job_submitter:Created job a35bc6b4-381e-4746-b47d-67e59125e0c4 for batch input/batches/20260202_151144/batch_0291.csv\n",
      "INFO:aws_job_submitter:Created job 6543be65-c84a-4a34-9f93-b79e12ee10b9 for batch input/batches/20260202_151144/batch_0292.csv\n",
      "INFO:aws_job_submitter:Created job ae31c645-a9d6-4201-ba02-8ec5e5f297e6 for batch input/batches/20260202_151144/batch_0293.csv\n",
      "INFO:aws_job_submitter:Created job e9dc53d9-c5f8-4021-a24e-369c37e5200f for batch input/batches/20260202_151144/batch_0294.csv\n",
      "INFO:aws_job_submitter:Created job 16d6aeb4-8644-4c22-a219-d18fc839a080 for batch input/batches/20260202_151144/batch_0295.csv\n",
      "INFO:aws_job_submitter:Created job 2f9a92b7-a2a6-4529-99f9-ba3656bbbe1c for batch input/batches/20260202_151144/batch_0296.csv\n",
      "INFO:aws_job_submitter:Created job 896d5a98-5e85-4606-8910-116134385ecc for batch input/batches/20260202_151144/batch_0297.csv\n",
      "INFO:aws_job_submitter:Created job 5a7f8314-32b0-486d-89c4-d767da4ef6b0 for batch input/batches/20260202_151144/batch_0298.csv\n",
      "INFO:aws_job_submitter:Created job 42d2dc94-8085-4703-9cf1-c8b5d6fc2de6 for batch input/batches/20260202_151144/batch_0299.csv\n",
      "INFO:aws_job_submitter:Created job be713554-63ea-4bd5-81cf-817d9f5d07f2 for batch input/batches/20260202_151144/batch_0300.csv\n",
      "INFO:aws_job_submitter:Created job 65f0e0e4-4092-46b9-9614-aa8d548105e8 for batch input/batches/20260202_151144/batch_0301.csv\n",
      "INFO:aws_job_submitter:Created job da9274e3-b951-4474-a170-78e26fd6d810 for batch input/batches/20260202_151144/batch_0302.csv\n",
      "INFO:aws_job_submitter:Created job 67194bd8-6d86-44f9-8c54-031b0d7803d8 for batch input/batches/20260202_151144/batch_0303.csv\n",
      "INFO:aws_job_submitter:Created job 4ca17c5e-8b87-4eef-a1c8-1455a2494d2c for batch input/batches/20260202_151144/batch_0304.csv\n",
      "INFO:aws_job_submitter:Created job e08a5001-9348-47f1-99b9-f60556379241 for batch input/batches/20260202_151144/batch_0305.csv\n",
      "INFO:aws_job_submitter:Created job c1128e1a-dfe6-4347-91be-f6182022f0f0 for batch input/batches/20260202_151144/batch_0306.csv\n",
      "INFO:aws_job_submitter:Created job 2d8c678c-3317-4114-b014-d183aa56232c for batch input/batches/20260202_151144/batch_0307.csv\n",
      "INFO:aws_job_submitter:Created job 072ed0b4-7d12-4d4b-b74d-10ca4966eaa6 for batch input/batches/20260202_151144/batch_0308.csv\n",
      "INFO:aws_job_submitter:Created job 1f7912bb-f34a-4bed-b213-ba830e1544fd for batch input/batches/20260202_151144/batch_0309.csv\n",
      "INFO:aws_job_submitter:Created job 18c8d928-2790-48e9-acaf-c0e83ff3a4cd for batch input/batches/20260202_151144/batch_0310.csv\n",
      "INFO:aws_job_submitter:Created job e407bc08-54db-4595-887b-ad8f9adfb7bd for batch input/batches/20260202_151144/batch_0311.csv\n",
      "INFO:aws_job_submitter:Created job 6c1c495e-4a16-462a-bf0e-563e72e45a2b for batch input/batches/20260202_151144/batch_0312.csv\n",
      "INFO:aws_job_submitter:Created job 7335b857-231e-4ee1-a335-87b47fd04903 for batch input/batches/20260202_151144/batch_0313.csv\n",
      "INFO:aws_job_submitter:Created job 9013bf91-430e-4531-a100-2851c137a917 for batch input/batches/20260202_151144/batch_0314.csv\n",
      "INFO:aws_job_submitter:Created job 482d3c31-fe9a-42ea-adbd-9174f6ecdd34 for batch input/batches/20260202_151144/batch_0315.csv\n",
      "INFO:aws_job_submitter:Created job 704d5401-a5bb-4fc3-9a96-718fa4dcfd09 for batch input/batches/20260202_151144/batch_0316.csv\n",
      "INFO:aws_job_submitter:Created job 6786fc23-cc53-4709-9686-b8b09c64a8a9 for batch input/batches/20260202_151144/batch_0317.csv\n",
      "INFO:aws_job_submitter:Created job dd8c603b-ec3a-41e5-bd2b-ea848e0a7ae4 for batch input/batches/20260202_151144/batch_0318.csv\n",
      "INFO:aws_job_submitter:Created job 99ba3a01-de8b-465b-a74e-1b1e1aedb7c9 for batch input/batches/20260202_151144/batch_0319.csv\n",
      "INFO:aws_job_submitter:Created job e38c8bcc-73c0-4dcd-9867-12776487bcf7 for batch input/batches/20260202_151144/batch_0320.csv\n",
      "INFO:aws_job_submitter:Created job b0574b72-de73-41be-ac30-76447bbf5023 for batch input/batches/20260202_151144/batch_0321.csv\n",
      "INFO:aws_job_submitter:Created job c9341347-43a2-41e9-b4ff-b69d8b67fbb9 for batch input/batches/20260202_151144/batch_0322.csv\n",
      "INFO:aws_job_submitter:Created job 19350769-3cc9-4d11-9dbd-6a79942ccc35 for batch input/batches/20260202_151144/batch_0323.csv\n",
      "INFO:aws_job_submitter:Created job bae62314-f13d-4a51-b32d-8c1778f2de2b for batch input/batches/20260202_151144/batch_0324.csv\n",
      "INFO:aws_job_submitter:Created job 391e6643-e748-4004-8121-24fd478008c4 for batch input/batches/20260202_151144/batch_0325.csv\n",
      "INFO:aws_job_submitter:Created job a5174505-ea30-4965-9757-0063cf16ab42 for batch input/batches/20260202_151144/batch_0326.csv\n",
      "INFO:aws_job_submitter:Created job 18336274-ef9b-48f3-a6a1-98046590d41b for batch input/batches/20260202_151144/batch_0327.csv\n",
      "INFO:aws_job_submitter:Created job 511c603d-d41e-4fe1-8a58-4673eee60ac1 for batch input/batches/20260202_151144/batch_0328.csv\n",
      "INFO:aws_job_submitter:Created job 6e305bba-bc17-475a-84ce-eee88d009d2d for batch input/batches/20260202_151144/batch_0329.csv\n",
      "INFO:aws_job_submitter:Created job 86a80828-e20e-4501-b7b4-af1a192b465b for batch input/batches/20260202_151144/batch_0330.csv\n",
      "INFO:aws_job_submitter:Created job 848dac61-8fff-42f5-bb2e-df09ccd5dafc for batch input/batches/20260202_151144/batch_0331.csv\n",
      "INFO:aws_job_submitter:Created job 4b2d69b1-ed84-4316-9e86-a764f975187b for batch input/batches/20260202_151144/batch_0332.csv\n",
      "INFO:aws_job_submitter:Created job 3bd90da0-d7b7-465b-8523-07f0cbee4fe0 for batch input/batches/20260202_151144/batch_0333.csv\n",
      "INFO:aws_job_submitter:Created job 901ca6e0-0af6-4909-8819-4e64f92500c4 for batch input/batches/20260202_151144/batch_0334.csv\n",
      "INFO:aws_job_submitter:Created job 37411243-fccf-4600-b6e0-f9fa4f180c74 for batch input/batches/20260202_151144/batch_0335.csv\n",
      "INFO:aws_job_submitter:Created job 55a07f27-3777-4dc8-843d-31b5f2db2b47 for batch input/batches/20260202_151144/batch_0336.csv\n",
      "INFO:aws_job_submitter:Created job 8782f0a4-e228-448a-b42c-1584e4c37e19 for batch input/batches/20260202_151144/batch_0337.csv\n",
      "INFO:aws_job_submitter:Created job fe4104e4-d310-4d39-87a8-72ba30800bae for batch input/batches/20260202_151144/batch_0338.csv\n",
      "INFO:aws_job_submitter:Created job d983da9e-71b3-4e12-9477-b2b39da82279 for batch input/batches/20260202_151144/batch_0339.csv\n",
      "INFO:aws_job_submitter:Created job a2b9a3da-3e27-4763-835c-d752a4c48c51 for batch input/batches/20260202_151144/batch_0340.csv\n",
      "INFO:aws_job_submitter:Created job 08b0346f-6c38-4cbb-8354-09a662b1a1b0 for batch input/batches/20260202_151144/batch_0341.csv\n",
      "INFO:aws_job_submitter:Created job aa8f2253-2a2e-4ae7-af35-650c9a47a690 for batch input/batches/20260202_151144/batch_0342.csv\n",
      "INFO:aws_job_submitter:Created job 3641ef26-e9bf-44ea-b025-e30183eac8c2 for batch input/batches/20260202_151144/batch_0343.csv\n",
      "INFO:aws_job_submitter:Created job d0e4a751-6d17-47ec-90c4-67706746eae8 for batch input/batches/20260202_151144/batch_0344.csv\n",
      "INFO:aws_job_submitter:Created job 70ae73d3-b1d2-4ef1-b410-3a823d3c7973 for batch input/batches/20260202_151144/batch_0345.csv\n",
      "INFO:aws_job_submitter:Created job 88d10892-34c4-4b2c-81ac-c7e8d9652b85 for batch input/batches/20260202_151144/batch_0346.csv\n",
      "INFO:aws_job_submitter:Created job 0082a684-cb48-4345-ad1f-f2cc24ccf6e6 for batch input/batches/20260202_151144/batch_0347.csv\n",
      "INFO:aws_job_submitter:Created job 7ba58a9e-dfc8-4f07-bff7-74b8d4b384ec for batch input/batches/20260202_151144/batch_0348.csv\n",
      "INFO:aws_job_submitter:Created job 980cf929-4ca8-4c26-91f0-b1cdeb203989 for batch input/batches/20260202_151144/batch_0349.csv\n",
      "INFO:aws_job_submitter:Created job 0616a0f9-a4e1-4f62-b967-9f32f8f0b269 for batch input/batches/20260202_151144/batch_0350.csv\n",
      "INFO:aws_job_submitter:Created job 68006627-e3e1-4604-b59a-8fc435e819b3 for batch input/batches/20260202_151144/batch_0351.csv\n",
      "INFO:aws_job_submitter:Created job 2a928208-63de-4b38-964c-fe2dffce1ba3 for batch input/batches/20260202_151144/batch_0352.csv\n",
      "INFO:aws_job_submitter:Created job 8f4dc563-696b-4a0f-bc69-44532f824b1f for batch input/batches/20260202_151144/batch_0353.csv\n",
      "INFO:aws_job_submitter:Created job 17593970-62eb-441b-af1f-f6ec6107a60b for batch input/batches/20260202_151144/batch_0354.csv\n",
      "INFO:aws_job_submitter:Created job 7a8185cb-dcb0-4b93-b0a8-c29021de1aee for batch input/batches/20260202_151144/batch_0355.csv\n",
      "INFO:aws_job_submitter:Created job 25e6c7bd-a44d-4112-aa8a-e93014965236 for batch input/batches/20260202_151144/batch_0356.csv\n",
      "INFO:aws_job_submitter:Created job 385e9575-0207-4a06-8b6f-30c1f6cbb790 for batch input/batches/20260202_151144/batch_0357.csv\n",
      "INFO:aws_job_submitter:Created job 42378d53-e8ff-411b-a96f-13642f465f28 for batch input/batches/20260202_151144/batch_0358.csv\n",
      "INFO:aws_job_submitter:Created job 3b5ac948-ce0c-493d-992e-ca2e66c7d297 for batch input/batches/20260202_151144/batch_0359.csv\n",
      "INFO:aws_job_submitter:Created job 5d7766a7-c7cf-4606-8171-4e7ca1f91405 for batch input/batches/20260202_151144/batch_0360.csv\n",
      "INFO:aws_job_submitter:Created job c9a93b05-197f-461d-b251-bf22dccc8dd3 for batch input/batches/20260202_151144/batch_0361.csv\n",
      "INFO:aws_job_submitter:Created job a03267e7-5a33-45aa-bc01-c637b51e3423 for batch input/batches/20260202_151144/batch_0362.csv\n",
      "INFO:aws_job_submitter:Created job e46f5729-6179-4b9f-8e7f-c8b56654456b for batch input/batches/20260202_151144/batch_0363.csv\n",
      "INFO:aws_job_submitter:Created 363 jobs\n",
      "INFO:aws_job_submitter:Launching 2 spot instances of type t3.medium\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 363 jobs in queue\n",
      "\n",
      "ðŸš€ Launching 2 spot instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:aws_job_submitter:Error launching spot instances: An error occurred (InvalidParameterValue) when calling the RequestSpotInstances operation: Invalid BASE64 encoding of user data\n",
      "INFO:aws_job_submitter:You can manually launch instances and run the worker script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Spot launch failed. Manual launch instructions:\n",
      "   aws ec2 run-instances --image-id ami-0c02fb55b34e3f5e2 \\\n",
      "     --instance-type t3.medium --iam-instance-profile Name=transcript-scorer-ec2-role-profile\n",
      "\n",
      "======================================================================\n",
      "âœ… JOBS SUBMITTED - YOU CAN NOW CLOSE THIS NOTEBOOK\n",
      "======================================================================\n",
      "\n",
      "ðŸ’¡ To check progress later, run in terminal:\n",
      "   python aws_monitor.py --bucket transcript-scoring-1770013499 --watch\n",
      "\n",
      "ðŸ“¥ To download results when done:\n",
      "   python aws_monitor.py --bucket transcript-scoring-1770013499 --download all_scored_transcripts_2015_2025.csv\n",
      "\n",
      "â±ï¸  Expected completion: ~363 minutes\n",
      "ðŸ’° Estimated cost: $18.15 (API) + $0.10 (EC2)\n",
      "\n",
      "ðŸ“ Job details saved to: aws_job_info.txt\n",
      "\n",
      "ðŸŽ‰ All set! Workers are processing in the background.\n",
      "   You can safely close your laptop now.\n"
     ]
    }
   ],
   "source": [
    "USE_AWS = True  # Set to False to use local scoring\n",
    "\n",
    "# Check if scoring_transcripts is defined\n",
    "if 'scoring_transcripts' not in dir() or 'save_path' not in dir():\n",
    "    print(\"ERROR: scoring_transcripts is not defined!\")\n",
    "    print(\"\\nPlease run the cell above (Choose scoring mode) first.\")\n",
    "    raise NameError(\"Run the 'Choose scoring mode' cell first to define scoring_transcripts\")\n",
    "\n",
    "if USE_AWS:\n",
    "    from aws_job_submitter import AWSJobSubmitter\n",
    "    from aws_monitor import AWSJobMonitor\n",
    "    import boto3\n",
    "    \n",
    "    # AWS Configuration\n",
    "    AWS_BUCKET = \"transcript-scoring-1770013499\"\n",
    "    AWS_REGION = \"us-east-1\"\n",
    "    IAM_INSTANCE_PROFILE = \"transcript-scorer-ec2-role-profile\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"AWS SPOT INSTANCE SCORING - FIRE & FORGET MODE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print()\n",
    "    \n",
    "    # Quick validation\n",
    "    s3 = boto3.client('s3', region_name=AWS_REGION)\n",
    "    s3.head_bucket(Bucket=AWS_BUCKET)\n",
    "    print(f\"[OK] S3 bucket validated\")\n",
    "    \n",
    "    # Initialize submitter\n",
    "    submitter = AWSJobSubmitter(AWS_BUCKET, AWS_REGION)\n",
    "    \n",
    "    # Create batches and upload to S3\n",
    "    print(f\"\\nProcessing {len(scoring_transcripts)} transcripts...\")\n",
    "    batch_keys = submitter.create_batch_files(scoring_transcripts, batch_size=50)\n",
    "    print(f\"[OK] Created {len(batch_keys)} batches -> uploaded to S3\")\n",
    "    \n",
    "    # Create jobs in DynamoDB\n",
    "    job_ids = submitter.create_jobs(batch_keys)\n",
    "    print(f\"[OK] Created {len(job_ids)} jobs in queue\")\n",
    "    \n",
    "    # Launch spot instances\n",
    "    print(f\"\\nLaunching 2 spot instances...\")\n",
    "    spot_requests = submitter.launch_spot_instances(\n",
    "        num_instances=2,\n",
    "        instance_type='t3.medium',\n",
    "        iam_instance_profile=IAM_INSTANCE_PROFILE\n",
    "    )\n",
    "    \n",
    "    if spot_requests:\n",
    "        print(f\"[OK] Launched {len(spot_requests)} spot instance requests\")\n",
    "        print(f\"     Workers will start within 2-5 minutes\")\n",
    "    else:\n",
    "        print(\"WARNING: Spot launch failed. Manual launch instructions:\")\n",
    "        print(f\"   aws ec2 run-instances --image-id ami-0c02fb55b34e3f5e2 \\\\\")\n",
    "        print(f\"     --instance-type t3.medium --iam-instance-profile Name={IAM_INSTANCE_PROFILE}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"JOBS SUBMITTED - YOU CAN NOW CLOSE THIS NOTEBOOK\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"To check progress later, run in terminal:\")\n",
    "    print(f\"   python aws_monitor.py --bucket {AWS_BUCKET} --watch\")\n",
    "    print()\n",
    "    print(\"To download results when done:\")\n",
    "    print(f\"   python aws_monitor.py --bucket {AWS_BUCKET} --download {save_path}\")\n",
    "    print()\n",
    "    print(\"Expected completion: ~{} minutes\".format(len(job_ids) * 2 // 2))  # 2 workers\n",
    "    print(f\"Estimated cost: ${len(job_ids) * 50 * 0.001:.2f} (API) + $0.10 (EC2)\")\n",
    "    print()\n",
    "    \n",
    "    # Store config for later use\n",
    "    print(\"Job details saved to: aws_job_info.txt\")\n",
    "    with open('aws_job_info.txt', 'w') as f:\n",
    "        f.write(f\"Bucket: {AWS_BUCKET}\\n\")\n",
    "        f.write(f\"Region: {AWS_REGION}\\n\")\n",
    "        f.write(f\"Jobs: {len(job_ids)}\\n\")\n",
    "        f.write(f\"Batches: {len(batch_keys)}\\n\")\n",
    "        f.write(f\"Output: {save_path}\\n\")\n",
    "        f.write(f\"Started: {datetime.now()}\\n\")\n",
    "        f.write(f\"\\nMonitor: python aws_monitor.py --bucket {AWS_BUCKET} --watch\\n\")\n",
    "        f.write(f\"Download: python aws_monitor.py --bucket {AWS_BUCKET} --download {save_path}\\n\")\n",
    "    \n",
    "    print(\"\\nAll set! Workers are processing in the background.\")\n",
    "    print(\"You can safely close your laptop now.\")\n",
    "    \n",
    "else:\n",
    "    # Local scoring (blocks execution)\n",
    "    print(\"=\" * 70)\n",
    "    print(\"LOCAL SCORING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"WARNING: This will block your notebook for hours!\")\n",
    "    print(f\"TIP: Set USE_AWS = True to run in background\\n\")\n",
    "    \n",
    "    scored_data = score_quarter_transcripts(\n",
    "        scoring_transcripts, \n",
    "        scorer, \n",
    "        save_path=save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scoring function with progress tracking\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def score_quarter_transcripts(transcripts_df, scorer, save_path='scored_transcripts.csv'):\n",
    "    \"\"\"\n",
    "    Score all transcripts with progress tracking, checkpointing, and error handling.\n",
    "    \"\"\"\n",
    "    # First, inspect the data structure\n",
    "    print(\"Inspecting data structure...\")\n",
    "    print(f\"Type: {type(transcripts_df)}\")\n",
    "    print(f\"Columns: {transcripts_df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst row type: {type(transcripts_df.iloc[0])}\")\n",
    "    print(f\"First row preview:\")\n",
    "    print(transcripts_df.iloc[0])\n",
    "    \n",
    "    print(f\"\\nScoring {len(transcripts_df)} transcripts...\")\n",
    "    print(f\"Estimated cost: ${len(transcripts_df) * 0.001:.2f} (GPT-4o-mini)\")\n",
    "    print(f\"Estimated time: {len(transcripts_df) * 2 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Check for existing progress\n",
    "    try:\n",
    "        existing = pd.read_csv(save_path)\n",
    "        already_scored = set(existing['symbol'] + '_' + existing['date'].astype(str))\n",
    "        print(f\"Found {len(already_scored)} previously scored transcripts\")\n",
    "    except FileNotFoundError:\n",
    "        already_scored = set()\n",
    "        existing = pd.DataFrame()\n",
    "    \n",
    "    scored_results = []\n",
    "    errors = []\n",
    "    \n",
    "    # Determine transcript column name - check what's actually in the DataFrame\n",
    "    available_cols = transcripts_df.columns.tolist()\n",
    "    transcript_col = None\n",
    "    \n",
    "    for possible_name in ['transcript', 'text', 'content', 'full_text', 'body']:\n",
    "        if possible_name in available_cols:\n",
    "            transcript_col = possible_name\n",
    "            break\n",
    "    \n",
    "    if transcript_col is None:\n",
    "        print(f\"ERROR: Could not find transcript column. Available columns: {available_cols}\")\n",
    "        return existing if len(existing) > 0 else pd.DataFrame()\n",
    "    \n",
    "    print(f\"Using transcript column: '{transcript_col}'\")\n",
    "    \n",
    "    # Convert to dict records for easier iteration\n",
    "    records = transcripts_df.to_dict('records')\n",
    "    \n",
    "    for idx, row in enumerate(tqdm(records, desc=\"Scoring\")):\n",
    "        # Handle different possible column names\n",
    "        symbol = row.get('symbol') or row.get('ticker') or 'UNKNOWN'\n",
    "        date = row.get('date') or row.get('filing_date') or 'UNKNOWN'\n",
    "        transcript_id = f\"{symbol}_{date}\"\n",
    "        \n",
    "        # Skip if already scored\n",
    "        if transcript_id in already_scored:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get the transcript text\n",
    "            transcript_text = row.get(transcript_col, '')\n",
    "            \n",
    "            if not transcript_text or transcript_text == '':\n",
    "                errors.append({'symbol': symbol, 'date': date, 'error': 'Empty transcript'})\n",
    "                continue\n",
    "            \n",
    "            # Score transcript - wrap in expected dictionary format\n",
    "            # The scorer expects a dict with 'full_text' key\n",
    "            transcript_dict = {'full_text': transcript_text}\n",
    "            result = scorer.score_transcript(transcript_dict, use_md_a_only=False)\n",
    "            score = result['firm_score']\n",
    "            \n",
    "            if score is None:\n",
    "                errors.append({'symbol': symbol, 'date': date, 'error': 'Scoring returned None'})\n",
    "                continue\n",
    "            \n",
    "            scored_results.append({\n",
    "                'symbol': symbol,\n",
    "                'date': date,\n",
    "                'score': score,\n",
    "                'transcript_length': len(str(transcript_text))\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint every 50 transcripts\n",
    "            if len(scored_results) % 50 == 0:\n",
    "                temp_df = pd.DataFrame(scored_results)\n",
    "                combined = pd.concat([existing, temp_df], ignore_index=True)\n",
    "                combined.to_csv(save_path, index=False)\n",
    "                print(f\"\\nCheckpoint: Saved {len(combined)} scores\")\n",
    "            \n",
    "            # Rate limiting (to avoid API limits)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append({'symbol': symbol, 'date': date, 'error': str(e)})\n",
    "            if idx < 5:  # Only print first few errors in detail\n",
    "                print(f\"\\nError scoring {symbol}: {e}\")\n",
    "    \n",
    "    # Final save - handle case where nothing was scored\n",
    "    if scored_results:\n",
    "        final_df = pd.DataFrame(scored_results)\n",
    "        combined = pd.concat([existing, final_df], ignore_index=True)\n",
    "        combined.to_csv(save_path, index=False)\n",
    "        print(f\"\\nSaved {len(combined)} total scored transcripts to {save_path}\")\n",
    "    elif len(existing) > 0:\n",
    "        combined = existing\n",
    "        print(f\"\\nNo new transcripts scored. Returning {len(existing)} existing scores.\")\n",
    "    else:\n",
    "        combined = pd.DataFrame(columns=['symbol', 'date', 'score', 'transcript_length'])\n",
    "        print(\"\\nWARNING: No transcripts were scored successfully!\")\n",
    "    \n",
    "    if errors:\n",
    "        error_df = pd.DataFrame(errors)\n",
    "        error_df.to_csv('scoring_errors.csv', index=False)\n",
    "        print(f\"\\nWARNING: {len(errors)} errors occurred (saved to scoring_errors.csv)\")\n",
    "        print(f\"First few unique errors:\")\n",
    "        unique_errors = error_df['error'].value_counts().head(3)\n",
    "        for error_msg, count in unique_errors.items():\n",
    "            print(f\"  {error_msg}: {count} occurrences\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"Scoring function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data structure before scoring (Optional)\n",
    "print(\"Data structure inspection:\")\n",
    "print(f\"Type of scoring_transcripts: {type(scoring_transcripts)}\")\n",
    "print(f\"Shape: {scoring_transcripts.shape}\")\n",
    "print(f\"Columns: {scoring_transcripts.columns.tolist()}\")\n",
    "print(f\"\\nFirst transcript preview:\")\n",
    "print(scoring_transcripts.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting scoring at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scored_data = score_quarter_transcripts(\n",
    "    scoring_transcripts, \n",
    "    scorer, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Total scored: {len(scored_data)}\")\n",
    "print(f\"  Date range: {scored_data['date'].min()} to {scored_data['date'].max()}\")\n",
    "print(f\"  Average score: {scored_data['score'].mean():.2f}\")\n",
    "print(f\"  Score distribution:\")\n",
    "print(scored_data['score'].value_counts().sort_index())\n",
    "print(f\"\\nSaved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate scored transcripts into quarterly AGG scores\n",
    "print(\"Aggregating individual scores into quarterly AGG scores...\")\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "if isinstance(scored_data, pd.DataFrame):\n",
    "    scored_df = scored_data.copy()\n",
    "else:\n",
    "    scored_df = pd.DataFrame(scored_data)\n",
    "\n",
    "# Ensure date column is datetime\n",
    "scored_df['date'] = pd.to_datetime(scored_df['date'])\n",
    "scored_df['year'] = scored_df['date'].dt.year\n",
    "scored_df['quarter'] = scored_df['date'].dt.quarter\n",
    "\n",
    "# Group by quarter and calculate aggregate score\n",
    "agg_scores = scored_df.groupby(['year', 'quarter']).agg({\n",
    "    'score': ['mean', 'std', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "agg_scores.columns = ['year', 'quarter', 'agg_score', 'score_std', 'num_firms']\n",
    "\n",
    "# Create quarter date\n",
    "agg_scores['date'] = pd.to_datetime(\n",
    "    agg_scores['year'].astype(str) + '-Q' + agg_scores['quarter'].astype(str)\n",
    ")\n",
    "\n",
    "# Reorder columns\n",
    "final_agg_scores = agg_scores[['date', 'year', 'quarter', 'agg_score', 'score_std', 'num_firms']]\n",
    "\n",
    "# Save AGG scores\n",
    "agg_filename = 'test_agg_scores_2024_2025.csv' if TEST_MODE else 'agg_scores_2015_2025.csv'\n",
    "final_agg_scores.to_csv(agg_filename, index=False)\n",
    "print(f\"\\nSUCCESS: Saved {len(final_agg_scores)} quarterly AGG scores to {agg_filename}\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAGG Scores Summary:\")\n",
    "print(final_agg_scores)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Quarters covered: {len(final_agg_scores)}\")\n",
    "print(f\"  Date range: {final_agg_scores['date'].min().strftime('%Y-%m-%d')} to {final_agg_scores['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Mean AGG score: {final_agg_scores['agg_score'].mean():.3f}\")\n",
    "print(f\"  Std AGG score: {final_agg_scores['agg_score'].std():.3f}\")\n",
    "print(f\"  Average firms/quarter: {final_agg_scores['num_firms'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer('config.yaml')\n",
    "\n",
    "# Load real AGG scores from saved file or create from actual transcript scoring\n",
    "try:\n",
    "    agg_scores = pd.read_csv('agg_scores.csv')\n",
    "    agg_scores['date'] = pd.to_datetime(agg_scores['date'])\n",
    "    print(f\"âœ“ Loaded real AGG scores from file: {len(agg_scores)} quarters\")\n",
    "    print(agg_scores.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  No saved AGG scores found. You need to:\")\n",
    "    print(\"  1. Score earnings transcripts using LLMScorer.score_multiple_transcripts()\")\n",
    "    print(\"  2. Aggregate scores by quarter using aggregate_scores_by_quarter()\")\n",
    "    print(\"  3. Save to 'agg_scores.csv'\")\n",
    "    print(\"\\n For demonstration, showing expected data structure...\")\n",
    "    # Show expected structure instead of generating synthetic data\n",
    "    agg_scores = pd.DataFrame({\n",
    "        'date': pd.date_range(start='2015-01-01', end='2023-12-31', freq='Q'),\n",
    "        'year': [],\n",
    "        'quarter': [],\n",
    "        'agg_score': []  # Real scores would be 1-5 from LLM\n",
    "    })\n",
    "    print(\"\\nExpected columns: date, year, quarter, agg_score\")\n",
    "    print(\"Cannot proceed with feature engineering without real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize scores (only if we have real data)\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    normalized = engineer.normalize_scores(agg_scores, method='zscore', window=20)\n",
    "    print(\"\\nNormalized Scores:\")\n",
    "\n",
    "    print(normalized[['date', 'agg_score', 'agg_score_norm']].head(10))    normalized = pd.DataFrame()\n",
    "\n",
    "else:    print(\"âš  Cannot normalize without real AGG scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delta features (only if we have normalized data)\n",
    "if len(normalized) > 0:\n",
    "    with_deltas = engineer.create_delta_features(normalized)\n",
    "    print(\"\\nDelta Features:\")\n",
    "\n",
    "    print(with_deltas[['date', 'agg_score', 'yoy_change', 'qoq_change', 'momentum']].tail(10))    with_deltas = pd.DataFrame()\n",
    "\n",
    "else:    print(\"âš  Cannot create delta features without normalized scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AGG score and deltas (only if we have features)\n",
    "if len(with_deltas) > 0:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "    # AGG score\n",
    "    axes[0].plot(with_deltas['date'], with_deltas['agg_score'], linewidth=2)\n",
    "    axes[0].set_title('AGG Score (National Economic Sentiment)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # YoY change\n",
    "    valid_yoy = with_deltas.dropna(subset=['yoy_change'])\n",
    "    axes[1].bar(valid_yoy['date'], valid_yoy['yoy_change'], color='steelblue', alpha=0.7)\n",
    "    axes[1].set_title('YoY Change (AGG_t - AGG_t-4)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Change')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Momentum\n",
    "    valid_momentum = with_deltas.dropna(subset=['momentum'])\n",
    "    axes[2].bar(valid_momentum['date'], valid_momentum['momentum'], color='coral', alpha=0.7)\n",
    "    axes[2].set_title('Momentum (Acceleration)', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Momentum')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"âœ“ Feature visualization complete\")\n",
    "else:\n",
    "    print(\"âš  Cannot visualize features without delta features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = PredictionModel('config.yaml')\n",
    "print(dir(pred_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna().reset_index(drop=True)\n",
    "X_train['date'] = with_deltas.loc[X_train.index, 'date'].values\n",
    "\n",
    "gdp_df = macro_data['gdp'].copy()\n",
    "gdp_df['date'] = pd.to_datetime(gdp_df['date'])\n",
    "train_data = X_train.merge(gdp_df, on='date', how='inner')\n",
    "X_train = train_data[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].values\n",
    "y_train = train_data['value'].values\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Target data: {y_train.shape}\")\n",
    "gdp_models = pred_model.train_gdp_models(X_train, y_train)\n",
    "print(f\"Model RÂ²: {gdp_models['gdp'].score(X_train, y_train):.3f}\")\n",
    "gdp_model = pred_model.train_gdp_model(X_train.values, y_train.values)\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Target data: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GDP prediction model\n",
    "gdp_model = pred_model.train_gdp_model(X_train, y_train)\n",
    "print(f\"\\nGDP Model Trained\")\n",
    "print(f\"  Model type: {type(gdp_model).__name__}\")\n",
    "print(f\"  Training RÂ²: {gdp_model.score(X_train, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using real test data\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    # Use the most recent features for out-of-sample prediction\n",
    "    test_features = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna().tail(10)\n",
    "    test_dates = with_deltas.loc[test_features.index, 'date']\n",
    "    \n",
    "    predictions = gdp_model.predict(test_features.values)\n",
    "\n",
    "    print(f\"\\nGDP Predictions (1Q ahead) for recent quarters:\")\n",
    "    for date, pred in zip(test_dates, predictions):\n",
    "        print(f\"  {date.strftime('%Y-%m-%d')}: {pred:.3f}%\")\n",
    "    print(f\"\\n  Mean: {predictions.mean():.3f}%\")\n",
    "    print(f\"  Std: {predictions.std():.3f}%\")\n",
    "    print(f\"  Range: [{predictions.min():.3f}, {predictions.max():.3f}]%\")\n",
    "else:\n",
    "    print(\"âš  Cannot make predictions without real AGG scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Signal Generation & Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize signal generator\n",
    "signal_gen = SignalGenerator('config.yaml')\n",
    "\n",
    "# Use real predictions from trained models\n",
    "# This requires: \n",
    "# 1. Features from AGG scores\n",
    "# 2. Trained GDP/IP models\n",
    "# 3. SPF forecasts from data_acq.fetch_spf_forecasts()\n",
    "\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    # Use real model predictions\n",
    "    features_for_pred = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna()\n",
    "    dates_for_pred = with_deltas.loc[features_for_pred.index, 'date']\n",
    "    \n",
    "\n",
    "    # Get predictions from trained model    predictions_df = pd.DataFrame()\n",
    "\n",
    "    gdp_predictions = gdp_model.predict(features_for_pred.values)    print(\"âš  Cannot generate predictions without real AGG scores\")\n",
    "\n",
    "    else:\n",
    "\n",
    "    # Fetch real SPF forecasts    print(predictions_df.head())\n",
    "\n",
    "    try:    print(\"âœ“ Real Predictions vs SPF:\")\n",
    "\n",
    "        spf_data = data_acq.fetch_spf_forecasts(start_date, end_date)    \n",
    "\n",
    "        spf_data['date'] = pd.to_datetime(spf_data['date'])    predictions_df.rename(columns={'rgdp_1q': 'gdp_spf'}, inplace=True)\n",
    "\n",
    "    except Exception as e:    predictions_df = predictions_df.merge(spf_data[['date', 'rgdp_1q']], on='date', how='left')\n",
    "\n",
    "        print(f\"âš  Could not fetch SPF data: {e}\")    })\n",
    "\n",
    "        spf_data = pd.DataFrame({'date': dates_for_pred, 'rgdp_1q': [2.0]*len(dates_for_pred)})        'gdp_pred': gdp_predictions\n",
    "\n",
    "            'date': dates_for_pred.values,\n",
    "\n",
    "    # Combine predictions with SPF    predictions_df = pd.DataFrame({"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trading signals (only if we have real predictions)\n",
    "if len(predictions_df) > 0:\n",
    "    signals = signal_gen.generate_signals(predictions_df)\n",
    "    print(f\"\\nðŸ“Š Trading Signals Generated:\")\n",
    "    print(signals.head(10))\n",
    "    print(f\"\\nSignal distribution:\")\n",
    "    print(signals['signal'].value_counts())\n",
    "else:\n",
    "    print(\"âš  Cannot generate signals without predictions\")\n",
    "    signals = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize backtester\n",
    "backtester = Backtester('config.yaml')\n",
    "\n",
    "# Use real returns from strategy execution\n",
    "# This requires:\n",
    "# 1. Trading signals from signal_gen.generate_signals()\n",
    "# 2. Sector ETF price data\n",
    "# 3. Portfolio construction and rebalancing\n",
    "\n",
    "if len(predictions_df) > 0:\n",
    "    # Fetch real ETF price data for sectors\n",
    "    sector_etfs = config['strategy']['sector_etfs']\n",
    "    etf_start = config['backtest']['test_start']\n",
    "    etf_end = config['backtest']['test_end']\n",
    "    \n",
    "    etf_prices = data_acq.fetch_etf_prices(sector_etfs, etf_start, etf_end)\n",
    "    \n",
    "    if etf_prices:\n",
    "        print(f\"âœ“ Fetched price data for {len(etf_prices)} sector ETFs\")\n",
    "\n",
    "                    print(f\"  {metric}: {value}\")\n",
    "\n",
    "        # Run backtest with real data        else:\n",
    "\n",
    "        # Note: This requires implementing the full backtesting logic            print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "        # For now, we show the structure        if isinstance(value, float):\n",
    "\n",
    "        print(\"\\nâš  Full backtest execution requires:\")    for metric, value in metrics.items():\n",
    "\n",
    "        print(\"  1. Signals from signal_gen.generate_signals(predictions_df)\")    print(f\"\\nðŸ“ˆ Performance Metrics:\")\n",
    "\n",
    "        print(\"  2. Portfolio construction based on signals\")    metrics = backtester.calculate_metrics(portfolio_returns)\n",
    "\n",
    "        print(\"  3. Daily rebalancing and return calculation\")    # Calculate performance metrics\n",
    "\n",
    "        print(\"  4. Benchmark comparison (SPY or equal-weight)\")if len(portfolio_returns) > 0:\n",
    "\n",
    "        \n",
    "\n",
    "        portfolio_returns = pd.DataFrame()    portfolio_returns = pd.DataFrame()\n",
    "\n",
    "        print(\"\\nPlease implement backtester.run_backtest(signals, etf_prices) for real returns\")    print(\"âš  Cannot run backtest without predictions\")\n",
    "\n",
    "    else:else:\n",
    "\n",
    "        print(\"âš  No ETF price data available\")        portfolio_returns = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns and plot (only if we have real returns)\n",
    "if len(portfolio_returns) > 0 and 'strategy_return' in portfolio_returns.columns:\n",
    "    portfolio_returns['strategy_cumret'] = (1 + portfolio_returns['strategy_return']).cumprod() - 1\n",
    "    portfolio_returns['benchmark_cumret'] = (1 + portfolio_returns['benchmark_return']).cumprod() - 1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(portfolio_returns['date'], portfolio_returns['strategy_cumret'] * 100, \n",
    "            label='Strategy', linewidth=2)\n",
    "    ax.plot(portfolio_returns['date'], portfolio_returns['benchmark_cumret'] * 100, \n",
    "            label='Benchmark', linewidth=2, linestyle='--')\n",
    "\n",
    "    ax.set_title('Strategy vs Benchmark Cumulative Returns', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Return (%)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"âœ“ Backtest visualization complete\")    print(\"5. Execute backtest with real ETF prices\")\n",
    "\n",
    "else:    print(\"4. Generate trading signals\")\n",
    "\n",
    "    print(\"âš  No portfolio returns available for visualization\")    print(\"3. Train prediction models\")\n",
    "\n",
    "    print(\"\\nTo complete the full pipeline with real data:\")    print(\"2. Engineer features from AGG scores\")\n",
    "    print(\"1. Score earnings transcripts â†’ agg_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete Pipeline with Real Data\n",
    "\n",
    "This notebook demonstrates the **AI Economy Score Predictor** strategy pipeline using **real data sources**:\n",
    "\n",
    "### âœ… Real Data Used:\n",
    "1. **Macroeconomic Data**: From FRED API (GDP, Industrial Production, Employment, Wages)\n",
    "2. **Control Variables**: From FRED API (Yield Curve, Consumer Sentiment, Unemployment)\n",
    "3. **PMI Data**: Loaded from `pmi_data.csv` \n",
    "4. **S&P 500 Constituents**: From `constituents.csv`\n",
    "5. **ETF Prices**: Fetched via yfinance API\n",
    "\n",
    "### âš ï¸ Real Data Needed:\n",
    "- **Earnings Call Transcripts** with LLM sentiment scores aggregated quarterly â†’ `agg_scores.csv`\n",
    "\n",
    "### Pipeline Steps:\n",
    "1. **Data Acquisition** âœ“ Uses real FRED API and local files\n",
    "2. **LLM Scoring** â†’ Requires real earnings transcripts (Seeking Alpha, CapIQ, Bloomberg)\n",
    "3. **Feature Engineering** âœ“ Works with real AGG scores once available\n",
    "4. **Prediction Models** âœ“ Trains on real macro data + AGG features\n",
    "5. **Signal Generation** âœ“ Compares predictions to SPF forecasts\n",
    "6. **Backtesting** âœ“ Uses real sector ETF prices\n",
    "\n",
    "### Next Steps:\n",
    "1. Obtain earnings call transcripts from a data provider\n",
    "2. Score transcripts using `LLMScorer.score_multiple_transcripts()`\n",
    "3. Aggregate scores by quarter and save to `agg_scores.csv`\n",
    "4. Re-run this notebook to execute the full pipeline with real signals\n",
    "\n",
    "**No synthetic/random data is used for actual trading signals - all results require real transcript scoring.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data availability\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“ Data File Status:\\n\")\n",
    "\n",
    "required_files = {\n",
    "    'config.yaml': 'Configuration file',\n",
    "    'constituents.csv': 'S&P 500 constituents',\n",
    "    'pmi_data.csv': 'PMI data'\n",
    "}\n",
    "\n",
    "optional_files = {\n",
    "    'agg_scores.csv': 'Aggregated LLM sentiment scores (REQUIRED for full pipeline)'\n",
    "}\n",
    "\n",
    "for file, desc in required_files.items():\n",
    "    status = \"âœ“\" if os.path.exists(file) else \"âœ—\"\n",
    "    print(f\"{status} {file}: {desc}\")\n",
    "\n",
    "print(\"\\nOptional (but critical):\")\n",
    "for file, desc in optional_files.items():\n",
    "    status = \"âœ“\" if os.path.exists(file) else \"âœ— MISSING\"\n",
    "    print(f\"{status} {file}: {desc}\")\n",
    "\n",
    "if not os.path.exists('agg_scores.csv'):\n",
    "    print(\"\\nâš ï¸  To create agg_scores.csv, you need to:\")\n",
    "    print(\"   1. Get earnings transcripts from a data provider\")\n",
    "    print(\"   2. Run LLM scoring (see 'Note: To Use Real Data' section above)\")\n",
    "    print(\"   3. Use the aggregate_scores_by_quarter() function\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
