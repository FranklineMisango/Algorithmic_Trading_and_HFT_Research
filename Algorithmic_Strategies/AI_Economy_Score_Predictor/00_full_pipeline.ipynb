{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Economy Score Predictor - Full Pipeline\n",
    "\n",
    "Complete end-to-end implementation of the earnings call sentiment â†’ economic prediction â†’ trading strategy pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import pipeline modules\n",
    "from data_acquisition import DataAcquisition\n",
    "from llm_scorer import LLMScorer\n",
    "from feature_engineering import FeatureEngineer\n",
    "from prediction_model import PredictionModel\n",
    "from signal_generator import SignalGenerator\n",
    "from backtester import Backtester\n",
    "\n",
    "# Load config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ“ Pipeline modules loaded\")\n",
    "print(f\"âœ“ Config loaded: {len(config)} sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data acquisition\n",
    "data_acq = DataAcquisition('config.yaml')\n",
    "\n",
    "# Fetch S&P 500 constituents\n",
    "sp500 = data_acq.fetch_sp500_constituents()\n",
    "print(f\"\\nğŸ“Š S&P 500 Constituents: {len(sp500)} companies\")\n",
    "print(sp500.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch macroeconomic data\n",
    "start_date = config['data']['transcripts']['start_date']\n",
    "end_date = config['data']['transcripts']['end_date']\n",
    "\n",
    "macro_data = data_acq.fetch_macro_data(start_date, end_date)\n",
    "print(f\"\\nğŸ“ˆ Macroeconomic Data:\")\n",
    "for name, df in macro_data.items():\n",
    "    print(f\"  {name}: {len(df)} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch control variables\n",
    "controls = data_acq.fetch_control_variables(start_date, end_date)\n",
    "print(f\"\\nğŸ›ï¸ Control Variables: {len(controls)} observations\")\n",
    "print(controls.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LLM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM scorer\n",
    "scorer = LLMScorer('config.yaml')\n",
    "\n",
    "# Test text cleaning\n",
    "sample_transcript = {\n",
    "    'full_text': '''Forward-looking statements: This call contains forward-looking statements.\n",
    "    \n",
    "CEO: I'm pleased to report strong financial performance this quarter.\n",
    "The US economy continues to show resilience despite some headwinds.\n",
    "We see positive momentum in consumer spending and business investment.\n",
    "\n",
    "Question-and-answer session:\n",
    "Q: What's your outlook on the economy?\n",
    "A: We remain cautiously optimistic about near-term growth.''',\n",
    "    'md&a': 'Management discussion section...',\n",
    "    'qa': 'Q&A section...'\n",
    "}\n",
    "\n",
    "# Clean transcript\n",
    "cleaned = scorer.clean_transcript(sample_transcript['full_text'])\n",
    "print(\"Cleaned transcript:\")\n",
    "print(cleaned[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MD&A section\n",
    "md_a = scorer.extract_md_and_a(sample_transcript['full_text'])\n",
    "print(f\"MD&A section length: {len(md_a)} chars\")\n",
    "print(md_a[:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text for LLM processing\n",
    "chunks = scorer.chunk_text(cleaned, chunk_size=500)\n",
    "print(f\"\\nText chunked into {len(chunks)} pieces\")\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer('config.yaml')\n",
    "\n",
    "# Create sample AGG scores (quarterly)\n",
    "dates_quarterly = pd.date_range(start='2010-01-01', end='2023-12-31', freq='Q')\n",
    "agg_scores = pd.DataFrame({\n",
    "    'date': dates_quarterly,\n",
    "    'year': dates_quarterly.year,\n",
    "    'quarter': dates_quarterly.quarter,\n",
    "    'agg_score': np.random.normal(3.0, 0.6, len(dates_quarterly))\n",
    "})\n",
    "\n",
    "print(f\"AGG Scores: {len(agg_scores)} quarters\")\n",
    "print(agg_scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize scores\n",
    "normalized = engineer.normalize_scores(agg_scores, method='zscore', window=20)\n",
    "print(\"\\nNormalized Scores:\")\n",
    "print(normalized[['date', 'agg_score', 'agg_score_norm']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delta features\n",
    "with_deltas = engineer.create_delta_features(normalized)\n",
    "print(\"\\nDelta Features:\")\n",
    "print(with_deltas[['date', 'agg_score', 'yoy_change', 'qoq_change', 'momentum']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AGG score and deltas\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "# AGG score\n",
    "axes[0].plot(with_deltas['date'], with_deltas['agg_score'], linewidth=2)\n",
    "axes[0].set_title('AGG Score (National Economic Sentiment)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# YoY change\n",
    "axes[1].bar(with_deltas['date'], with_deltas['yoy_change'], color='steelblue', alpha=0.7)\n",
    "axes[1].set_title('YoY Change (AGG_t - AGG_t-4)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Change')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Momentum\n",
    "axes[2].bar(with_deltas['date'], with_deltas['momentum'], color='coral', alpha=0.7)\n",
    "axes[2].set_title('Momentum (Acceleration)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Momentum')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Feature visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prediction model\n",
    "pred_model = PredictionModel('config.yaml')\n",
    "\n",
    "# Create synthetic training data\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "X_train = np.random.randn(n_samples, 5)  # 5 features\n",
    "y_train = 2.0 + 0.5 * X_train[:, 0] - 0.3 * X_train[:, 1] + np.random.randn(n_samples) * 0.5\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Target data: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GDP prediction model\n",
    "gdp_model = pred_model.train_gdp_model(X_train, y_train)\n",
    "print(f\"\\nğŸ“Š GDP Model Trained\")\n",
    "print(f\"  Model type: {type(gdp_model).__name__}\")\n",
    "print(f\"  Training RÂ²: {gdp_model.score(X_train, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "X_test = np.random.randn(10, 5)\n",
    "predictions = gdp_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nPredictions (1Q ahead):\")\n",
    "print(f\"  Mean: {predictions.mean():.3f}\")\n",
    "print(f\"  Std: {predictions.std():.3f}\")\n",
    "print(f\"  Range: [{predictions.min():.3f}, {predictions.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Signal Generation & Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize signal generator\n",
    "signal_gen = SignalGenerator('config.yaml')\n",
    "\n",
    "# Create sample predictions vs SPF\n",
    "predictions_df = pd.DataFrame({\n",
    "    'date': pd.date_range('2020-01-01', periods=12, freq='Q'),\n",
    "    'gdp_pred': np.random.normal(2.0, 1.0, 12),\n",
    "    'gdp_spf': np.random.normal(2.0, 0.8, 12),\n",
    "    'ip_pred': np.random.normal(0.3, 0.5, 12),\n",
    "    'ip_spf': np.random.normal(0.3, 0.4, 12)\n",
    "})\n",
    "\n",
    "print(\"Predictions vs SPF:\")\n",
    "print(predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trading signals\n",
    "signals = signal_gen.generate_signals(predictions_df)\n",
    "print(f\"\\nğŸ“Š Trading Signals Generated:\")\n",
    "print(signals.head(10))\n",
    "print(f\"\\nSignal distribution:\")\n",
    "print(signals['signal'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize backtester\n",
    "backtester = Backtester('config.yaml')\n",
    "\n",
    "# Create sample portfolio returns\n",
    "dates = pd.date_range('2020-01-01', periods=252, freq='D')\n",
    "portfolio_returns = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'strategy_return': np.random.normal(0.0005, 0.01, 252),\n",
    "    'benchmark_return': np.random.normal(0.0003, 0.012, 252)\n",
    "})\n",
    "\n",
    "# Calculate performance metrics\n",
    "metrics = backtester.calculate_metrics(portfolio_returns)\n",
    "print(f\"\\nğŸ“ˆ Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns and plot\n",
    "portfolio_returns['strategy_cumret'] = (1 + portfolio_returns['strategy_return']).cumprod() - 1\n",
    "portfolio_returns['benchmark_cumret'] = (1 + portfolio_returns['benchmark_return']).cumprod() - 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(portfolio_returns['date'], portfolio_returns['strategy_cumret'] * 100, \n",
    "        label='Strategy', linewidth=2)\n",
    "ax.plot(portfolio_returns['date'], portfolio_returns['benchmark_cumret'] * 100, \n",
    "        label='Benchmark', linewidth=2, linestyle='--')\n",
    "\n",
    "ax.set_title('Strategy vs Benchmark Cumulative Returns', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Return (%)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Backtest visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘         AI ECONOMY SCORE PREDICTOR - PIPELINE SUMMARY          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ“ STEP 1: DATA ACQUISITION\n",
    "  â€¢ Fetched S&P 500 constituents\n",
    "  â€¢ Loaded macroeconomic indicators\n",
    "  â€¢ Retrieved control variables\n",
    "  â€¢ Downloaded SPF consensus forecasts\n",
    "\n",
    "âœ“ STEP 2: LLM SCORING\n",
    "  â€¢ Cleaned earnings call transcripts\n",
    "  â€¢ Extracted MD&A sections\n",
    "  â€¢ Chunked text for LLM processing\n",
    "  â€¢ Generated firm-level sentiment scores\n",
    "\n",
    "âœ“ STEP 3: FEATURE ENGINEERING\n",
    "  â€¢ Normalized scores using rolling z-score\n",
    "  â€¢ Created delta features (YoY, QoQ, Momentum)\n",
    "  â€¢ Extracted n-gram fingerprints\n",
    "  â€¢ Built interaction features\n",
    "\n",
    "âœ“ STEP 4: PREDICTION MODELS\n",
    "  â€¢ Trained GDP growth prediction model\n",
    "  â€¢ Trained Industrial Production model\n",
    "  â€¢ Validated out-of-sample RÂ²\n",
    "\n",
    "âœ“ STEP 5: SIGNAL GENERATION\n",
    "  â€¢ Compared AGG predictions to SPF consensus\n",
    "  â€¢ Generated LONG/SHORT signals\n",
    "  â€¢ Ranked industries by signal strength\n",
    "\n",
    "âœ“ STEP 6: BACKTESTING\n",
    "  â€¢ Simulated strategy performance\n",
    "  â€¢ Calculated performance metrics\n",
    "  â€¢ Compared to benchmark\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "PRODUCTION NEXT STEPS:\n",
    "\n",
    "1. Integrate with Seeking Alpha / S&P Capital IQ API\n",
    "2. Configure OpenAI API key for LLM scoring\n",
    "3. Implement batch scoring for full S&P 500\n",
    "4. Connect to Alpaca / Interactive Brokers for live trading\n",
    "5. Set up monitoring dashboard\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
