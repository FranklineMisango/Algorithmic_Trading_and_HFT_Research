{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Economy Score Predictor - Full Pipeline\n",
    "\n",
    "Complete end-to-end implementation of the earnings call sentiment ‚Üí economic prediction ‚Üí trading strategy pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from data_acquisition import DataAcquisition\n",
    "from llm_scorer import LLMScorer\n",
    "from feature_engineering import FeatureEngineer\n",
    "from prediction_model import PredictionModel\n",
    "from signal_generator import SignalGenerator\n",
    "from backtester import Backtester\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚úì Pipeline modules loaded\")\n",
    "print(f\"‚úì Config loaded: {len(config)} sections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data acquisition\n",
    "data_acq = DataAcquisition('config.yaml')\n",
    "sp500 = data_acq.fetch_sp500_constituents()\n",
    "sp500.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fetch Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_acquisition import DataAcquisition\n",
    "data = DataAcquisition(\"config.yaml\")\n",
    "transcripts = data.fetch_earnings_transcripts('2015-01-01', '2026-01-01')\n",
    "print(f\"Loaded {len(transcripts)} transcripts for Q1 2015\")\n",
    "macro = data.fetch_macro_data('2015-01-01', '2025-12-31')\n",
    "print(f\"Loaded {len(macro)} macro indicators\")\n",
    "sp500 = data.fetch_sp500_constituents()\n",
    "print(f\"Loaded {len(sp500)} S&P 500 stocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fetch Macro Data (FRED API)\n",
    "\n",
    "**Note**: If you get FRED API errors, restart the kernel to reload the config with the updated API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch macroeconomic data\n",
    "start_date = config['data']['transcripts']['start_date']\n",
    "end_date = config['data']['transcripts']['end_date']\n",
    "macro_data = data_acq.fetch_macro_data(start_date, end_date)\n",
    "print(f\"\\n Macroeconomic Data:\")\n",
    "for name, df in macro_data.items():\n",
    "    print(f\"  {name}: {len(df)} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "pmi_path = 'pmi_data.csv'\n",
    "pmi_df = pd.read_csv(pmi_path)\n",
    "pmi_df.columns = [c.strip().lower().replace(' ', '_') for c in pmi_df.columns]\n",
    "print(\"Columns in PMI file:\", pmi_df.columns.tolist())\n",
    "date_col = [col for col in pmi_df.columns if 'date' in col][0]\n",
    "pmi_col = [col for col in pmi_df.columns if 'pmi' in col][0]\n",
    "def clean_date(val):\n",
    "    # Extract the part before the first parenthesis\n",
    "    val = str(val).split('(')[0].strip()\n",
    "    try:\n",
    "        return pd.to_datetime(val)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "pmi_df[date_col] = pmi_df[date_col].apply(clean_date)\n",
    "pmi_df = pmi_df.dropna(subset=[date_col, pmi_col])\n",
    "print(f\"Loaded PMI data: {len(pmi_df)} rows\")\n",
    "print(pmi_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch control variables\n",
    "controls = data_acq.fetch_control_variables(start_date, end_date)\n",
    "print(f\"\\nControl Variables: {len(controls)} observations\")\n",
    "controls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_acq.pmi_df = pmi_df\n",
    "controls = data_acq.fetch_control_variables(start_date, end_date, pmi_df=pmi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts['date'] = pd.to_datetime(transcripts['date'])\n",
    "transcripts_2024_2025 = transcripts[\n",
    "    (transcripts['date'] >= '2024-01-01') & \n",
    "    (transcripts['date'] <= '2025-12-31')\n",
    "].copy()\n",
    "\n",
    "print(f\"Filtered to {len(transcripts_2024_2025)} transcripts (2024-2025)\")\n",
    "print(f\"Date range: {transcripts_2024_2025['date'].min()} to {transcripts_2024_2025['date'].max()}\")\n",
    "print(f\"\\nBreakdown by year:\")\n",
    "print(transcripts_2024_2025['year'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_2024_2025.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in macro_data:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_data['gdp'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count NAN \n",
    "macro_data['wages'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: LLM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM scorer\n",
    "scorer = LLMScorer('config.yaml')\n",
    "\n",
    "# Test text cleaning\n",
    "sample_transcript = {\n",
    "    'full_text': '''Forward-looking statements: This call contains forward-looking statements.\n",
    "    \n",
    "CEO: I'm pleased to report strong financial performance this quarter.\n",
    "The US economy continues to show resilience despite some headwinds.\n",
    "We see positive momentum in consumer spending and business investment.\n",
    "\n",
    "Question-and-answer session:\n",
    "Q: What's your outlook on the economy?\n",
    "A: We remain cautiously optimistic about near-term growth.''',\n",
    "    'md&a': 'Management discussion section...',\n",
    "    'qa': 'Q&A section...'\n",
    "}\n",
    "\n",
    "# Clean transcript\n",
    "cleaned = scorer.clean_transcript(sample_transcript['full_text'])\n",
    "print(\"Cleaned transcript:\")\n",
    "print(cleaned[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MD&A section\n",
    "md_a = scorer.extract_md_and_a(sample_transcript['full_text'])\n",
    "print(f\"MD&A section length: {len(md_a)} chars\")\n",
    "print(md_a[:150] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text for LLM processing\n",
    "chunks = scorer.chunk_text(cleaned, chunk_size=500)\n",
    "print(f\"\\nText chunked into {len(chunks)} pieces\")\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_scores_by_quarter(scored_transcripts):\n",
    "    \"\"\"\n",
    "    Aggregate individual transcript scores into quarterly AGG scores.\n",
    "    \n",
    "    Args:\n",
    "        scored_transcripts: List of dicts with 'symbol', 'date', 'score', 'market_cap'\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quarterly AGG scores\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(scored_transcripts)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    df['quarter_date'] = df['date'].dt.to_period('Q').dt.to_timestamp()\n",
    "    \n",
    "    # Aggregate by quarter using value-weighted average\n",
    "    quarterly = df.groupby('quarter_date').apply(\n",
    "        lambda x: np.average(x['score'], weights=x.get('market_cap', [1]*len(x)))\n",
    "    ).reset_index()\n",
    "    \n",
    "    quarterly.columns = ['date', 'agg_score']\n",
    "    quarterly['year'] = quarterly['date'].dt.year\n",
    "    quarterly['quarter'] = quarterly['date'].dt.quarter\n",
    "    \n",
    "    return quarterly[['date', 'year', 'quarter', 'agg_score']]\n",
    "\n",
    "# Example usage (commented out - requires real transcript scores):\n",
    "# scored_transcripts = scorer.score_multiple_transcripts(transcripts)\n",
    "# agg_scores = aggregate_scores_by_quarter(scored_transcripts)\n",
    "# agg_scores.to_csv('agg_scores.csv', index=False)\n",
    "print(\"‚úì AGG score aggregation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Transcript Scoring - Choose Your Option\n",
    "\n",
    "**OPTION A: Test Pipeline (2024-2025 only) - RECOMMENDED FIRST**\n",
    "- Score approximately 2,500 transcripts (2 years)\n",
    "- Cost: approximately $2.50-5.00 (GPT-4o-mini)\n",
    "- Time: approximately 20-40 minutes\n",
    "- Purpose: Test full pipeline before committing to full dataset\n",
    "\n",
    "**OPTION B: Full Dataset (2015-2025)**\n",
    "- Score approximately 13,600 transcripts (10 years)\n",
    "- Cost: approximately $13-27 (GPT-4o-mini)\n",
    "- Time: approximately 2-5 hours\n",
    "- Purpose: Complete research dataset for publication-quality results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose scoring mode\n",
    "TEST_MODE = True  # Set to False to run full dataset (2015-2025)\n",
    "\n",
    "if TEST_MODE:\n",
    "    # OPTION A: Test with 2024-2025 data\n",
    "    print(\"TEST MODE: Checking for existing transcript data...\")\n",
    "    \n",
    "    # Check if we already have filtered 2024-2025 data\n",
    "    if 'transcripts_2024_2025' in dir() and len(transcripts_2024_2025) > 0:\n",
    "        test_transcripts = transcripts_2024_2025.copy()\n",
    "        print(f\"Using pre-filtered transcripts_2024_2025 data: {len(test_transcripts)} transcripts\")\n",
    "    elif 'transcripts' in dir() and len(transcripts) > 0:\n",
    "        # Filter existing transcripts to 2024-2025\n",
    "        print(\"Filtering full transcript data to 2024-2025...\")\n",
    "        transcripts_copy = transcripts.copy()\n",
    "        transcripts_copy['date'] = pd.to_datetime(transcripts_copy['date'])\n",
    "        test_transcripts = transcripts_copy[\n",
    "            (transcripts_copy['date'] >= '2024-01-01') & \n",
    "            (transcripts_copy['date'] <= '2025-12-31')\n",
    "        ].copy()\n",
    "        print(f\"Filtered {len(transcripts)} ‚Üí {len(test_transcripts)} transcripts\")\n",
    "    else:\n",
    "        print(\"No transcripts loaded yet, fetching 2024-2025...\")\n",
    "        test_transcripts = data_acq.fetch_earnings_transcripts('2024-01-01', '2025-12-31')\n",
    "    \n",
    "    print(f\"\\nTotal transcripts to score: {len(test_transcripts)}\")\n",
    "    print(f\"  Estimated cost: ${len(test_transcripts) * 0.001:.2f} - ${len(test_transcripts) * 0.002:.2f}\")\n",
    "    print(f\"  Estimated time: {len(test_transcripts) * 2 / 60:.1f} - {len(test_transcripts) * 3 / 60:.1f} minutes\")\n",
    "    print(f\"\\nData will be saved to: test_scored_transcripts_2024_2025.csv\")\n",
    "    \n",
    "    # Show breakdown by year\n",
    "    test_transcripts['year'] = pd.to_datetime(test_transcripts['date']).dt.year\n",
    "    year_counts = test_transcripts['year'].value_counts().sort_index()\n",
    "    print(f\"\\nTranscripts by year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} transcripts\")\n",
    "    \n",
    "    scoring_transcripts = test_transcripts\n",
    "    save_path = 'test_scored_transcripts_2024_2025.csv'\n",
    "    \n",
    "else:\n",
    "    # OPTION B: Full dataset (2015-2025)\n",
    "    print(\"FULL MODE: Checking for existing transcript data...\")\n",
    "    \n",
    "    # Check if we already have full dataset loaded\n",
    "    if 'transcripts' in dir() and len(transcripts) > 0:\n",
    "        transcripts_copy = transcripts.copy()\n",
    "        transcripts_copy['date'] = pd.to_datetime(transcripts_copy['date'])\n",
    "        date_range = (transcripts_copy['date'].min(), transcripts_copy['date'].max())\n",
    "        \n",
    "        # Check if we have enough coverage\n",
    "        if date_range[0] <= pd.Timestamp('2015-01-01') and date_range[1] >= pd.Timestamp('2025-01-01'):\n",
    "            print(f\"Reusing {len(transcripts_copy)} transcripts from already-loaded data\")\n",
    "            print(f\"  Date range: {date_range[0].date()} to {date_range[1].date()}\")\n",
    "            all_transcripts = transcripts_copy[\n",
    "                (transcripts_copy['date'] >= '2015-01-01') & \n",
    "                (transcripts_copy['date'] <= '2025-12-31')\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Loaded data has limited range ({date_range[0].date()} to {date_range[1].date()})\")\n",
    "            print(\"Fetching complete 2015-2025 dataset...\")\n",
    "            all_transcripts = data_acq.fetch_earnings_transcripts('2015-01-01', '2025-12-31')\n",
    "    else:\n",
    "        print(\"No transcripts loaded yet, fetching 2015-2025...\")\n",
    "        all_transcripts = data_acq.fetch_earnings_transcripts('2015-01-01', '2025-12-31')\n",
    "    \n",
    "    print(f\"\\nTotal transcripts to score: {len(all_transcripts)}\")\n",
    "    print(f\"  Estimated cost: ${len(all_transcripts) * 0.001:.2f} - ${len(all_transcripts) * 0.002:.2f}\")\n",
    "    print(f\"  Estimated time: {len(all_transcripts) * 2 / 3600:.1f} - {len(all_transcripts) * 3 / 3600:.1f} hours\")\n",
    "    print(f\"\\nData will be saved to: all_scored_transcripts_2015_2025.csv\")\n",
    "    \n",
    "    # Show breakdown by year\n",
    "    all_transcripts['year'] = pd.to_datetime(all_transcripts['date']).dt.year\n",
    "    year_counts = all_transcripts['year'].value_counts().sort_index()\n",
    "    print(f\"\\nTranscripts by year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} transcripts\")\n",
    "    \n",
    "    scoring_transcripts = all_transcripts\n",
    "    save_path = 'all_scored_transcripts_2015_2025.csv'\n",
    "\n",
    "print(f\"\\nReady to score {len(scoring_transcripts)} transcripts\")\n",
    "print(f\"Checkpoints will be saved every 50 transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scoring function with progress tracking\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def score_quarter_transcripts(transcripts_df, scorer, save_path='scored_transcripts.csv'):\n",
    "    \"\"\"\n",
    "    Score all transcripts with progress tracking, checkpointing, and error handling.\n",
    "    \"\"\"\n",
    "    # First, inspect the data structure\n",
    "    print(\"Inspecting data structure...\")\n",
    "    print(f\"Type: {type(transcripts_df)}\")\n",
    "    print(f\"Columns: {transcripts_df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst row type: {type(transcripts_df.iloc[0])}\")\n",
    "    print(f\"First row preview:\")\n",
    "    print(transcripts_df.iloc[0])\n",
    "    \n",
    "    print(f\"\\nScoring {len(transcripts_df)} transcripts...\")\n",
    "    print(f\"Estimated cost: ${len(transcripts_df) * 0.001:.2f} (GPT-4o-mini)\")\n",
    "    print(f\"Estimated time: {len(transcripts_df) * 2 / 60:.1f} minutes\")\n",
    "    \n",
    "    # Check for existing progress\n",
    "    try:\n",
    "        existing = pd.read_csv(save_path)\n",
    "        already_scored = set(existing['symbol'] + '_' + existing['date'].astype(str))\n",
    "        print(f\"Found {len(already_scored)} previously scored transcripts\")\n",
    "    except FileNotFoundError:\n",
    "        already_scored = set()\n",
    "        existing = pd.DataFrame()\n",
    "    \n",
    "    scored_results = []\n",
    "    errors = []\n",
    "    \n",
    "    # Determine transcript column name - check what's actually in the DataFrame\n",
    "    available_cols = transcripts_df.columns.tolist()\n",
    "    transcript_col = None\n",
    "    \n",
    "    for possible_name in ['transcript', 'text', 'content', 'full_text', 'body']:\n",
    "        if possible_name in available_cols:\n",
    "            transcript_col = possible_name\n",
    "            break\n",
    "    \n",
    "    if transcript_col is None:\n",
    "        print(f\"ERROR: Could not find transcript column. Available columns: {available_cols}\")\n",
    "        return existing if len(existing) > 0 else pd.DataFrame()\n",
    "    \n",
    "    print(f\"Using transcript column: '{transcript_col}'\")\n",
    "    \n",
    "    # Convert to dict records for easier iteration\n",
    "    records = transcripts_df.to_dict('records')\n",
    "    \n",
    "    for idx, row in enumerate(tqdm(records, desc=\"Scoring\")):\n",
    "        # Handle different possible column names\n",
    "        symbol = row.get('symbol') or row.get('ticker') or 'UNKNOWN'\n",
    "        date = row.get('date') or row.get('filing_date') or 'UNKNOWN'\n",
    "        transcript_id = f\"{symbol}_{date}\"\n",
    "        \n",
    "        # Skip if already scored\n",
    "        if transcript_id in already_scored:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get the transcript text\n",
    "            transcript_text = row.get(transcript_col, '')\n",
    "            \n",
    "            if not transcript_text or transcript_text == '':\n",
    "                errors.append({'symbol': symbol, 'date': date, 'error': 'Empty transcript'})\n",
    "                continue\n",
    "            \n",
    "            # Score transcript - wrap in expected dictionary format\n",
    "            # The scorer expects a dict with 'full_text' key\n",
    "            transcript_dict = {'full_text': transcript_text}\n",
    "            result = scorer.score_transcript(transcript_dict, use_md_a_only=False)\n",
    "            score = result['firm_score']\n",
    "            \n",
    "            if score is None:\n",
    "                errors.append({'symbol': symbol, 'date': date, 'error': 'Scoring returned None'})\n",
    "                continue\n",
    "            \n",
    "            scored_results.append({\n",
    "                'symbol': symbol,\n",
    "                'date': date,\n",
    "                'score': score,\n",
    "                'transcript_length': len(str(transcript_text))\n",
    "            })\n",
    "            \n",
    "            # Save checkpoint every 50 transcripts\n",
    "            if len(scored_results) % 50 == 0:\n",
    "                temp_df = pd.DataFrame(scored_results)\n",
    "                combined = pd.concat([existing, temp_df], ignore_index=True)\n",
    "                combined.to_csv(save_path, index=False)\n",
    "                print(f\"\\nCheckpoint: Saved {len(combined)} scores\")\n",
    "            \n",
    "            # Rate limiting (to avoid API limits)\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append({'symbol': symbol, 'date': date, 'error': str(e)})\n",
    "            if idx < 5:  # Only print first few errors in detail\n",
    "                print(f\"\\nError scoring {symbol}: {e}\")\n",
    "    \n",
    "    # Final save - handle case where nothing was scored\n",
    "    if scored_results:\n",
    "        final_df = pd.DataFrame(scored_results)\n",
    "        combined = pd.concat([existing, final_df], ignore_index=True)\n",
    "        combined.to_csv(save_path, index=False)\n",
    "        print(f\"\\nSaved {len(combined)} total scored transcripts to {save_path}\")\n",
    "    elif len(existing) > 0:\n",
    "        combined = existing\n",
    "        print(f\"\\nNo new transcripts scored. Returning {len(existing)} existing scores.\")\n",
    "    else:\n",
    "        combined = pd.DataFrame(columns=['symbol', 'date', 'score', 'transcript_length'])\n",
    "        print(\"\\nWARNING: No transcripts were scored successfully!\")\n",
    "    \n",
    "    if errors:\n",
    "        error_df = pd.DataFrame(errors)\n",
    "        error_df.to_csv('scoring_errors.csv', index=False)\n",
    "        print(f\"\\nWARNING: {len(errors)} errors occurred (saved to scoring_errors.csv)\")\n",
    "        print(f\"First few unique errors:\")\n",
    "        unique_errors = error_df['error'].value_counts().head(3)\n",
    "        for error_msg, count in unique_errors.items():\n",
    "            print(f\"  {error_msg}: {count} occurrences\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"Scoring function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data structure before scoring (Optional)\n",
    "print(\"Data structure inspection:\")\n",
    "print(f\"Type of scoring_transcripts: {type(scoring_transcripts)}\")\n",
    "print(f\"Shape: {scoring_transcripts.shape}\")\n",
    "print(f\"Columns: {scoring_transcripts.columns.tolist()}\")\n",
    "print(f\"\\nFirst transcript preview:\")\n",
    "print(scoring_transcripts.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting scoring at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "scored_data = score_quarter_transcripts(\n",
    "    scoring_transcripts, \n",
    "    scorer, \n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Total scored: {len(scored_data)}\")\n",
    "print(f\"  Date range: {scored_data['date'].min()} to {scored_data['date'].max()}\")\n",
    "print(f\"  Average score: {scored_data['score'].mean():.2f}\")\n",
    "print(f\"  Score distribution:\")\n",
    "print(scored_data['score'].value_counts().sort_index())\n",
    "print(f\"\\nSaved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate scored transcripts into quarterly AGG scores\n",
    "print(\"Aggregating individual scores into quarterly AGG scores...\")\n",
    "\n",
    "# Convert to DataFrame if needed\n",
    "if isinstance(scored_data, pd.DataFrame):\n",
    "    scored_df = scored_data.copy()\n",
    "else:\n",
    "    scored_df = pd.DataFrame(scored_data)\n",
    "\n",
    "# Ensure date column is datetime\n",
    "scored_df['date'] = pd.to_datetime(scored_df['date'])\n",
    "scored_df['year'] = scored_df['date'].dt.year\n",
    "scored_df['quarter'] = scored_df['date'].dt.quarter\n",
    "\n",
    "# Group by quarter and calculate aggregate score\n",
    "agg_scores = scored_df.groupby(['year', 'quarter']).agg({\n",
    "    'score': ['mean', 'std', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "agg_scores.columns = ['year', 'quarter', 'agg_score', 'score_std', 'num_firms']\n",
    "\n",
    "# Create quarter date\n",
    "agg_scores['date'] = pd.to_datetime(\n",
    "    agg_scores['year'].astype(str) + '-Q' + agg_scores['quarter'].astype(str)\n",
    ")\n",
    "\n",
    "# Reorder columns\n",
    "final_agg_scores = agg_scores[['date', 'year', 'quarter', 'agg_score', 'score_std', 'num_firms']]\n",
    "\n",
    "# Save AGG scores\n",
    "agg_filename = 'test_agg_scores_2024_2025.csv' if TEST_MODE else 'agg_scores_2015_2025.csv'\n",
    "final_agg_scores.to_csv(agg_filename, index=False)\n",
    "print(f\"\\nSUCCESS: Saved {len(final_agg_scores)} quarterly AGG scores to {agg_filename}\")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nAGG Scores Summary:\")\n",
    "print(final_agg_scores)\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Quarters covered: {len(final_agg_scores)}\")\n",
    "print(f\"  Date range: {final_agg_scores['date'].min().strftime('%Y-%m-%d')} to {final_agg_scores['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Mean AGG score: {final_agg_scores['agg_score'].mean():.3f}\")\n",
    "print(f\"  Std AGG score: {final_agg_scores['agg_score'].std():.3f}\")\n",
    "print(f\"  Average firms/quarter: {final_agg_scores['num_firms'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer('config.yaml')\n",
    "\n",
    "# Load real AGG scores from saved file or create from actual transcript scoring\n",
    "try:\n",
    "    agg_scores = pd.read_csv('agg_scores.csv')\n",
    "    agg_scores['date'] = pd.to_datetime(agg_scores['date'])\n",
    "    print(f\"‚úì Loaded real AGG scores from file: {len(agg_scores)} quarters\")\n",
    "    print(agg_scores.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö† No saved AGG scores found. You need to:\")\n",
    "    print(\"  1. Score earnings transcripts using LLMScorer.score_multiple_transcripts()\")\n",
    "    print(\"  2. Aggregate scores by quarter using aggregate_scores_by_quarter()\")\n",
    "    print(\"  3. Save to 'agg_scores.csv'\")\n",
    "    print(\"\\n For demonstration, showing expected data structure...\")\n",
    "    # Show expected structure instead of generating synthetic data\n",
    "    agg_scores = pd.DataFrame({\n",
    "        'date': pd.date_range(start='2015-01-01', end='2023-12-31', freq='Q'),\n",
    "        'year': [],\n",
    "        'quarter': [],\n",
    "        'agg_score': []  # Real scores would be 1-5 from LLM\n",
    "    })\n",
    "    print(\"\\nExpected columns: date, year, quarter, agg_score\")\n",
    "    print(\"Cannot proceed with feature engineering without real data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize scores (only if we have real data)\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    normalized = engineer.normalize_scores(agg_scores, method='zscore', window=20)\n",
    "    print(\"\\nNormalized Scores:\")\n",
    "\n",
    "    print(normalized[['date', 'agg_score', 'agg_score_norm']].head(10))    normalized = pd.DataFrame()\n",
    "\n",
    "else:    print(\"‚ö† Cannot normalize without real AGG scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delta features (only if we have normalized data)\n",
    "if len(normalized) > 0:\n",
    "    with_deltas = engineer.create_delta_features(normalized)\n",
    "    print(\"\\nDelta Features:\")\n",
    "\n",
    "    print(with_deltas[['date', 'agg_score', 'yoy_change', 'qoq_change', 'momentum']].tail(10))    with_deltas = pd.DataFrame()\n",
    "\n",
    "else:    print(\"‚ö† Cannot create delta features without normalized scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize AGG score and deltas (only if we have features)\n",
    "if len(with_deltas) > 0:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "    # AGG score\n",
    "    axes[0].plot(with_deltas['date'], with_deltas['agg_score'], linewidth=2)\n",
    "    axes[0].set_title('AGG Score (National Economic Sentiment)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # YoY change\n",
    "    valid_yoy = with_deltas.dropna(subset=['yoy_change'])\n",
    "    axes[1].bar(valid_yoy['date'], valid_yoy['yoy_change'], color='steelblue', alpha=0.7)\n",
    "    axes[1].set_title('YoY Change (AGG_t - AGG_t-4)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Change')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Momentum\n",
    "    valid_momentum = with_deltas.dropna(subset=['momentum'])\n",
    "    axes[2].bar(valid_momentum['date'], valid_momentum['momentum'], color='coral', alpha=0.7)\n",
    "    axes[2].set_title('Momentum (Acceleration)', fontsize=12, fontweight='bold')\n",
    "    axes[2].set_ylabel('Momentum')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úì Feature visualization complete\")\n",
    "else:\n",
    "    print(\"‚ö† Cannot visualize features without delta features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_model = PredictionModel('config.yaml')\n",
    "print(dir(pred_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna().reset_index(drop=True)\n",
    "X_train['date'] = with_deltas.loc[X_train.index, 'date'].values\n",
    "\n",
    "gdp_df = macro_data['gdp'].copy()\n",
    "gdp_df['date'] = pd.to_datetime(gdp_df['date'])\n",
    "train_data = X_train.merge(gdp_df, on='date', how='inner')\n",
    "X_train = train_data[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].values\n",
    "y_train = train_data['value'].values\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Target data: {y_train.shape}\")\n",
    "gdp_models = pred_model.train_gdp_models(X_train, y_train)\n",
    "print(f\"Model R¬≤: {gdp_models['gdp'].score(X_train, y_train):.3f}\")\n",
    "gdp_model = pred_model.train_gdp_model(X_train.values, y_train.values)\n",
    "print(f\"Training data: {X_train.shape}\")\n",
    "print(f\"Target data: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GDP prediction model\n",
    "gdp_model = pred_model.train_gdp_model(X_train, y_train)\n",
    "print(f\"\\nGDP Model Trained\")\n",
    "print(f\"  Model type: {type(gdp_model).__name__}\")\n",
    "print(f\"  Training R¬≤: {gdp_model.score(X_train, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using real test data\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    # Use the most recent features for out-of-sample prediction\n",
    "    test_features = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna().tail(10)\n",
    "    test_dates = with_deltas.loc[test_features.index, 'date']\n",
    "    \n",
    "    predictions = gdp_model.predict(test_features.values)\n",
    "\n",
    "    print(f\"\\nGDP Predictions (1Q ahead) for recent quarters:\")\n",
    "    for date, pred in zip(test_dates, predictions):\n",
    "        print(f\"  {date.strftime('%Y-%m-%d')}: {pred:.3f}%\")\n",
    "    print(f\"\\n  Mean: {predictions.mean():.3f}%\")\n",
    "    print(f\"  Std: {predictions.std():.3f}%\")\n",
    "    print(f\"  Range: [{predictions.min():.3f}, {predictions.max():.3f}]%\")\n",
    "else:\n",
    "    print(\"‚ö† Cannot make predictions without real AGG scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Signal Generation & Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize signal generator\n",
    "signal_gen = SignalGenerator('config.yaml')\n",
    "\n",
    "# Use real predictions from trained models\n",
    "# This requires: \n",
    "# 1. Features from AGG scores\n",
    "# 2. Trained GDP/IP models\n",
    "# 3. SPF forecasts from data_acq.fetch_spf_forecasts()\n",
    "\n",
    "if len(agg_scores) > 0 and 'agg_score' in agg_scores.columns:\n",
    "    # Use real model predictions\n",
    "    features_for_pred = with_deltas[['agg_score_norm', 'yoy_change', 'qoq_change', 'momentum']].dropna()\n",
    "    dates_for_pred = with_deltas.loc[features_for_pred.index, 'date']\n",
    "    \n",
    "\n",
    "    # Get predictions from trained model    predictions_df = pd.DataFrame()\n",
    "\n",
    "    gdp_predictions = gdp_model.predict(features_for_pred.values)    print(\"‚ö† Cannot generate predictions without real AGG scores\")\n",
    "\n",
    "    else:\n",
    "\n",
    "    # Fetch real SPF forecasts    print(predictions_df.head())\n",
    "\n",
    "    try:    print(\"‚úì Real Predictions vs SPF:\")\n",
    "\n",
    "        spf_data = data_acq.fetch_spf_forecasts(start_date, end_date)    \n",
    "\n",
    "        spf_data['date'] = pd.to_datetime(spf_data['date'])    predictions_df.rename(columns={'rgdp_1q': 'gdp_spf'}, inplace=True)\n",
    "\n",
    "    except Exception as e:    predictions_df = predictions_df.merge(spf_data[['date', 'rgdp_1q']], on='date', how='left')\n",
    "\n",
    "        print(f\"‚ö† Could not fetch SPF data: {e}\")    })\n",
    "\n",
    "        spf_data = pd.DataFrame({'date': dates_for_pred, 'rgdp_1q': [2.0]*len(dates_for_pred)})        'gdp_pred': gdp_predictions\n",
    "\n",
    "            'date': dates_for_pred.values,\n",
    "\n",
    "    # Combine predictions with SPF    predictions_df = pd.DataFrame({"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate trading signals (only if we have real predictions)\n",
    "if len(predictions_df) > 0:\n",
    "    signals = signal_gen.generate_signals(predictions_df)\n",
    "    print(f\"\\nüìä Trading Signals Generated:\")\n",
    "    print(signals.head(10))\n",
    "    print(f\"\\nSignal distribution:\")\n",
    "    print(signals['signal'].value_counts())\n",
    "else:\n",
    "    print(\"‚ö† Cannot generate signals without predictions\")\n",
    "    signals = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize backtester\n",
    "backtester = Backtester('config.yaml')\n",
    "\n",
    "# Use real returns from strategy execution\n",
    "# This requires:\n",
    "# 1. Trading signals from signal_gen.generate_signals()\n",
    "# 2. Sector ETF price data\n",
    "# 3. Portfolio construction and rebalancing\n",
    "\n",
    "if len(predictions_df) > 0:\n",
    "    # Fetch real ETF price data for sectors\n",
    "    sector_etfs = config['strategy']['sector_etfs']\n",
    "    etf_start = config['backtest']['test_start']\n",
    "    etf_end = config['backtest']['test_end']\n",
    "    \n",
    "    etf_prices = data_acq.fetch_etf_prices(sector_etfs, etf_start, etf_end)\n",
    "    \n",
    "    if etf_prices:\n",
    "        print(f\"‚úì Fetched price data for {len(etf_prices)} sector ETFs\")\n",
    "\n",
    "                    print(f\"  {metric}: {value}\")\n",
    "\n",
    "        # Run backtest with real data        else:\n",
    "\n",
    "        # Note: This requires implementing the full backtesting logic            print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "        # For now, we show the structure        if isinstance(value, float):\n",
    "\n",
    "        print(\"\\n‚ö† Full backtest execution requires:\")    for metric, value in metrics.items():\n",
    "\n",
    "        print(\"  1. Signals from signal_gen.generate_signals(predictions_df)\")    print(f\"\\nüìà Performance Metrics:\")\n",
    "\n",
    "        print(\"  2. Portfolio construction based on signals\")    metrics = backtester.calculate_metrics(portfolio_returns)\n",
    "\n",
    "        print(\"  3. Daily rebalancing and return calculation\")    # Calculate performance metrics\n",
    "\n",
    "        print(\"  4. Benchmark comparison (SPY or equal-weight)\")if len(portfolio_returns) > 0:\n",
    "\n",
    "        \n",
    "\n",
    "        portfolio_returns = pd.DataFrame()    portfolio_returns = pd.DataFrame()\n",
    "\n",
    "        print(\"\\nPlease implement backtester.run_backtest(signals, etf_prices) for real returns\")    print(\"‚ö† Cannot run backtest without predictions\")\n",
    "\n",
    "    else:else:\n",
    "\n",
    "        print(\"‚ö† No ETF price data available\")        portfolio_returns = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns and plot (only if we have real returns)\n",
    "if len(portfolio_returns) > 0 and 'strategy_return' in portfolio_returns.columns:\n",
    "    portfolio_returns['strategy_cumret'] = (1 + portfolio_returns['strategy_return']).cumprod() - 1\n",
    "    portfolio_returns['benchmark_cumret'] = (1 + portfolio_returns['benchmark_return']).cumprod() - 1\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(portfolio_returns['date'], portfolio_returns['strategy_cumret'] * 100, \n",
    "            label='Strategy', linewidth=2)\n",
    "    ax.plot(portfolio_returns['date'], portfolio_returns['benchmark_cumret'] * 100, \n",
    "            label='Benchmark', linewidth=2, linestyle='--')\n",
    "\n",
    "    ax.set_title('Strategy vs Benchmark Cumulative Returns', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Return (%)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"‚úì Backtest visualization complete\")    print(\"5. Execute backtest with real ETF prices\")\n",
    "\n",
    "else:    print(\"4. Generate trading signals\")\n",
    "\n",
    "    print(\"‚ö† No portfolio returns available for visualization\")    print(\"3. Train prediction models\")\n",
    "\n",
    "    print(\"\\nTo complete the full pipeline with real data:\")    print(\"2. Engineer features from AGG scores\")\n",
    "    print(\"1. Score earnings transcripts ‚Üí agg_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Complete Pipeline with Real Data\n",
    "\n",
    "This notebook demonstrates the **AI Economy Score Predictor** strategy pipeline using **real data sources**:\n",
    "\n",
    "### ‚úÖ Real Data Used:\n",
    "1. **Macroeconomic Data**: From FRED API (GDP, Industrial Production, Employment, Wages)\n",
    "2. **Control Variables**: From FRED API (Yield Curve, Consumer Sentiment, Unemployment)\n",
    "3. **PMI Data**: Loaded from `pmi_data.csv` \n",
    "4. **S&P 500 Constituents**: From `constituents.csv`\n",
    "5. **ETF Prices**: Fetched via yfinance API\n",
    "\n",
    "### ‚ö†Ô∏è Real Data Needed:\n",
    "- **Earnings Call Transcripts** with LLM sentiment scores aggregated quarterly ‚Üí `agg_scores.csv`\n",
    "\n",
    "### Pipeline Steps:\n",
    "1. **Data Acquisition** ‚úì Uses real FRED API and local files\n",
    "2. **LLM Scoring** ‚Üí Requires real earnings transcripts (Seeking Alpha, CapIQ, Bloomberg)\n",
    "3. **Feature Engineering** ‚úì Works with real AGG scores once available\n",
    "4. **Prediction Models** ‚úì Trains on real macro data + AGG features\n",
    "5. **Signal Generation** ‚úì Compares predictions to SPF forecasts\n",
    "6. **Backtesting** ‚úì Uses real sector ETF prices\n",
    "\n",
    "### Next Steps:\n",
    "1. Obtain earnings call transcripts from a data provider\n",
    "2. Score transcripts using `LLMScorer.score_multiple_transcripts()`\n",
    "3. Aggregate scores by quarter and save to `agg_scores.csv`\n",
    "4. Re-run this notebook to execute the full pipeline with real signals\n",
    "\n",
    "**No synthetic/random data is used for actual trading signals - all results require real transcript scoring.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data availability\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Data File Status:\\n\")\n",
    "\n",
    "required_files = {\n",
    "    'config.yaml': 'Configuration file',\n",
    "    'constituents.csv': 'S&P 500 constituents',\n",
    "    'pmi_data.csv': 'PMI data'\n",
    "}\n",
    "\n",
    "optional_files = {\n",
    "    'agg_scores.csv': 'Aggregated LLM sentiment scores (REQUIRED for full pipeline)'\n",
    "}\n",
    "\n",
    "for file, desc in required_files.items():\n",
    "    status = \"‚úì\" if os.path.exists(file) else \"‚úó\"\n",
    "    print(f\"{status} {file}: {desc}\")\n",
    "\n",
    "print(\"\\nOptional (but critical):\")\n",
    "for file, desc in optional_files.items():\n",
    "    status = \"‚úì\" if os.path.exists(file) else \"‚úó MISSING\"\n",
    "    print(f\"{status} {file}: {desc}\")\n",
    "\n",
    "if not os.path.exists('agg_scores.csv'):\n",
    "    print(\"\\n‚ö†Ô∏è  To create agg_scores.csv, you need to:\")\n",
    "    print(\"   1. Get earnings transcripts from a data provider\")\n",
    "    print(\"   2. Run LLM scoring (see 'Note: To Use Real Data' section above)\")\n",
    "    print(\"   3. Use the aggregate_scores_by_quarter() function\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
