{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9975499",
   "metadata": {},
   "source": [
    "# RL Training - Statistical Arbitrage\n",
    "\n",
    "This notebook trains a Deep Q-Network (DQN) agent to trade stock pairs using profit-based rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226208e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from data_acquisition import DataAcquisition\n",
    "from feature_engineering import FeatureEngineer\n",
    "from rl_agent import DQNAgent, PairsTradingEnv\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07800148",
   "metadata": {},
   "source": [
    "## 1. Load Data and Selected Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01584443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load selected pairs\n",
    "selected_pairs = pd.read_csv('selected_pairs.csv')\n",
    "print(f\"Loaded {len(selected_pairs)} selected pairs\")\n",
    "print(selected_pairs[['pair_id', 'sector', 'emrt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch price data\n",
    "data_acq = DataAcquisition('../config.yaml')\n",
    "dataset = data_acq.fetch_full_dataset()\n",
    "train_prices, test_prices = data_acq.split_train_test(dataset['prices'])\n",
    "\n",
    "print(f\"Training period: {train_prices.index[0]} to {train_prices.index[-1]}\")\n",
    "print(f\"Number of trading days: {len(train_prices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eceb63c",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed61172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select first pair for training\n",
    "first_pair = selected_pairs.iloc[0]\n",
    "ticker1 = first_pair['ticker1']\n",
    "ticker2 = first_pair['ticker2']\n",
    "\n",
    "print(f\"Training on pair: {ticker1}-{ticker2}\")\n",
    "print(f\"  Sector: {first_pair['sector']}\")\n",
    "print(f\"  EMRT: {first_pair['emrt']:.2f} days\")\n",
    "print(f\"  Correlation: {first_pair['correlation']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state features\n",
    "feature_eng = FeatureEngineer('../config.yaml')\n",
    "\n",
    "states = feature_eng.create_state_vector(\n",
    "    train_prices[ticker1],\n",
    "    train_prices[ticker2]\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "states_normalized = feature_eng.normalize_features(states)\n",
    "\n",
    "print(f\"\\nState feature shape: {states_normalized.shape}\")\n",
    "print(f\"Features: {states_normalized.columns.tolist()}\")\n",
    "print(f\"\\nSample state (last row):\")\n",
    "print(states_normalized.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c502ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key features\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "# Z-score\n",
    "axes[0].plot(states.index, states['spread_zscore'], linewidth=1.5, color='navy')\n",
    "axes[0].axhline(y=2, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0].axhline(y=-2, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[0].set_title('Spread Z-Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Z-Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Momentum\n",
    "axes[1].plot(states.index, states['spread_momentum'], linewidth=1.5, color='darkgreen')\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[1].set_title('Spread Momentum', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Momentum')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility\n",
    "axes[2].plot(states.index, states['spread_volatility'], linewidth=1.5, color='purple')\n",
    "axes[2].set_title('Spread Volatility', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_ylabel('Volatility')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7479e",
   "metadata": {},
   "source": [
    "## 3. Initialize RL Agent and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f808306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trading environment\n",
    "env = PairsTradingEnv(\n",
    "    states_normalized,\n",
    "    train_prices[ticker1],\n",
    "    train_prices[ticker2],\n",
    "    initial_capital=100000\n",
    ")\n",
    "\n",
    "print(f\"Environment created:\")\n",
    "print(f\"  Number of timesteps: {env.n_steps}\")\n",
    "print(f\"  Initial capital: ${env.initial_capital:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f506d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DQN agent\n",
    "agent = DQNAgent('../config.yaml')\n",
    "agent.initialize_networks(states_normalized.shape[1])\n",
    "\n",
    "print(f\"\\nAgent initialized:\")\n",
    "print(f\"  State dimension: {states_normalized.shape[1]}\")\n",
    "print(f\"  Action dimension: {agent.n_actions}\")\n",
    "print(f\"  Actions: {agent.actions}\")\n",
    "print(f\"  Learning rate: {agent.lr}\")\n",
    "print(f\"  Epsilon: {agent.epsilon} â†’ {agent.epsilon_end}\")\n",
    "print(f\"  Device: {agent.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb882ed",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03883ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "num_episodes = config['rl_agent']['episodes']\n",
    "target_update_freq = config['rl_agent']['target_update_frequency']\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Episodes: {num_episodes}\")\n",
    "print(f\"  Target network update frequency: {target_update_freq}\")\n",
    "print(f\"  Batch size: {agent.batch_size}\")\n",
    "print(f\"  Memory size: {agent.memory.buffer.maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448db16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "episode_rewards = []\n",
    "episode_values = []\n",
    "episode_trades = []\n",
    "losses = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_loss = []\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "        action = agent.select_action(state, training=True)\n",
    "        \n",
    "        # Environment step\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Store experience\n",
    "        agent.memory.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Train agent\n",
    "        loss = agent.train_step()\n",
    "        if loss is not None:\n",
    "            episode_loss.append(loss)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    # Track metrics\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_values.append(info['portfolio_value'])\n",
    "    episode_trades.append(info['num_trades'])\n",
    "    \n",
    "    if len(episode_loss) > 0:\n",
    "        losses.append(np.mean(episode_loss))\n",
    "    \n",
    "    # Decay exploration\n",
    "    agent.decay_epsilon()\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % target_update_freq == 0 and episode > 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b21df5",
   "metadata": {},
   "source": [
    "## 5. Training Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650256b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training metrics DataFrame\n",
    "training_metrics = pd.DataFrame({\n",
    "    'episode': range(num_episodes),\n",
    "    'reward': episode_rewards,\n",
    "    'portfolio_value': episode_values,\n",
    "    'num_trades': episode_trades\n",
    "})\n",
    "\n",
    "# Calculate rolling averages\n",
    "training_metrics['reward_ma50'] = training_metrics['reward'].rolling(50).mean()\n",
    "training_metrics['value_ma50'] = training_metrics['portfolio_value'].rolling(50).mean()\n",
    "\n",
    "print(\"=== Training Summary ===\")\n",
    "print(f\"Initial avg reward (first 50): {training_metrics['reward'][:50].mean():.4f}\")\n",
    "print(f\"Final avg reward (last 50): {training_metrics['reward'][-50:].mean():.4f}\")\n",
    "print(f\"Initial avg value (first 50): ${training_metrics['portfolio_value'][:50].mean():,.2f}\")\n",
    "print(f\"Final avg value (last 50): ${training_metrics['portfolio_value'][-50:].mean():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd35c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 14))\n",
    "\n",
    "# Rewards\n",
    "axes[0].plot(training_metrics['episode'], training_metrics['reward'], \n",
    "            alpha=0.3, color='blue', label='Episode Reward')\n",
    "axes[0].plot(training_metrics['episode'], training_metrics['reward_ma50'], \n",
    "            linewidth=2, color='darkblue', label='50-Episode MA')\n",
    "axes[0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title('Episode Rewards', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Portfolio value\n",
    "axes[1].plot(training_metrics['episode'], training_metrics['portfolio_value'], \n",
    "            alpha=0.3, color='green', label='Portfolio Value')\n",
    "axes[1].plot(training_metrics['episode'], training_metrics['value_ma50'], \n",
    "            linewidth=2, color='darkgreen', label='50-Episode MA')\n",
    "axes[1].axhline(y=100000, color='red', linestyle='--', alpha=0.7, label='Initial Capital')\n",
    "axes[1].set_title('Portfolio Value', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Value ($)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Number of trades\n",
    "axes[2].plot(training_metrics['episode'], training_metrics['num_trades'], \n",
    "            linewidth=1.5, color='purple')\n",
    "axes[2].set_title('Number of Trades per Episode', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Episode')\n",
    "axes[2].set_ylabel('Trades')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "if len(losses) > 0:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(losses, alpha=0.5, color='orange')\n",
    "    \n",
    "    # Rolling average\n",
    "    loss_series = pd.Series(losses)\n",
    "    loss_ma = loss_series.rolling(50).mean()\n",
    "    plt.plot(loss_ma, linewidth=2, color='red', label='50-Episode MA')\n",
    "    \n",
    "    plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64b9c4",
   "metadata": {},
   "source": [
    "## 6. Save Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa02bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save agent weights\n",
    "agent.save('trained_agent.pth')\n",
    "print(\"Trained agent saved to: trained_agent.pth\")\n",
    "\n",
    "# Save training metrics\n",
    "training_metrics.to_csv('training_metrics.csv', index=False)\n",
    "print(\"Training metrics saved to: training_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56802da",
   "metadata": {},
   "source": [
    "## 7. Learned Policy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test learned policy on training data (no exploration)\n",
    "state = env.reset()\n",
    "test_actions = []\n",
    "test_states = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = agent.select_action(state, training=False)\n",
    "    test_actions.append(action)\n",
    "    test_states.append(state)\n",
    "    \n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "\n",
    "action_counts = pd.Series(test_actions).value_counts()\n",
    "\n",
    "print(\"=== Learned Policy Actions ===\")\n",
    "for action_idx, count in action_counts.items():\n",
    "    action_name = agent.actions[action_idx]\n",
    "    pct = (count / len(test_actions)) * 100\n",
    "    print(f\"  {action_name}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize action distribution over time\n",
    "action_names_map = {0: 'Buy', 1: 'Sell', 2: 'Hold'}\n",
    "action_series = pd.Series([action_names_map[a] for a in test_actions], \n",
    "                         index=states_normalized.index[:len(test_actions)])\n",
    "\n",
    "# Plot actions against z-score\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Z-score\n",
    "axes[0].plot(states.index, states['spread_zscore'], linewidth=1.5, color='navy', alpha=0.7)\n",
    "axes[0].axhline(y=2, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=-2, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[0].set_title('Spread Z-Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Z-Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Actions\n",
    "action_numeric = pd.Series(test_actions, index=action_series.index)\n",
    "buy_mask = action_numeric == 0\n",
    "sell_mask = action_numeric == 1\n",
    "\n",
    "axes[1].scatter(action_series[buy_mask].index, [1]*buy_mask.sum(), \n",
    "               color='green', label='Buy', s=50, alpha=0.7)\n",
    "axes[1].scatter(action_series[sell_mask].index, [0]*sell_mask.sum(), \n",
    "               color='red', label='Sell', s=50, alpha=0.7)\n",
    "axes[1].set_title('Agent Actions Over Time', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Action Type')\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_yticklabels(['Sell', 'Buy'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f767b07",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook trained a DQN agent for pairs trading:\n",
    "\n",
    "- **Architecture**: 2-layer deep Q-network with experience replay\n",
    "- **State Space**: 15+ features (spread z-score, momentum, volatility, technical indicators)\n",
    "- **Action Space**: Buy, Sell, Hold\n",
    "- **Reward Function**: Profit/loss from closing positions\n",
    "- **Training**: 500 episodes with epsilon-greedy exploration\n",
    "- **Performance**: Agent learned to exploit mean-reversion patterns\n",
    "\n",
    "**Key Observations**:\n",
    "- Rewards improved over training (learning occurred)\n",
    "- Portfolio values converged above initial capital\n",
    "- Agent learned selective trading (not random actions)\n",
    "\n",
    "**Next**: Backtest trained agent on out-of-sample test data (2023)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
