{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58619926",
   "metadata": {},
   "source": [
    "# Pragmatic Asset Allocation - Data Exploration\n",
    "\n",
    "This notebook explores the asset data quality, statistical properties, and relationships for the Pragmatic Asset Allocation Model.\n",
    "\n",
    "## Objectives:\n",
    "- Assess data completeness and quality\n",
    "- Analyze return distributions and correlations\n",
    "- Evaluate liquidity and trading characteristics\n",
    "- Check for survivorship bias and data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbad715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Strategy: {config['strategy']['name']}\")\n",
    "print(f\"Backtest period: {config['backtest']['start_date']} to {config['backtest']['end_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26eb80",
   "metadata": {},
   "source": [
    "## 1. Load and Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39495fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from data acquisition\n",
    "try:\n",
    "    from data_acquisition import PragmaticAssetAllocationData\n",
    "    \n",
    "    data_acq = PragmaticAssetAllocationData()\n",
    "    \n",
    "    # Try to load cached data first\n",
    "    all_data = data_acq.load_cached_data()\n",
    "    \n",
    "    if not all_data:\n",
    "        print(\"No cached data found. Run data acquisition first.\")\n",
    "        print(\"Use: python ../main.py --mode data\")\n",
    "        all_data = {}\n",
    "    else:\n",
    "        print(\"Cached data loaded successfully\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Could not import data acquisition module\")\n",
    "    all_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59911b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "if all_data:\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\\n\")\n",
    "    \n",
    "    for data_type, df in all_data.items():\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            print(f\"{data_type.upper()}:\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Date range: {df.index.min()} to {df.index.max()}\")\n",
    "            print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "            print(f\"  Completeness: {(1 - df.isnull().sum().sum() / df.size):.1%}\")\n",
    "            \n",
    "            # Check for data quality issues\n",
    "            if df.isnull().sum().sum() > 0:\n",
    "                print(f\"  ⚠️  Missing data detected\")\n",
    "            else:\n",
    "                print(f\"  ✓ Complete data\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"{data_type}: Not a DataFrame\")\n",
    "else:\n",
    "    print(\"No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdbec3d",
   "metadata": {},
   "source": [
    "## 2. Risky Assets Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze risky assets\n",
    "if 'risky_assets' in all_data:\n",
    "    risky_data = all_data['risky_assets']\n",
    "    print(\"=== RISKY ASSETS ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    risky_assets = config['assets']['risky']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    for i, asset in enumerate(risky_assets):\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in risky_data.columns.levels[0]:\n",
    "            prices = risky_data[ticker]['Adj Close']\n",
    "            returns = prices.pct_change().dropna()\n",
    "            \n",
    "            # Price chart\n",
    "            ax = axes[i//3, i%3]\n",
    "            prices.plot(ax=ax, linewidth=1)\n",
    "            ax.set_title(f'{asset[\"name\"]} ({ticker})')\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Price ($)')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"{asset['name']} ({ticker}):\")\n",
    "            print(f\"  Start Price: ${prices.iloc[0]:.2f}\")\n",
    "            print(f\"  End Price: ${prices.iloc[-1]:.2f}\")\n",
    "            print(f\"  Total Return: {((prices.iloc[-1]/prices.iloc[0])-1):.1%}\")\n",
    "            print(f\"  Annual Volatility: {returns.std() * np.sqrt(252):.1%}\")\n",
    "            print(f\"  Sharpe Ratio: {(returns.mean()/returns.std()) * np.sqrt(252):.2f}\")\n",
    "            print(f\"  Max Drawdown: {((prices/prices.expanding().max()-1).min()):.1%}\")\n",
    "            print()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Risky assets data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return distribution analysis\n",
    "if 'risky_assets' in all_data:\n",
    "    risky_data = all_data['risky_assets']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    for i, asset in enumerate(config['assets']['risky']):\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in risky_data.columns.levels[0]:\n",
    "            prices = risky_data[ticker]['Adj Close']\n",
    "            returns = prices.pct_change().dropna()\n",
    "            \n",
    "            # Returns distribution\n",
    "            ax = axes[i//3, i%3]\n",
    "            ax.hist(returns, bins=50, alpha=0.7, density=True, edgecolor='black')\n",
    "            ax.axvline(returns.mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {returns.mean():.2%}')\n",
    "            ax.axvline(returns.median(), color='blue', linestyle='--',\n",
    "                      label=f'Median: {returns.median():.2%}')\n",
    "            ax.set_title(f'{asset[\"name\"]} Return Distribution')\n",
    "            ax.set_xlabel('Daily Return')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics text\n",
    "            stats_text = f'Skew: {returns.skew():.2f}\\nKurt: {returns.kurtosis():.2f}'\n",
    "            ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, \n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics table\n",
    "    print(\"\\n=== RETURN DISTRIBUTION SUMMARY ===\\n\")\n",
    "    summary_stats = []\n",
    "    \n",
    "    for asset in config['assets']['risky']:\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in risky_data.columns.levels[0]:\n",
    "            returns = risky_data[ticker]['Adj Close'].pct_change().dropna()\n",
    "            \n",
    "            stats = {\n",
    "                'Asset': asset['name'],\n",
    "                'Mean': returns.mean(),\n",
    "                'Std': returns.std(),\n",
    "                'Skewness': returns.skew(),\n",
    "                'Kurtosis': returns.kurtosis(),\n",
    "                'Min': returns.min(),\n",
    "                'Max': returns.max(),\n",
    "                '5% VaR': returns.quantile(0.05),\n",
    "                '95% VaR': returns.quantile(0.95)\n",
    "            }\n",
    "            summary_stats.append(stats)\n",
    "    \n",
    "    if summary_stats:\n",
    "        summary_df = pd.DataFrame(summary_stats)\n",
    "        # Format as percentages\n",
    "        pct_cols = ['Mean', 'Std', 'Min', 'Max', '5% VaR', '95% VaR']\n",
    "        summary_df[pct_cols] = summary_df[pct_cols].apply(lambda x: x.map('{:.2%}'.format))\n",
    "        summary_df[['Skewness', 'Kurtosis']] = summary_df[['Skewness', 'Kurtosis']].apply(lambda x: x.map('{:.2f}'.format))\n",
    "        \n",
    "        print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e308da1b",
   "metadata": {},
   "source": [
    "## 3. Hedging Assets Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e49970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hedging assets\n",
    "if 'hedging_assets' in all_data:\n",
    "    hedging_data = all_data['hedging_assets']\n",
    "    print(\"=== HEDGING ASSETS ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    hedging_assets = config['assets']['hedging']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    for i, asset in enumerate(hedging_assets):\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in hedging_data.columns.levels[0]:\n",
    "            prices = hedging_data[ticker]['Adj Close']\n",
    "            returns = prices.pct_change().dropna()\n",
    "            \n",
    "            # Price chart\n",
    "            ax = axes[i]\n",
    "            prices.plot(ax=ax, linewidth=1)\n",
    "            ax.set_title(f'{asset[\"name\"]} ({ticker})')\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Price ($)')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"{asset['name']} ({ticker}):\")\n",
    "            print(f\"  Start Price: ${prices.iloc[0]:.2f}\")\n",
    "            print(f\"  End Price: ${prices.iloc[-1]:.2f}\")\n",
    "            print(f\"  Total Return: {((prices.iloc[-1]/prices.iloc[0])-1):.1%}\")\n",
    "            print(f\"  Annual Volatility: {returns.std() * np.sqrt(252):.1%}\")\n",
    "            print(f\"  Sharpe Ratio: {(returns.mean()/returns.std()) * np.sqrt(252):.2f}\")\n",
    "            print(f\"  Max Drawdown: {((prices/prices.expanding().max()-1).min()):.1%}\")\n",
    "            print()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Hedging assets data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419e97a",
   "metadata": {},
   "source": [
    "## 4. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between all assets\n",
    "if 'risky_assets' in all_data and 'hedging_assets' in all_data:\n",
    "    print(\"=== CORRELATION ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Combine all asset returns\n",
    "    all_returns = pd.DataFrame()\n",
    "    \n",
    "    # Risky assets\n",
    "    for asset in config['assets']['risky']:\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in all_data['risky_assets'].columns.levels[0]:\n",
    "            returns = all_data['risky_assets'][ticker]['Adj Close'].pct_change()\n",
    "            all_returns[f'{ticker}'] = returns\n",
    "    \n",
    "    # Hedging assets\n",
    "    for asset in config['assets']['hedging']:\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in all_data['hedging_assets'].columns.levels[0]:\n",
    "            returns = all_data['hedging_assets'][ticker]['Adj Close'].pct_change()\n",
    "            all_returns[f'{ticker}'] = returns\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = all_returns.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Asset Return Correlations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation insights\n",
    "    print(\"\\nCorrelation Insights:\")\n",
    "    print(f\"Average correlation between risky assets: {corr_matrix.loc[['QQQ', 'URTH', 'EEM'], ['QQQ', 'URTH', 'EEM']].mean().mean():.2f}\")\n",
    "    print(f\"Average correlation between hedging assets: {corr_matrix.loc[['IEF', 'GLD'], ['IEF', 'GLD']].mean().mean():.2f}\")\n",
    "    \n",
    "    # Risky vs Hedging correlations\n",
    "    risky_hedge_corr = corr_matrix.loc[['QQQ', 'URTH', 'EEM'], ['IEF', 'GLD']].mean().mean()\n",
    "    print(f\"Average correlation between risky and hedging assets: {risky_hedge_corr:.2f}\")\n",
    "    \n",
    "    if risky_hedge_corr < -0.2:\n",
    "        print(\"✅ Good diversification: Negative correlation between risky and hedging assets\")\n",
    "    elif risky_hedge_corr > 0.5:\n",
    "        print(\"⚠️ Poor diversification: High correlation between risky and hedging assets\")\n",
    "    else:\n",
    "        print(\"✓ Moderate diversification between asset classes\")\n",
    "    \n",
    "    # Rolling correlations (last 5 years if available)\n",
    "    if len(all_returns) > 252*5:\n",
    "        print(\"\\n=== ROLLING CORRELATIONS (5-year windows) ===\")\n",
    "        rolling_corr = all_returns.rolling(window=252*5).corr()\n",
    "        \n",
    "        # Plot rolling correlations for key pairs\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # QQQ vs IEF\n",
    "        if ('QQQ' in rolling_corr.columns.levels[0] and \n",
    "            'IEF' in rolling_corr.columns.levels[0]):\n",
    "            qqq_ief_corr = rolling_corr.loc[pd.IndexSlice[:, 'QQQ'], 'IEF'].droplevel(0)\n",
    "            qqq_ief_corr.plot(ax=axes[0,0], title='QQQ vs IEF Rolling Correlation')\n",
    "            axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # QQQ vs GLD\n",
    "        if ('QQQ' in rolling_corr.columns.levels[0] and \n",
    "            'GLD' in rolling_corr.columns.levels[0]):\n",
    "            qqq_gld_corr = rolling_corr.loc[pd.IndexSlice[:, 'QQQ'], 'GLD'].droplevel(0)\n",
    "            qqq_gld_corr.plot(ax=axes[0,1], title='QQQ vs GLD Rolling Correlation')\n",
    "            axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # URTH vs IEF\n",
    "        if ('URTH' in rolling_corr.columns.levels[0] and \n",
    "            'IEF' in rolling_corr.columns.levels[0]):\n",
    "            urth_ief_corr = rolling_corr.loc[pd.IndexSlice[:, 'URTH'], 'IEF'].droplevel(0)\n",
    "            urth_ief_corr.plot(ax=axes[1,0], title='URTH vs IEF Rolling Correlation')\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # EEM vs GLD\n",
    "        if ('EEM' in rolling_corr.columns.levels[0] and \n",
    "            'GLD' in rolling_corr.columns.levels[0]):\n",
    "            eem_gld_corr = rolling_corr.loc[pd.IndexSlice[:, 'EEM'], 'GLD'].droplevel(0)\n",
    "            eem_gld_corr.plot(ax=axes[1,1], title='EEM vs GLD Rolling Correlation')\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb25e55",
   "metadata": {},
   "source": [
    "## 5. Macroeconomic Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ae5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze macroeconomic indicators\n",
    "if 'macroeconomic' in all_data:\n",
    "    macro_data = all_data['macroeconomic']\n",
    "    print(\"=== MACROECONOMIC ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Yield curve analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Treasury yields over time\n",
    "    if '3M_Treasury_Yield' in macro_data.columns:\n",
    "        macro_data['3M_Treasury_Yield'].plot(ax=axes[0,0], label='3M Yield', linewidth=1)\n",
    "    if '10Y_Treasury_Yield' in macro_data.columns:\n",
    "        macro_data['10Y_Treasury_Yield'].plot(ax=axes[0,0], label='10Y Yield', linewidth=1)\n",
    "    axes[0,0].set_title('Treasury Yields Over Time')\n",
    "    axes[0,0].set_ylabel('Yield (%)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Yield curve spread\n",
    "    if 'Yield_Curve_Spread' in macro_data.columns:\n",
    "        macro_data['Yield_Curve_Spread'].plot(ax=axes[0,1], linewidth=1, color='purple')\n",
    "        axes[0,1].axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Inversion Line')\n",
    "        axes[0,1].set_title('Yield Curve Spread (10Y - 3M)')\n",
    "        axes[0,1].set_ylabel('Spread (bps)')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight inversion periods\n",
    "        inversion_periods = macro_data[macro_data['Yield_Curve_Spread'] < 0]\n",
    "        if not inversion_periods.empty:\n",
    "            axes[0,1].fill_between(inversion_periods.index, inversion_periods['Yield_Curve_Spread'], 0, \n",
    "                                  color='red', alpha=0.3, label='Inverted')\n",
    "    \n",
    "    # Yield curve inversion frequency\n",
    "    if 'Yield_Curve_Inverted' in macro_data.columns:\n",
    "        inversion_pct = macro_data['Yield_Curve_Inverted'].mean()\n",
    "        print(f\"Yield curve inversion frequency: {inversion_pct:.1%}\")\n",
    "        \n",
    "        # Plot inversion periods\n",
    "        macro_data['Yield_Curve_Inverted'].astype(int).plot(ax=axes[1,0], linewidth=1, color='red')\n",
    "        axes[1,0].set_title('Yield Curve Inversion Periods')\n",
    "        axes[1,0].set_ylabel('Inverted (1=Yes, 0=No)')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add recession overlays (simplified)\n",
    "        recession_periods = [\n",
    "            ('1937-05-01', '1938-06-01'),\n",
    "            ('1940-10-01', '1941-03-01'),\n",
    "            ('1945-02-01', '1945-10-01'),\n",
    "            ('1948-11-01', '1949-10-01'),\n",
    "            ('1953-07-01', '1954-05-01'),\n",
    "            ('1957-08-01', '1958-04-01'),\n",
    "            ('1960-04-01', '1961-02-01'),\n",
    "            ('1969-12-01', '1970-11-01'),\n",
    "            ('1973-11-01', '1975-03-01'),\n",
    "            ('1980-01-01', '1980-07-01'),\n",
    "            ('1981-07-01', '1982-11-01'),\n",
    "            ('1990-07-01', '1991-03-01'),\n",
    "            ('2001-03-01', '2001-11-01'),\n",
    "            ('2007-12-01', '2009-06-01'),\n",
    "            ('2020-02-01', '2020-04-01')\n",
    "        ]\n",
    "        \n",
    "        for start, end in recession_periods:\n",
    "            try:\n",
    "                axes[1,0].axvspan(pd.to_datetime(start), pd.to_datetime(end), \n",
    "                                color='gray', alpha=0.3, label='Recession' if start == '1937-05-01' else \"\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        axes[1,0].legend()\n",
    "    \n",
    "    # Spread distribution\n",
    "    if 'Yield_Curve_Spread' in macro_data.columns:\n",
    "        spread_data = macro_data['Yield_Curve_Spread'].dropna()\n",
    "        axes[1,1].hist(spread_data, bins=50, alpha=0.7, density=True, edgecolor='black')\n",
    "        axes[1,1].axvline(spread_data.mean(), color='red', linestyle='--', \n",
    "                         label=f'Mean: {spread_data.mean():.1f} bps')\n",
    "        axes[1,1].axvline(0, color='black', linestyle='-', alpha=0.7, label='Inversion')\n",
    "        axes[1,1].set_title('Yield Curve Spread Distribution')\n",
    "        axes[1,1].set_xlabel('Spread (bps)')\n",
    "        axes[1,1].set_ylabel('Density')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print macroeconomic statistics\n",
    "    print(\"\\nMacroeconomic Statistics:\")\n",
    "    if 'Yield_Curve_Spread' in macro_data.columns:\n",
    "        spread = macro_data['Yield_Curve_Spread'].dropna()\n",
    "        print(f\"Average yield curve spread: {spread.mean():.1f} bps\")\n",
    "        print(f\"Spread volatility: {spread.std():.1f} bps\")\n",
    "        print(f\"Minimum spread: {spread.min():.1f} bps\")\n",
    "        print(f\"Maximum spread: {spread.max():.1f} bps\")\n",
    "        \n",
    "        # Inversion analysis\n",
    "        inversions = spread < 0\n",
    "        print(f\"Total inversion days: {inversions.sum()}\")\n",
    "        print(f\"Inversion percentage: {inversions.mean():.1%}\")\n",
    "        \n",
    "        if inversions.sum() > 0:\n",
    "            print(f\"Average inversion magnitude: {spread[inversions].mean():.1f} bps\")\n",
    "            print(f\"Worst inversion: {spread.min():.1f} bps\")\n",
    "else:\n",
    "    print(\"Macroeconomic data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a12ff0",
   "metadata": {},
   "source": [
    "## 6. Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "print(\"=== DATA QUALITY SUMMARY ===\\n\")\n",
    "\n",
    "quality_checks = {\n",
    "    'Data Completeness': [],\n",
    "    'Survivorship Bias': [],\n",
    "    'Liquidity Assessment': [],\n",
    "    'Statistical Properties': []\n",
    "}\n",
    "\n",
    "# Data completeness\n",
    "if all_data:\n",
    "    total_rows = 0\n",
    "    total_missing = 0\n",
    "    \n",
    "    for data_type, df in all_data.items():\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            total_rows += df.shape[0] * df.shape[1]\n",
    "            total_missing += df.isnull().sum().sum()\n",
    "    \n",
    "    completeness = (1 - total_missing / total_rows) if total_rows > 0 else 0\n",
    "    quality_checks['Data Completeness'].append(f\"Overall completeness: {completeness:.1%}\")\n",
    "    \n",
    "    if completeness > 0.95:\n",
    "        quality_checks['Data Completeness'].append(\"✅ Excellent data completeness\")\n",
    "    elif completeness > 0.90:\n",
    "        quality_checks['Data Completeness'].append(\"✓ Good data completeness\")\n",
    "    else:\n",
    "        quality_checks['Data Completeness'].append(\"⚠️ Data completeness concerns\")\n",
    "\n",
    "# Survivorship bias assessment\n",
    "start_date = pd.to_datetime(config['backtest']['start_date'])\n",
    "end_date = pd.to_datetime(config['backtest']['end_date'])\n",
    "expected_days = (end_date - start_date).days\n",
    "\n",
    "if 'risky_assets' in all_data:\n",
    "    risky_data = all_data['risky_assets']\n",
    "    actual_days = len(risky_data)\n",
    "    coverage = actual_days / expected_days if expected_days > 0 else 0\n",
    "    \n",
    "    quality_checks['Survivorship Bias'].append(f\"Data coverage: {coverage:.1%} of expected period\")\n",
    "    \n",
    "    if coverage > 0.8:\n",
    "        quality_checks['Survivorship Bias'].append(\"✅ Good historical coverage\")\n",
    "    elif coverage > 0.6:\n",
    "        quality_checks['Survivorship Bias'].append(\"✓ Acceptable historical coverage\")\n",
    "    else:\n",
    "        quality_checks['Survivorship Bias'].append(\"⚠️ Limited historical coverage - survivorship bias possible\")\n",
    "\n",
    "# Liquidity assessment (simplified)\n",
    "if 'risky_assets' in all_data:\n",
    "    risky_data = all_data['risky_assets']\n",
    "    \n",
    "    # Check for ETF availability dates\n",
    "    etf_start_dates = {}\n",
    "    for asset in config['assets']['risky'] + config['assets']['hedging']:\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in risky_data.columns.levels[0]:\n",
    "            first_valid = risky_data[ticker]['Adj Close'].first_valid_index()\n",
    "            etf_start_dates[ticker] = first_valid\n",
    "    \n",
    "    if etf_start_dates:\n",
    "        earliest_etf = min(etf_start_dates.values())\n",
    "        latest_etf = max(etf_start_dates.values())\n",
    "        \n",
    "        quality_checks['Liquidity Assessment'].append(f\"ETF availability range: {earliest_etf.date()} to {latest_etf.date()}\")\n",
    "        \n",
    "        # Check if we have data before ETF inception (proxy quality)\n",
    "        if earliest_etf > start_date:\n",
    "            quality_checks['Liquidity Assessment'].append(\"⚠️ ETF data starts after backtest begin - using proxies\")\n",
    "        else:\n",
    "            quality_checks['Liquidity Assessment'].append(\"✅ ETF data covers full backtest period\")\n",
    "\n",
    "# Statistical properties check\n",
    "if 'risky_assets' in all_data:\n",
    "    risky_data = all_data['risky_assets']\n",
    "    \n",
    "    # Check for reasonable return distributions\n",
    "    reasonable_stats = True\n",
    "    for asset in config['assets']['risky']:\n",
    "        ticker = asset['ticker']\n",
    "        if ticker in risky_data.columns.levels[0]:\n",
    "            returns = risky_data[ticker]['Adj Close'].pct_change().dropna()\n",
    "            \n",
    "            # Check for extreme outliers\n",
    "            if returns.std() > 0.05:  # >5% daily vol seems unreasonable\n",
    "                reasonable_stats = False\n",
    "                break\n",
    "            \n",
    "            # Check for negative skew (crashes)\n",
    "            if abs(returns.skew()) > 2:\n",
    "                reasonable_stats = False\n",
    "                break\n",
    "    \n",
    "    if reasonable_stats:\n",
    "        quality_checks['Statistical Properties'].append(\"✅ Reasonable statistical properties\")\n",
    "    else:\n",
    "        quality_checks['Statistical Properties'].append(\"⚠️ Unusual statistical properties detected\")\n",
    "\n",
    "# Print quality assessment\n",
    "for category, checks in quality_checks.items():\n",
    "    print(f\"{category}:\")\n",
    "    for check in checks:\n",
    "        print(f\"  {check}\")\n",
    "    print()\n",
    "\n",
    "print(\"=== DATA EXPLORATION COMPLETE ===\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review data quality issues identified above\")\n",
    "print(\"2. Run signal analysis notebook (02_signal_analysis.ipynb)\")\n",
    "print(\"3. Proceed with portfolio construction analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
