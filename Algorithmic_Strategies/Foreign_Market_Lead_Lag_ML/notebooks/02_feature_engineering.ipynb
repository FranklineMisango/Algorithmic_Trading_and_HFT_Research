{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreign Market Lead-Lag ML Strategy\n",
    "## Notebook 2: Feature Engineering\n",
    "\n",
    "This notebook creates and analyzes predictive features:\n",
    "- Create lagged weekly return features (188 features: 47 markets × 4 lags)\n",
    "- Apply cross-sectional standardization\n",
    "- Winsorize extreme values\n",
    "- Align features with target returns\n",
    "- Analyze feature distributions and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from feature_engineering import FeatureEngineering\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load data\n",
    "sp500_returns = pd.read_csv('../data/sp500_daily_returns.csv', index_col=0, parse_dates=True)\n",
    "foreign_returns = pd.read_csv('../data/foreign_weekly_returns.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"S&P 500 returns: {sp500_returns.shape}\")\n",
    "print(f\"Foreign returns: {foreign_returns.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Lagged Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineering\n",
    "feature_eng = FeatureEngineering(config)\n",
    "\n",
    "# Create lagged features\n",
    "lagged_features = feature_eng.create_lagged_features(foreign_returns)\n",
    "\n",
    "print(f\"\\nLagged features shape: {lagged_features.shape}\")\n",
    "print(f\"Number of features: {len(lagged_features.columns)}\")\n",
    "print(f\"\\nFirst 10 feature names:\")\n",
    "print(lagged_features.columns[:10].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Distribution Before Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of raw features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sample 4 random features\n",
    "sample_features = np.random.choice(lagged_features.columns, 4, replace=False)\n",
    "\n",
    "for idx, feature in enumerate(sample_features):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    lagged_features[feature].dropna().hist(bins=50, ax=ax, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'Distribution: {feature}', fontweight='bold')\n",
    "    ax.set_xlabel('Return')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nFeature Statistics (Before Standardization):\")\n",
    "print(lagged_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Winsorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize features\n",
    "winsorized_features = feature_eng.winsorize_features(lagged_features)\n",
    "\n",
    "# Compare before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sample_feature = lagged_features.columns[0]\n",
    "\n",
    "axes[0].hist(lagged_features[sample_feature].dropna(), bins=50, \n",
    "            edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_title(f'Before Winsorization: {sample_feature}', fontweight='bold')\n",
    "axes[0].set_xlabel('Return')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(winsorized_features[sample_feature].dropna(), bins=50, \n",
    "            edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_title(f'After Winsorization: {sample_feature}', fontweight='bold')\n",
    "axes[1].set_xlabel('Return')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Cross-Sectional Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "standardized_features = feature_eng.standardize_features(winsorized_features)\n",
    "\n",
    "print(\"\\nFeature Statistics (After Standardization):\")\n",
    "print(standardized_features.describe())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Before standardization\n",
    "winsorized_features.iloc[100].plot(kind='hist', bins=50, ax=axes[0], \n",
    "                                   edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_title('Cross-Sectional Distribution (Before)', fontweight='bold')\n",
    "axes[0].set_xlabel('Return')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# After standardization\n",
    "standardized_features.iloc[100].plot(kind='hist', bins=50, ax=axes[1], \n",
    "                                     edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_title('Cross-Sectional Distribution (After)', fontweight='bold')\n",
    "axes[1].set_xlabel('Standardized Return')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Training Data for Sample Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample stock\n",
    "sample_stock = sp500_returns.columns[0]\n",
    "print(f\"Sample stock: {sample_stock}\")\n",
    "\n",
    "# Prepare training data\n",
    "X, y = feature_eng.prepare_training_data(foreign_returns, sp500_returns, sample_stock)\n",
    "\n",
    "print(f\"\\nTraining data shape:\")\n",
    "print(f\"  Features (X): {X.shape}\")\n",
    "print(f\"  Target (y): {y.shape}\")\n",
    "print(f\"\\nDate range: {X.index[0]} to {X.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between features and target\n",
    "feature_target_corr = X.corrwith(y).sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 most correlated features\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "feature_target_corr.head(20).plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_title(f'Top 20 Features Correlated with {sample_stock} Returns', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Correlation')\n",
    "ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Correlated Features:\")\n",
    "for feature, corr in feature_target_corr.head(10).items():\n",
    "    print(f\"  {feature}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance by Lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation by lag\n",
    "lag_correlations = {}\n",
    "\n",
    "for lag in config['features']['lags']:\n",
    "    lag_features = [col for col in X.columns if f'_lag{lag}' in col]\n",
    "    lag_corr = X[lag_features].corrwith(y).abs().mean()\n",
    "    lag_correlations[f'Lag {lag}'] = lag_corr\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "pd.Series(lag_correlations).plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
    "ax.set_title(f'Average Absolute Correlation by Lag ({sample_stock})', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Lag')\n",
    "ax.set_ylabel('Avg |Correlation|')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage Correlation by Lag:\")\n",
    "for lag, corr in lag_correlations.items():\n",
    "    print(f\"  {lag}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Data for All Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for all stocks (this may take a few minutes)\n",
    "print(\"Preparing data for all stocks...\")\n",
    "stock_data = feature_eng.prepare_all_stocks(foreign_returns, sp500_returns)\n",
    "\n",
    "print(f\"\\nPrepared data for {len(stock_data)} stocks\")\n",
    "\n",
    "# Analyze sample sizes\n",
    "sample_sizes = {stock: len(X) for stock, (X, y) in stock_data.items()}\n",
    "sample_sizes_series = pd.Series(sample_sizes)\n",
    "\n",
    "print(f\"\\nSample Size Statistics:\")\n",
    "print(f\"  Mean: {sample_sizes_series.mean():.0f}\")\n",
    "print(f\"  Median: {sample_sizes_series.median():.0f}\")\n",
    "print(f\"  Min: {sample_sizes_series.min():.0f}\")\n",
    "print(f\"  Max: {sample_sizes_series.max():.0f}\")\n",
    "\n",
    "# Plot distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sample_sizes_series.hist(bins=50, ax=ax, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.set_title('Distribution of Sample Sizes Across Stocks', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Number of Samples')\n",
    "ax.set_ylabel('Number of Stocks')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook created and analyzed predictive features:\n",
    "- Created 188 lagged features (47 markets × 4 lags)\n",
    "- Applied winsorization to handle extreme values\n",
    "- Applied cross-sectional standardization to prevent volatility bias\n",
    "- Aligned features with target returns\n",
    "- Analyzed feature correlations and importance\n",
    "\n",
    "**Key Findings**:\n",
    "- Features show varying correlations with target returns\n",
    "- Different lags capture different predictive information\n",
    "- Cross-sectional standardization ensures fair comparison across markets\n",
    "\n",
    "**Next Steps**: Proceed to Notebook 3 for model training and validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
