{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff9584b",
   "metadata": {},
   "source": [
    "# Deep Learning Options Trading - Feature Analysis\n",
    "\n",
    "This notebook analyzes the engineered features for **individual option contracts** from Databento:\n",
    "- Feature importance and predictive power for option pricing\n",
    "- Correlation with option prices (calls/puts from OHLCV data)\n",
    "- Stationarity and time series properties\n",
    "- Feature engineering validation\n",
    "\n",
    "**Note:** Databento OHLCV schema provides individual contract data. Straddle construction happens at strategy execution, not in raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9693e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af126e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature engineering to create processed features\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Check if features already exist\n",
    "features_file = Path('../data/processed/features.csv')\n",
    "\n",
    "if features_file.exists():\n",
    "    print(\"Features already exist. Skipping feature engineering.\")\n",
    "    print(f\"File: {features_file}\")\n",
    "    print(f\"Size: {features_file.stat().st_size / 1024:.1f} KB\")\n",
    "else:\n",
    "    print(\"Running feature engineering...\")\n",
    "    print(\"This will process raw options and underlying price data.\\n\")\n",
    "    \n",
    "    # Import and run the feature engineering module\n",
    "    try:\n",
    "        # Change to parent directory (where create_features.py expects to run from)\n",
    "        original_dir = os.getcwd()\n",
    "        parent_dir = Path('..').resolve()\n",
    "        os.chdir(parent_dir)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "        \n",
    "        import create_features\n",
    "        \n",
    "        # Load raw data\n",
    "        prices, options = create_features.load_raw_data()\n",
    "        \n",
    "        # Create features\n",
    "        features = create_features.create_features(prices, options)\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        X, y, metadata = create_features.create_sequences(features, lookback_window=30)\n",
    "        \n",
    "        # Save everything\n",
    "        output_file = create_features.save_processed_data(features, X, y, metadata)\n",
    "        \n",
    "        # Change back to original directory\n",
    "        os.chdir(original_dir)\n",
    "        \n",
    "        print(\"\\nSUCCESS - Feature engineering complete!\")\n",
    "        print(f\"Features saved: {output_file}\")\n",
    "        print(f\"  - Feature records: {len(features)}\")\n",
    "        print(f\"  - Feature columns: {features.shape[1]}\")\n",
    "        print(f\"  - LSTM sequences: {len(X)}\")\n",
    "        print(f\"  - Sequence shape: {X.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Make sure to change back to original directory on error\n",
    "        if 'original_dir' in locals():\n",
    "            os.chdir(original_dir)\n",
    "        print(f\"Error during feature engineering: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nTip: Make sure you have run the data fetching notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c7e986",
   "metadata": {},
   "source": [
    "## 1. Load Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and targets\n",
    "try:\n",
    "    features_df = pd.read_csv('../data/processed/features.csv')\n",
    "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
    "    print(f\"Loaded features: {len(features_df)} records\")\n",
    "    print(f\"Columns: {features_df.columns.tolist()}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    # Target is option_price (individual contracts, not straddles)\n",
    "    feature_cols = [col for col in features_df.columns \n",
    "                   if col not in ['date', 'ticker', 'option_price', 'option_symbol']]\n",
    "    \n",
    "    target_col = 'option_price'\n",
    "    X = features_df[feature_cols]\n",
    "    y = features_df[target_col]\n",
    "    \n",
    "    print(f\"\\nFeatures ({len(feature_cols)}): {feature_cols}\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"\\nData shape: X={X.shape}, y={y.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Features not found. Run feature engineering first.\")\n",
    "    X, y = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate feature distributions and relationships with target \n",
    "features_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc7d0a",
   "metadata": {},
   "source": [
    "## 2. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Correlation with target\n",
    "    correlations = X.corrwith(y).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    correlations.plot(kind='bar')\n",
    "    plt.title('Feature Correlations with Option Price')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Correlation Coefficient')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 5 positive correlations:\")\n",
    "    print(correlations.head())\n",
    "    print(\"\\nTop 5 negative correlations:\")\n",
    "    print(correlations.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Feature-to-feature correlations\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr_matrix = X.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0, fmt='.2f', square=True)\n",
    "    plt.title('Feature Correlation Matrix (Lower Triangle)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify highly correlated features\n",
    "    high_corr = np.where(np.abs(corr_matrix) > 0.8)\n",
    "    high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y]) \n",
    "                      for x, y in zip(*high_corr) if x != y and x < y]\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"Highly correlated feature pairs (|corr| > 0.8):\")\n",
    "        for pair in high_corr_pairs:\n",
    "            corr_value = corr_matrix.loc[pair[0], pair[1]]\n",
    "            print(f\"{pair[0]} - {pair[1]}: {corr_value:.3f}\")\n",
    "    else:\n",
    "        print(\"No highly correlated feature pairs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab3af7",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Mutual information\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mi_scores.plot(kind='bar')\n",
    "    plt.title('Feature Importance (Mutual Information)')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Mutual Information Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Mutual Information Scores:\")\n",
    "    for feature, score in mi_scores.items():\n",
    "        print(f\"{feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Linear regression coefficients\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_scaled, y)\n",
    "    \n",
    "    # Get coefficients\n",
    "    coefficients = pd.Series(lr.coef_, index=X.columns).sort_values(ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    coefficients.plot(kind='bar')\n",
    "    plt.title('Linear Regression Coefficients (Standardized)')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"RÂ² Score: {lr.score(X_scaled, y):.4f}\")\n",
    "    print(\"\\nTop positive coefficients:\")\n",
    "    print(coefficients.head())\n",
    "    print(\"\\nTop negative coefficients:\")\n",
    "    print(coefficients.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e30e3d",
   "metadata": {},
   "source": [
    "## 4. Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Augmented Dickey-Fuller test for stationarity\n",
    "    stationarity_results = {}\n",
    "    \n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            adf_result = adfuller(X[col].dropna())\n",
    "            stationarity_results[col] = {\n",
    "                'adf_statistic': adf_result[0],\n",
    "                'p_value': adf_result[1],\n",
    "                'critical_values': adf_result[4],\n",
    "                'stationary': adf_result[1] < 0.05\n",
    "            }\n",
    "        except:\n",
    "            stationarity_results[col] = {'error': 'Could not test stationarity'}\n",
    "    \n",
    "    # Display results\n",
    "    print(\"Stationarity Test Results (Augmented Dickey-Fuller):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for feature, result in stationarity_results.items():\n",
    "        if 'error' not in result:\n",
    "            print(f\"{feature}:\")\n",
    "            print(f\"  ADF Statistic: {result['adf_statistic']:.4f}\")\n",
    "            print(f\"  p-value: {result['p_value']:.4f}\")\n",
    "            print(f\"  Stationary: {result['stationary']}\")\n",
    "            print(f\"  5% Critical Value: {result['critical_values']['5%']:.4f}\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"{feature}: {result['error']}\")\n",
    "    \n",
    "    # Summary\n",
    "    stationary_features = [f for f, r in stationarity_results.items() \n",
    "                          if 'stationary' in r and r['stationary']]\n",
    "    non_stationary_features = [f for f, r in stationarity_results.items() \n",
    "                              if 'stationary' in r and not r['stationary']]\n",
    "    \n",
    "    print(f\"\\nStationary features: {len(stationary_features)}/{len(X.columns)}\")\n",
    "    print(f\"Non-stationary features: {len(non_stationary_features)}/{len(X.columns)}\")\n",
    "    \n",
    "    if non_stationary_features:\n",
    "        print(f\"\\nNon-stationary features: {', '.join(non_stationary_features)}\")\n",
    "        print(\"Consider differencing or other transformations for LSTM input.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d012f9a",
   "metadata": {},
   "source": [
    "## 5. Time Series Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Autocorrelation analysis for key features\n",
    "    from statsmodels.graphics.tsaplots import plot_acf\n",
    "    \n",
    "    # Use features that actually exist in our data\n",
    "    key_features = ['option_premium_normalized', 'underlying_return_1d', 'underlying_volatility_30d', 'option_price_ma5']\n",
    "    key_features = [f for f in key_features if f in X.columns]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(key_features), 1, figsize=(12, 4*len(key_features)))\n",
    "    if len(key_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, feature in enumerate(key_features):\n",
    "        plot_acf(X[feature].dropna(), lags=30, ax=axes[i], title=f'Autocorrelation - {feature}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Rolling statistics to check for structural breaks\n",
    "    # Note: Using smaller window since our test data is only ~120 days\n",
    "    # Only plot top 5 features by correlation to prevent memory issues\n",
    "    top_features = correlations.abs().nlargest(5).index.tolist()\n",
    "    plot_features = [f for f in top_features if f in feature_cols]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(plot_features), 1, figsize=(15, 3*len(plot_features)))\n",
    "    if len(plot_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    window = 30  # 30-day rolling window (adjust to 252 for full dataset)\n",
    "    \n",
    "    print(f\"Plotting rolling statistics for top {len(plot_features)} features: {', '.join(plot_features)}\")\n",
    "    \n",
    "    for i, feature in enumerate(plot_features):\n",
    "        rolling_mean = X[feature].rolling(window=window).mean()\n",
    "        rolling_std = X[feature].rolling(window=window).std()\n",
    "        \n",
    "        ax1 = axes[i]\n",
    "        ax1.plot(X[feature].index, X[feature].values, alpha=0.7, label='Feature')\n",
    "        ax1.plot(X[feature].index, rolling_mean, color='red', label=f'{window}-day Mean')\n",
    "        ax1.set_title(f'{feature} with Rolling Mean')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Add secondary y-axis for std\n",
    "        ax2 = ax1.twinx()\n",
    "\n",
    "        ax2.plot(X[feature].index, rolling_std, color='green', alpha=0.7, label=f'{window}-day Std')    plt.show()\n",
    "\n",
    "        ax2.set_ylabel('Rolling Std', color='green')    plt.tight_layout()\n",
    "\n",
    "        ax2.legend(loc='upper right')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e2808",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506879e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None:\n",
    "    # Check for outliers\n",
    "    # Only plot top 5 features by correlation to prevent memory issues\n",
    "    top_features = correlations.abs().nlargest(5).index.tolist()\n",
    "    plot_features = [f for f in top_features if f in feature_cols]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(plot_features), 1, figsize=(12, 3*len(plot_features)))\n",
    "    if len(plot_features) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    print(f\"Plotting box plots for top {len(plot_features)} features: {', '.join(plot_features)}\")\n",
    "    \n",
    "    for i, feature in enumerate(plot_features):\n",
    "        # Box plot for outlier detection\n",
    "        axes[i].boxplot(X[feature].dropna(), vert=False)\n",
    "        axes[i].set_title(f'Box Plot - {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Outlier statistics\n",
    "    outlier_stats = {}\n",
    "    for feature in feature_cols:\n",
    "        data = X[feature].dropna()\n",
    "        q1, q3 = data.quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        outlier_stats[feature] = {\n",
    "            'outlier_count': len(outliers),\n",
    "            'outlier_percentage': len(outliers) / len(data) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "\n",
    "            'upper_bound': upper_bound        print(f\"{feature}: {stats['outlier_count']} outliers ({stats['outlier_percentage']:.2f}%)\")\n",
    "\n",
    "        }    for feature, stats in outlier_stats.items():\n",
    "\n",
    "        print(\"Outlier Analysis:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cdbc0",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9016737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive feature analysis report\n",
    "if X is not None:\n",
    "    print(\"=== FEATURE ANALYSIS SUMMARY ===\\n\")\n",
    "    \n",
    "    # Feature overview\n",
    "    print(f\"Total features: {len(feature_cols)}\")\n",
    "    print(f\"Total samples: {len(X)}\")\n",
    "    print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Predictive power ranking\n",
    "    print(\"\\nTop 5 features by correlation with target:\")\n",
    "    top_corr = correlations.head()\n",
    "    for i, (feature, corr) in enumerate(top_corr.items(), 1):\n",
    "        print(f\"{i}. {feature}: {corr:.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 features by mutual information:\")\n",
    "    top_mi = mi_scores.head()\n",
    "    for i, (feature, mi) in enumerate(top_mi.items(), 1):\n",
    "        print(f\"{i}. {feature}: {mi:.4f}\")\n",
    "    \n",
    "    # Stationarity summary\n",
    "    stationary_count = sum(1 for r in stationarity_results.values() if r.get('stationary', False))\n",
    "    print(f\"\\nStationary features: {stationary_count}/{len(feature_cols)}\")\n",
    "    \n",
    "    # Data quality\n",
    "    total_outliers = sum(stats['outlier_count'] for stats in outlier_stats.values())\n",
    "    avg_outlier_pct = np.mean([stats['outlier_percentage'] for stats in outlier_stats.values()])\n",
    "    print(f\"Total outliers: {total_outliers}\")\n",
    "    print(f\"Average outlier percentage: {avg_outlier_pct:.2f}%\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "    \n",
    "    if non_stationary_features:\n",
    "        print(f\"Consider differencing for non-stationary features: {', '.join(non_stationary_features[:3])}...\")\n",
    "    \n",
    "    if avg_outlier_pct > 5:\n",
    "        print(\"High outlier percentage detected. Consider robust scaling or outlier treatment.\")\n",
    "    \n",
    "    low_corr_features = correlations[correlations.abs() < 0.1]\n",
    "    if len(low_corr_features) > 0:\n",
    "        print(f\"Consider removing low-correlation features: {', '.join(low_corr_features.index[:3])}...\")\n",
    "    \n",
    "    print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "else:\n",
    "    print(\"Features not available for analysis. Run feature engineering first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
