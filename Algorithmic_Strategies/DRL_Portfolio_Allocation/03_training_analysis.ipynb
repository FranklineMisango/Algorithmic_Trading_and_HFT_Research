{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b5ae77",
   "metadata": {},
   "source": [
    "# Training Analysis for DRL Portfolio\n",
    "\n",
    "This notebook analyzes the training process and hyperparameter sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9d8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from data_acquisition import DataAcquisition\n",
    "from portfolio_env import PortfolioEnv\n",
    "from rl_agent import DRLAgent\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c873f5",
   "metadata": {},
   "source": [
    "## 1. Load Data and Create Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbf58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_acq = DataAcquisition('config.yaml')\n",
    "dataset = data_acq.fetch_full_dataset()\n",
    "\n",
    "# Create environments\n",
    "train_env = PortfolioEnv(\n",
    "    prices=dataset['train']['prices'],\n",
    "    returns=dataset['train']['returns']\n",
    ")\n",
    "\n",
    "val_env = PortfolioEnv(\n",
    "    prices=dataset['val']['prices'],\n",
    "    returns=dataset['val']['returns']\n",
    ")\n",
    "\n",
    "print(f\"Training data: {len(dataset['train']['prices'])} days\")\n",
    "print(f\"Validation data: {len(dataset['val']['prices'])} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55accd19",
   "metadata": {},
   "source": [
    "## 2. Train Agent (Quick Test)\n",
    "\n",
    "**Note:** For full training (500k timesteps), run the main.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent\n",
    "agent = DRLAgent(train_env, algorithm='ppo')\n",
    "\n",
    "print(f\"Algorithm: {agent.algorithm_name.upper()}\")\n",
    "print(f\"Policy: {agent.model.policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f81292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training test (10k steps instead of 500k)\n",
    "print(\"Running quick training test (10k timesteps)...\\n\")\n",
    "\n",
    "# Temporarily override config for quick test\n",
    "agent.training_config['total_timesteps'] = 10000\n",
    "\n",
    "# Train\n",
    "agent.train(eval_env=val_env, save_path='models/test_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138fdf6",
   "metadata": {},
   "source": [
    "## 3. Evaluate Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "eval_results = agent.evaluate(val_env, n_episodes=5)\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for metric, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a9aeb",
   "metadata": {},
   "source": [
    "## 4. Analyze TensorBoard Logs\n",
    "\n",
    "**Note:** After full training, use TensorBoard to visualize:\n",
    "- `tensorboard --logdir=logs/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acc6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for TensorBoard log analysis\n",
    "print(\"\"\"\\nTo view training curves:\n",
    "\n",
    "1. Run full training: python main.py --mode train\n",
    "2. Start TensorBoard: tensorboard --logdir=logs/\n",
    "3. Open browser: http://localhost:6006\n",
    "\n",
    "You'll see:\n",
    "- Episode reward mean (performance over time)\n",
    "- Value loss (critic network learning)\n",
    "- Policy loss (actor network learning)\n",
    "- Entropy (exploration vs exploitation)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e4f97",
   "metadata": {},
   "source": [
    "## 5. Test Policy on Sample Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d534ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one episode with trained policy\n",
    "obs, _ = val_env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "max_steps = 50\n",
    "\n",
    "while not done and step < max_steps:\n",
    "    action = agent.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = val_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    step += 1\n",
    "\n",
    "print(f\"Episode completed in {step} steps\")\n",
    "print(f\"Final portfolio value: ${val_env.portfolio_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89324c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Portfolio value\n",
    "ax1.plot(val_env.portfolio_history, linewidth=2, color='blue')\n",
    "ax1.set_title('Trained Policy - Portfolio Value', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Step', fontsize=12)\n",
    "ax1.set_ylabel('Portfolio Value', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Weights evolution\n",
    "weights_array = np.array(val_env.weights_history)\n",
    "symbols = dataset['val']['prices'].columns\n",
    "\n",
    "for i, symbol in enumerate(symbols):\n",
    "    ax2.plot(weights_array[:, i], label=symbol, linewidth=2)\n",
    "\n",
    "ax2.set_title('Trained Policy - Weight Evolution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Step', fontsize=12)\n",
    "ax2.set_ylabel('Weight', fontsize=12)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.axhline(y=0.4, color='red', linestyle='--', alpha=0.5, label='Max Weight')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18261dd",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Sensitivity (Conceptual)\n",
    "\n",
    "**Note:** For full hyperparameter optimization, use Optuna integration in main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for hyperparameter sensitivity analysis\n",
    "print(\"\"\"\\nHyperparameter Optimization:\n",
    "\n",
    "The config.yaml includes Optuna settings for optimizing:\n",
    "- learning_rate: [0.0001, 0.001]\n",
    "- gamma: [0.95, 0.999]\n",
    "- ent_coef: [0.0, 0.1]\n",
    "- volatility_penalty: [0.0, 1.0]\n",
    "\n",
    "To run full hyperparameter search:\n",
    "1. Implement Optuna study in main.py\n",
    "2. Run 50 trials (parallelized)\n",
    "3. Select best by validation Sharpe ratio\n",
    "\n",
    "Expected impact:\n",
    "- Learning rate: Trade-off between speed and stability\n",
    "- Gamma: Long-term vs short-term focus\n",
    "- Entropy coefficient: Exploration vs exploitation\n",
    "- Volatility penalty: Risk-return preference\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f24f8",
   "metadata": {},
   "source": [
    "## 7. Learning Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate learning progress (placeholder)\n",
    "# In practice, this would come from TensorBoard logs\n",
    "\n",
    "timesteps = np.arange(0, 500000, 10000)\n",
    "reward_mean = -0.01 + 0.015 * (1 - np.exp(-timesteps / 100000))\n",
    "reward_std = 0.02 * np.exp(-timesteps / 200000)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(timesteps, reward_mean, linewidth=2, label='Mean Reward')\n",
    "ax.fill_between(\n",
    "    timesteps,\n",
    "    reward_mean - reward_std,\n",
    "    reward_mean + reward_std,\n",
    "    alpha=0.3,\n",
    "    label='Â±1 Std Dev'\n",
    ")\n",
    "ax.set_title('Learning Progress (Simulated)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Timesteps', fontsize=12)\n",
    "ax.set_ylabel('Episode Reward', fontsize=12)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: This is a simulated learning curve. Run full training to see actual progress.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
