{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7204be",
   "metadata": {},
   "source": [
    "# Deep Learning Momentum Trading Strategy - Complete Pipeline\n",
    "\n",
    "Implementation of \"Applying Deep Learning to Enhance Momentum Trading Strategies in Stocks\" (Taki & Lee, 2013)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the complete workflow:\n",
    "1. **Data Preparation** - Download, filter, and engineer features\n",
    "2. **Model Training** - Build and train neural network with rolling window validation\n",
    "3. **Signal Generation** - Rank stocks by predicted probability\n",
    "4. **Strategy Backtesting** - Long-short portfolio construction\n",
    "5. **Performance Evaluation** - Analyze results vs benchmark\n",
    "\n",
    "## Key Results (Expected)\n",
    "- **Annualized Return**: ~12.8% (vs S&P 500: 7.0%)\n",
    "- **Sharpe Ratio**: ~1.03 (vs S&P 500: 0.5)\n",
    "- **Maximum Drawdown**: ~24% (vs S&P 500: 52.6%)\n",
    "\n",
    "## The \"Bitter Lesson\" Applied to Finance\n",
    "\n",
    "This strategy demonstrates that general-purpose deep learning on relatively raw features (simple returns) can discover patterns without complex hand-crafted financial rules. The value is in **ranking** stocks consistently, not predicting returns perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92491e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5662d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Data period: {config['data']['start_date']} to {config['data']['end_date']}\")\n",
    "print(f\"  Min price filter: ${config['data']['min_price']}\")\n",
    "print(f\"  Features: {config['model']['input_size']}\")\n",
    "print(f\"  Model architecture: {config['model']['hidden_layers']}\")\n",
    "print(f\"  Strategy: Long Q{config['strategy']['long_quantile']}, Short Q{config['strategy']['short_quantile']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba4271",
   "metadata": {},
   "source": [
    "## Part 1: Data Preparation\n",
    "\n",
    "### Step 1: Define Stock Universe\n",
    "\n",
    "Start with a broad universe of US stocks. In production, you would use all stocks from NYSE, AMEX, and NASDAQ. For this demo, we'll use a subset of liquid, large-cap stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a893d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stock universe (subset for demo)\n",
    "# In production, use full universe from NYSE, AMEX, NASDAQ\n",
    "stock_universe = [\n",
    "    # Technology\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'NFLX', 'ADBE', 'CRM',\n",
    "    'ORCL', 'INTC', 'AMD', 'QCOM', 'AVGO', 'TXN', 'MU', 'AMAT', 'LRCX', 'KLAC',\n",
    "    \n",
    "    # Financials\n",
    "    'JPM', 'BAC', 'WFC', 'GS', 'MS', 'C', 'BLK', 'SCHW', 'AXP', 'USB',\n",
    "    \n",
    "    # Healthcare\n",
    "    'UNH', 'JNJ', 'PFE', 'ABBV', 'MRK', 'TMO', 'ABT', 'DHR', 'BMY', 'AMGN',\n",
    "    \n",
    "    # Consumer\n",
    "    'WMT', 'HD', 'MCD', 'NKE', 'SBUX', 'TGT', 'LOW', 'COST', 'TJX', 'DG',\n",
    "    \n",
    "    # Industrials\n",
    "    'BA', 'CAT', 'HON', 'UPS', 'GE', 'MMM', 'LMT', 'DE', 'UNP', 'RTX',\n",
    "    \n",
    "    # Energy\n",
    "    'XOM', 'CVX', 'COP', 'SLB', 'EOG', 'PXD', 'MPC', 'PSX', 'VLO', 'OXY',\n",
    "    \n",
    "    # Materials\n",
    "    'LIN', 'APD', 'NEM', 'FCX', 'DOW', 'NUE', 'ECL', 'SHW', 'DD', 'ALB'\n",
    "]\n",
    "\n",
    "print(f\"Stock universe: {len(stock_universe)} tickers\")\n",
    "print(f\"Sectors: Technology, Financials, Healthcare, Consumer, Industrials, Energy, Materials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c395433b",
   "metadata": {},
   "source": [
    "### Step 2: Download and Process Data\n",
    "\n",
    "This step:\n",
    "1. Downloads historical price data\n",
    "2. Filters stocks by minimum price ($5)\n",
    "3. Engineers 33 momentum features:\n",
    "   - 12 long-term (monthly returns from t-13 to t-2)\n",
    "   - 20 short-term (daily returns from recent month)\n",
    "   - 1 anomaly (January Effect)\n",
    "4. Applies **cross-sectional z-score standardization** (critical!)\n",
    "5. Generates binary classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processor import DataProcessor\n",
    "\n",
    "# Initialize processor\n",
    "processor = DataProcessor(config)\n",
    "\n",
    "# Run complete pipeline\n",
    "features, labels = processor.prepare_dataset(stock_universe)\n",
    "\n",
    "print(f\"\\nFeatures shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"\\nFeature columns: {list(features.columns)}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions (after z-score normalization)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Sample a few features\n",
    "sample_features = ['ret_m2', 'ret_m6', 'ret_d1', 'ret_d5']\n",
    "\n",
    "for idx, feature in enumerate(sample_features):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    features[feature].hist(bins=50, ax=ax, alpha=0.7)\n",
    "    ax.set_title(f'{feature} Distribution (Z-Scored)')\n",
    "    ax.set_xlabel('Z-Score')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.axvline(0, color='red', linestyle='--', alpha=0.5, label='Mean')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: All features are centered around 0 with unit variance (z-scored)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53616af",
   "metadata": {},
   "source": [
    "## Part 2: Model Training with Rolling Window Validation\n",
    "\n",
    "### The Importance of Rolling Windows\n",
    "\n",
    "We use **Rolling Window Cross-Validation** to:\n",
    "1. Prevent look-ahead bias\n",
    "2. Simulate realistic live trading\n",
    "3. Account for market regime changes\n",
    "\n",
    "Process:\n",
    "- **Train** on 3 years of data\n",
    "- **Validate** on next 1 year (model selection, early stopping)\n",
    "- **Test** on following 1 year (performance evaluation)\n",
    "- **Roll forward** 6 months and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for PyTorch\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = features.values.astype(np.float32)\n",
    "y = labels.values.astype(np.float32)\n",
    "\n",
    "# Get date index for time-based splitting\n",
    "dates = features.index.get_level_values('date')\n",
    "\n",
    "print(f\"Dataset prepared:\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  y shape: {y.shape}\")\n",
    "print(f\"  Date range: {dates.min()} to {dates.max()}\")\n",
    "print(f\"  GPU available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7422acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model, print_model_summary, EarlyStopping\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create model\n",
    "model = create_model(config)\n",
    "print_model_summary(model)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['training']['learning_rate'])\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Max epochs: {config['training']['epochs']}\")\n",
    "print(f\"  Early stopping patience: {config['training']['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f2dd1",
   "metadata": {},
   "source": [
    "### Simplified Training (Single Window)\n",
    "\n",
    "For demonstration, we'll train on one window. In production, you would:\n",
    "1. Loop through all rolling windows\n",
    "2. Train a separate model for each window\n",
    "3. Use each model's predictions for its test period\n",
    "4. Concatenate all predictions for final strategy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple train/val/test split (70/15/15)\n",
    "n_samples = len(X)\n",
    "train_end = int(n_samples * 0.70)\n",
    "val_end = int(n_samples * 0.85)\n",
    "\n",
    "X_train, y_train = X[:train_end], y[:train_end]\n",
    "X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "print(f\"Data splits:\")\n",
    "print(f\"  Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Val:   {X_val.shape[0]:,} samples\")\n",
    "print(f\"  Test:  {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
    "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['training']['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['training']['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f939ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "early_stopping = EarlyStopping(patience=config['training']['early_stopping_patience'])\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(config['training']['epochs']):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch_X, batch_y = batch_X.cuda(), batch_y.cuda()\n",
    "            \n",
    "            outputs = model(batch_X).squeeze()\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{config['training']['epochs']} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping(val_loss):\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e16666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.plot(val_losses, label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (Binary Cross-Entropy)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0dcb76",
   "metadata": {},
   "source": [
    "## Part 3: Model Evaluation & Signal Generation\n",
    "\n",
    "### Prediction Accuracy vs Ranking Ability\n",
    "\n",
    "**Key Insight**: Raw prediction accuracy (~52%) may seem low, but the model's **ranking ability** is what matters. The spread between high-confidence and low-confidence predictions creates profitable opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214f26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "test_preds = (test_probs > 0.5).astype(int)\n",
    "accuracy = (test_preds == y_test).mean()\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.2%}\")\n",
    "print(f\"  Predicted Class 1: {test_preds.sum() / len(test_preds):.2%}\")\n",
    "print(f\"  Actual Class 1: {y_test.mean():.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_preds, target_names=['Below Median', 'Above Median']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28221857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(test_probs, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(0.5, color='red', linestyle='--', label='Threshold=0.5')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_probs[y_test == 0], bins=30, alpha=0.5, label='Actual: Below Median', edgecolor='black')\n",
    "plt.hist(test_probs[y_test == 1], bins=30, alpha=0.5, label='Actual: Above Median', edgecolor='black')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Predictions by True Label')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608462b",
   "metadata": {},
   "source": [
    "### Generate Trading Signals via Quantile Ranking\n",
    "\n",
    "**Strategy**:\n",
    "1. Rank all stocks by predicted probability (0 to 1)\n",
    "2. Divide into 10 quantiles (deciles)\n",
    "3. **LONG** top quantile (Q10) - highest confidence\n",
    "4. **SHORT** bottom quantile (Q1) - lowest confidence\n",
    "5. Rebalance monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcf70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataframe with predictions\n",
    "test_dates = dates[val_end:]\n",
    "test_tickers = features.index.get_level_values('ticker')[val_end:]\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    'date': test_dates,\n",
    "    'ticker': test_tickers,\n",
    "    'predicted_prob': test_probs,\n",
    "    'actual_label': y_test\n",
    "})\n",
    "\n",
    "# Add quantile rankings (cross-sectional, per day)\n",
    "test_df['quantile'] = test_df.groupby('date')['predicted_prob'].transform(\n",
    "    lambda x: pd.qcut(x, q=10, labels=range(1, 11), duplicates='drop')\n",
    ")\n",
    "\n",
    "print(f\"Test data with predictions:\")\n",
    "print(test_df.head(20))\n",
    "\n",
    "print(f\"\\nQuantile distribution:\")\n",
    "print(test_df['quantile'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by quantile\n",
    "quantile_performance = test_df.groupby('quantile').agg({\n",
    "    'predicted_prob': ['mean', 'std'],\n",
    "    'actual_label': 'mean'  # % that actually outperformed\n",
    "}).round(4)\n",
    "\n",
    "quantile_performance.columns = ['Avg Predicted Prob', 'Std Predicted Prob', 'Actual Outperform %']\n",
    "\n",
    "print(\"Performance by Quantile:\")\n",
    "print(\"=\"*80)\n",
    "print(quantile_performance)\n",
    "print(\"\\nKey Insight: Q10 should have highest actual outperformance rate\")\n",
    "print(\"            Q1 should have lowest actual outperformance rate\")\n",
    "print(\"            The SPREAD between Q10 and Q1 drives strategy returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9898c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantile performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Predicted probability by quantile\n",
    "ax1 = axes[0]\n",
    "quantile_performance['Avg Predicted Prob'].plot(kind='bar', ax=ax1, color='steelblue', alpha=0.7)\n",
    "ax1.set_xlabel('Quantile')\n",
    "ax1.set_ylabel('Average Predicted Probability')\n",
    "ax1.set_title('Model Confidence by Quantile')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)\n",
    "\n",
    "# Plot 2: Actual outperformance by quantile\n",
    "ax2 = axes[1]\n",
    "quantile_performance['Actual Outperform %'].plot(kind='bar', ax=ax2, color='coral', alpha=0.7)\n",
    "ax2.axhline(0.5, color='red', linestyle='--', label='50% (Random)')\n",
    "ax2.set_xlabel('Quantile')\n",
    "ax2.set_ylabel('Actual Outperformance Rate')\n",
    "ax2.set_title('Realized Outperformance by Quantile')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate spread\n",
    "q10_rate = quantile_performance.loc[10.0, 'Actual Outperform %']\n",
    "q1_rate = quantile_performance.loc[1.0, 'Actual Outperform %']\n",
    "spread = q10_rate - q1_rate\n",
    "\n",
    "print(f\"\\nQuantile Spread Analysis:\")\n",
    "print(f\"  Q10 (Long) outperformance rate: {q10_rate:.2%}\")\n",
    "print(f\"  Q1 (Short) outperformance rate: {q1_rate:.2%}\")\n",
    "print(f\"  Spread: {spread:.2%}\")\n",
    "print(f\"\\nA positive spread indicates the model successfully ranks stocks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ec39a",
   "metadata": {},
   "source": [
    "## Part 4: Strategy Backtesting\n",
    "\n",
    "### Long-Short Portfolio Construction\n",
    "\n",
    "- **Long**: Equal-weight portfolio of Q10 stocks\n",
    "- **Short**: Equal-weight portfolio of Q1 stocks  \n",
    "- **Rebalance**: Monthly\n",
    "- **Leverage**: 100% long + 100% short = 200% gross, 0% net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STRATEGY IMPLEMENTATION NOTE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "For a complete backtest, you would need:\n",
    "\n",
    "1. Monthly forward returns for each stock\n",
    "2. Portfolio rebalancing logic\n",
    "3. Transaction cost modeling\n",
    "4. Short selling cost/borrow fee modeling\n",
    "5. Position sizing and risk management\n",
    "\n",
    "This requires actual price data for all stocks in the test period.\n",
    "The following cells demonstrate the logic with simplified assumptions.\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96668e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified strategy simulation\n",
    "# Assumes:\n",
    "# - actual_label=1 means stock returned above median\n",
    "# - actual_label=0 means stock returned below median\n",
    "# - Simplified: Long Q10 earns +1 if actual_label=1, -1 if actual_label=0\n",
    "# - Simplified: Short Q1 earns +1 if actual_label=0, -1 if actual_label=1\n",
    "\n",
    "# Calculate strategy returns by quantile\n",
    "def calculate_quantile_returns(df):\n",
    "    \"\"\"\n",
    "    Simplified return calculation:\n",
    "    - Long positions: return = 2 * (actual_label - 0.5)\n",
    "    - Short positions: return = -2 * (actual_label - 0.5)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for q in range(1, 11):\n",
    "        q_df = df[df['quantile'] == float(q)]\n",
    "        \n",
    "        # For long strategy (Q10)\n",
    "        if q == 10:\n",
    "            long_return = 2 * (q_df['actual_label'].mean() - 0.5)\n",
    "            results.append({'Quantile': q, 'Type': 'Long', 'Return': long_return})\n",
    "        \n",
    "        # For short strategy (Q1)\n",
    "        elif q == 1:\n",
    "            short_return = -2 * (q_df['actual_label'].mean() - 0.5)\n",
    "            results.append({'Quantile': q, 'Type': 'Short', 'Return': short_return})\n",
    "        \n",
    "        # Other quantiles (for comparison)\n",
    "        else:\n",
    "            neutral_return = 2 * (q_df['actual_label'].mean() - 0.5)\n",
    "            results.append({'Quantile': q, 'Type': 'Neutral', 'Return': neutral_return})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "quantile_returns = calculate_quantile_returns(test_df)\n",
    "\n",
    "print(\"Simplified Quantile Returns (Per Period):\")\n",
    "print(quantile_returns)\n",
    "\n",
    "# Calculate long-short return\n",
    "long_ret = quantile_returns[quantile_returns['Type'] == 'Long']['Return'].values[0]\n",
    "short_ret = quantile_returns[quantile_returns['Type'] == 'Short']['Return'].values[0]\n",
    "long_short_ret = long_ret + short_ret\n",
    "\n",
    "print(f\"\\nLong-Short Strategy (Simplified):\")\n",
    "print(f\"  Long Q10 return: {long_ret:+.2%}\")\n",
    "print(f\"  Short Q1 return: {short_ret:+.2%}\")\n",
    "print(f\"  Combined return: {long_short_ret:+.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175c263",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We've Demonstrated\n",
    "\n",
    "1. **Data Preparation**: Engineered 33 momentum features and applied critical cross-sectional standardization\n",
    "\n",
    "2. **Model Architecture**: Built a neural network with bottleneck layer to learn compressed momentum representation\n",
    "\n",
    "3. **Validation Strategy**: Emphasized rolling window cross-validation (though simplified here for demo)\n",
    "\n",
    "4. **Signal Generation**: Showed that ranking by predicted probability creates a spread between top and bottom quantiles\n",
    "\n",
    "5. **The Bitter Lesson**: Demonstrated that general-purpose deep learning on relatively raw features can discover patterns without complex hand-crafted rules\n",
    "\n",
    "### Expected Production Results (From Paper)\n",
    "\n",
    "- **Annualized Return**: 12.8% vs S&P 500: 7.0%\n",
    "- **Sharpe Ratio**: 1.03 vs S&P 500: 0.5\n",
    "- **Max Drawdown**: 24% vs S&P 500: 52.6%\n",
    "- **Market Correlation**: Negative (diversification benefit)\n",
    "\n",
    "### Important Caveats\n",
    "\n",
    "⚠️ **This is Educational**: Not investment advice\n",
    "\n",
    "⚠️ **Simplified Backtest**: Production requires:\n",
    "- Complete rolling window implementation\n",
    "- Realistic transaction costs\n",
    "- Short selling costs and constraints\n",
    "- Market impact modeling\n",
    "- Risk management systems\n",
    "\n",
    "⚠️ **Results Will Vary**: Due to:\n",
    "- Different data sources\n",
    "- Market evolution\n",
    "- Model architecture choices\n",
    "- Training procedures\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To extend this research:\n",
    "\n",
    "1. **Complete Rolling Windows**: Implement full rolling window validation\n",
    "2. **Expand Universe**: Use all NYSE/AMEX/NASDAQ stocks\n",
    "3. **Feature Engineering**: Add more sophisticated features (volume, volatility, etc.)\n",
    "4. **Architecture Search**: Experiment with different network architectures\n",
    "5. **Ensemble Methods**: Combine multiple models\n",
    "6. **Risk Management**: Add position limits, stop-losses, etc.\n",
    "7. **Cost Modeling**: Include realistic transaction and borrowing costs\n",
    "\n",
    "### References\n",
    "\n",
    "1. Taki, D., & Lee, A. (2013). \"Applying Deep Learning to Enhance Momentum Trading Strategies in Stocks\"\n",
    "2. Sutton, R. (2019). \"The Bitter Lesson\"\n",
    "3. Jegadeesh, N., & Titman, S. (1993). \"Returns to Buying Winners and Selling Losers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a064d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "output_dir = Path(config['output']['models_dir'])\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save model\n",
    "model_path = output_dir / 'momentum_ranker_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save predictions\n",
    "results_dir = Path(config['output']['results_dir'])\n",
    "results_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "test_df.to_csv(results_dir / 'test_predictions.csv', index=False)\n",
    "quantile_returns.to_csv(results_dir / 'quantile_returns.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to: {results_dir}\")\n",
    "print(\"\\nPipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
